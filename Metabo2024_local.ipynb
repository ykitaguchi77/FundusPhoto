{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpJGAAaxZnTyuh/ZEOziof",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/FundusPhoto/blob/main/Metabo2024_local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Metab2024 local**"
      ],
      "metadata": {
        "id": "Aptf1_G_GgbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要ライブラリ読み込み\n",
        "import random\n",
        "import timm\n",
        "import copy\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from timm.scheduler import CosineLRScheduler\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# モデル枠組み読み込み\n",
        "model = timm.create_model(model_name='swin_base_patch4_window12_384', num_classes=1, pretrained=False)\n",
        "\n",
        "# GPU使用する場合\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")\n",
        "model = model.to(device)\n",
        "\n",
        "# 学習済みモデル読み込み\n",
        "model_path = 'model_20220903.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "\n",
        "original_csv_path = \"label_train.csv\"\n",
        "original_df = pd.read_csv(original_csv_path)\n",
        "\n",
        "# Load the provided metabo_disease.csv file again\n",
        "disease_csv_path = 'metabo_disease.csv'\n",
        "disease_df = pd.read_csv(disease_csv_path)\n",
        "\n",
        "# Filter the disease_df to get only rows where reason is \"AH\" or \"Blur\"\n",
        "#exclude_df = disease_df[disease_df['reason'].isin(['AH', 'Blur', 'ERM', \"Hemorrhage\", \"Coagulation\", \"VO\", \"Degeneration\", \"AMD\", \"CRA\", \"Drusen\"])]\n",
        "exclude_df = disease_df[disease_df['reason'].isin(['AH', 'Blur', 'ERM', \"Hemorrhage\", \"Coagulation\", \"VO\", \"Degeneration\", \"AMD\", \"CRA\", \"Drusen\"])]\n",
        "\n",
        "# Extract the ids (filenames) from ah_blur_df that match the 'AH' or 'Blur' criteria\n",
        "ah_blur_ids = exclude_df['id'].tolist()\n",
        "\n",
        "# Now remove these filenames from comparison_df\n",
        "cleaned_df = original_df[~original_df['filename'].isin(ah_blur_ids)]\n",
        "len(cleaned_df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Training settings\n",
        "num_epochs = 200\n",
        "patience = 10\n",
        "seed = 42\n",
        "img_dir = \"images_whole_384px\"\n",
        "\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(seed)\n",
        "\n",
        "\n",
        "# データセットクラスの定義\n",
        "class FundusDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_dir, transform=None):\n",
        "        self.data = dataframe\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        label = self.data.iloc[idx, 1]  # AGEカラムのインデックス\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# データの前処理とオーグメンテーション\n",
        "transform_train = transforms.Compose([\n",
        "    #transforms.Resize((224, 224)),\n",
        "    #transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=5),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
        "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.RandomGrayscale(p=0.1),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 検証用の変換（オーグメンテーションなし）\n",
        "transform_val = transforms.Compose([\n",
        "    #transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "full_dataset = FundusDataset(cleaned_df, img_dir)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_dataset.dataset.transform = transform_train\n",
        "val_dataset.dataset.transform = transform_val\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "# CosineLRSchedulerの設定\n",
        "scheduler = CosineLRScheduler(\n",
        "    optimizer,\n",
        "    t_initial=num_epochs,\n",
        "    lr_min=1e-6,\n",
        "    warmup_t=5,\n",
        "    warmup_lr_init=1e-7,\n",
        "    warmup_prefix=True\n",
        ")\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "# Early Stopping クラス\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.best_model = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_model = copy.deepcopy(model.state_dict())\n",
        "            return True\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_model = copy.deepcopy(model.state_dict())\n",
        "            self.counter = 0\n",
        "            return True\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "            return False\n",
        "\n",
        "# トレーニング関数の修正\n",
        "def train(model, train_loader, criterion, optimizer, device, scaler):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device).float()\n",
        "        inputs = inputs.to(memory_format=torch.channels_last)\n",
        "\n",
        "        for param in model.parameters():\n",
        "            param.grad = None\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda'):\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "        all_predictions.extend(outputs.detach().cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    mse = mean_squared_error(all_targets, all_predictions)\n",
        "    r2 = r2_score(all_targets, all_predictions)\n",
        "    return epoch_loss, mse, r2\n",
        "\n",
        "# 評価関数の修正\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device).float()\n",
        "            inputs = inputs.to(memory_format=torch.channels_last)\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    mse = mean_squared_error(all_targets, all_predictions)\n",
        "    r2 = r2_score(all_targets, all_predictions)\n",
        "    return epoch_loss, mse, r2\n",
        "\n",
        "# GradScalerの更新\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "# モデルの出力層の調整（必要に応じて）\n",
        "# model.fc = nn.Linear(model.fc.in_features, 1)  # 1つの出力（回帰の場合）\n",
        "\n",
        "# 損失関数の変更\n",
        "criterion = nn.MSELoss()  # 回帰問題の場合\n",
        "\n",
        "# トレーニングループの修正\n",
        "early_stopping = EarlyStopping(patience=patience)\n",
        "history = {'train_loss': [], 'train_mse': [], 'train_r2': [],\n",
        "           'val_loss': [], 'val_mse': [], 'val_r2': [], 'lr': []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train_loss, train_mse, train_r2 = train(model, train_loader, criterion, optimizer, device, scaler)\n",
        "    val_loss, val_mse, val_r2 = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    scheduler.step(epoch + 1)\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    history['lr'].append(current_lr)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_mse'].append(train_mse)\n",
        "    history['train_r2'].append(train_r2)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_mse'].append(val_mse)\n",
        "    history['val_r2'].append(val_r2)\n",
        "\n",
        "    is_best = early_stopping(val_loss, model)\n",
        "\n",
        "    if early_stopping.best_model is not None:\n",
        "        model.load_state_dict(early_stopping.best_model)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train MSE: {train_mse:.4f}, Train R2: {train_r2:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val MSE: {val_mse:.4f}, Val R2: {val_r2:.4f}\")\n",
        "    print(f\"Epoch duration: {epoch_duration:.2f} seconds\")\n",
        "    print(f\"Best model {'updated' if is_best else 'not updated'}\")\n",
        "    print(f\"Current learning rate: {current_lr:.6f}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "# 最終評価の修正\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        inputs = inputs.to(memory_format=torch.channels_last)\n",
        "        outputs = model(inputs).squeeze()\n",
        "        all_preds.extend(outputs.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "final_mse = mean_squared_error(all_labels, all_preds)\n",
        "final_r2 = r2_score(all_labels, all_preds)\n",
        "print(f\"Final MSE: {final_mse:.4f}\")\n",
        "print(f\"Final R2 Score: {final_r2:.4f}\")\n",
        "\n",
        "# 訓練結果のグラフ表示\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.plot(history['train_mse'], label='Train MSE')\n",
        "plt.plot(history['val_mse'], label='Validation MSE')\n",
        "plt.title('Mean Squared Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.plot(history['train_r2'], label='Train R2')\n",
        "plt.plot(history['val_r2'], label='Validation R2')\n",
        "plt.title('R2 Score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('R2')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.plot(history['lr'], label='Learning Rate')\n",
        "plt.title('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW2s64k-GrNC",
        "outputId": "9c12dd59-65ce-4df1-c2c4-480e8cf98a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\ykita\\AppData\\Local\\Temp\\ipykernel_24316\\2893370614.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "C:\\Users\\ykita\\AppData\\Local\\Temp\\ipykernel_24316\\2893370614.py:147: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "Train Loss: 15.3058, Train MSE: 15.3068, Train R2: 0.8647\n",
            "Val Loss: 15.5585, Val MSE: 15.5969, Val R2: 0.8561\n",
            "Epoch duration: 208.80 seconds\n",
            "Best model updated\n",
            "Current learning rate: 0.000020\n",
            "----------------------------------------\n",
            "Epoch 2/200\n",
            "Train Loss: 13.4634, Train MSE: 13.4637, Train R2: 0.8810\n",
            "Val Loss: 13.2644, Val MSE: 13.3079, Val R2: 0.8772\n",
            "Epoch duration: 223.41 seconds\n",
            "Best model updated\n",
            "Current learning rate: 0.000040\n",
            "----------------------------------------\n",
            "Epoch 3/200\n",
            "Train Loss: 10.1926, Train MSE: 10.1958, Train R2: 0.9099\n",
            "Val Loss: 13.3496, Val MSE: 13.4046, Val R2: 0.8763\n",
            "Epoch duration: 223.14 seconds\n",
            "Best model not updated\n",
            "Current learning rate: 0.000060\n",
            "----------------------------------------\n",
            "Epoch 4/200\n",
            "Train Loss: 10.6090, Train MSE: 10.6120, Train R2: 0.9062\n",
            "Val Loss: 13.5534, Val MSE: 13.6096, Val R2: 0.8744\n",
            "Epoch duration: 226.38 seconds\n",
            "Best model not updated\n",
            "Current learning rate: 0.000080\n",
            "----------------------------------------\n",
            "Epoch 5/200\n",
            "Train Loss: 11.1819, Train MSE: 11.1827, Train R2: 0.9011\n",
            "Val Loss: 14.9374, Val MSE: 14.9968, Val R2: 0.8616\n",
            "Epoch duration: 224.64 seconds\n",
            "Best model not updated\n",
            "Current learning rate: 0.000100\n",
            "----------------------------------------\n",
            "Epoch 6/200\n",
            "Train Loss: 12.8157, Train MSE: 12.8176, Train R2: 0.8867\n",
            "Val Loss: 14.9062, Val MSE: 14.9607, Val R2: 0.8619\n",
            "Epoch duration: 226.44 seconds\n",
            "Best model not updated\n",
            "Current learning rate: 0.000100\n",
            "----------------------------------------\n",
            "Epoch 7/200\n",
            "Train Loss: 12.4103, Train MSE: 12.4140, Train R2: 0.8903\n",
            "Val Loss: 13.7417, Val MSE: 13.7935, Val R2: 0.8727\n",
            "Epoch duration: 226.67 seconds\n",
            "Best model not updated\n",
            "Current learning rate: 0.000100\n",
            "----------------------------------------\n",
            "Epoch 8/200\n",
            "Train Loss: 12.4487, Train MSE: 12.4511, Train R2: 0.8899\n",
            "Val Loss: 12.7770, Val MSE: 12.8251, Val R2: 0.8817\n",
            "Epoch duration: 229.29 seconds\n",
            "Best model updated\n",
            "Current learning rate: 0.000100\n",
            "----------------------------------------\n",
            "Epoch 9/200\n",
            "Train Loss: 6.1163, Train MSE: 6.1182, Train R2: 0.9459\n",
            "Val Loss: 13.8668, Val MSE: 13.9159, Val R2: 0.8716\n",
            "Epoch duration: 232.96 seconds\n",
            "Best model not updated\n",
            "Current learning rate: 0.000100\n",
            "----------------------------------------\n",
            "Epoch 10/200\n",
            "Train Loss: 6.4439, Train MSE: 6.4462, Train R2: 0.9430\n",
            "Val Loss: 13.0581, Val MSE: 13.1090, Val R2: 0.8790\n",
            "Epoch duration: 235.97 seconds\n",
            "Best model not updated\n",
            "Current learning rate: 0.000100\n",
            "----------------------------------------\n",
            "Epoch 11/200\n",
            "Train Loss: 6.9026, Train MSE: 6.8966, Train R2: 0.9390\n",
            "Val Loss: 13.8558, Val MSE: 13.8830, Val R2: 0.8719\n",
            "Epoch duration: 230.01 seconds\n",
            "Best model not updated\n",
            "Current learning rate: 0.000100\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# 散布図の追加（サイズを大きくする）\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.scatter(all_labels, all_preds, alpha=0.5)\n",
        "plt.plot([min(all_labels), max(all_labels)], [min(all_labels), max(all_labels)], 'r--', lw=2)\n",
        "plt.xlabel('True Values', fontsize=12)\n",
        "plt.ylabel('Predictions', fontsize=12)\n",
        "plt.title('True vs Predicted Values', fontsize=14)\n",
        "\n",
        "# 残差プロットの追加\n",
        "residuals = np.array(all_labels) - np.array(all_preds)\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.scatter(all_preds, residuals, alpha=0.5)\n",
        "plt.xlabel('Predicted Values', fontsize=12)\n",
        "plt.ylabel('Residuals', fontsize=12)\n",
        "plt.title('Residual Plot', fontsize=14)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# MAEの計算\n",
        "mae = mean_absolute_error(all_labels, all_preds)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w36WwZMPLhQf",
        "outputId": "1f6402dd-3faf-4222-bb7d-003a010a5fbb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aaa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# モデルの保存先パスを設定\n",
        "save_path = \"finetuned_model/age.pth\"\n",
        "\n",
        "# パスが存在することを確認し、必要に応じてディレクトリを作成\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "# モデルの状態辞書を保存\n",
        "torch.save(model.state_dict(), save_path)\n",
        "\n",
        "print(f\"Model saved successfully to {save_path}\")\n",
        "\n",
        "# オプション: モデルの読み込みを確認\n",
        "# モデルの状態辞書を読み込む\n",
        "loaded_state_dict = torch.load(save_path)\n",
        "\n",
        "# 新しいモデルインスタンスに状態辞書を適用する\n",
        "# (ここでは 'model' が既に定義されていると仮定しています)\n",
        "model.load_state_dict(loaded_state_dict)\n",
        "\n",
        "print(\"Model loaded successfully for verification.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFcKs-TBLV27",
        "outputId": "d417f725-ff12-493a-b540-69566de7396c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 100%|█████████████████████████████████████████████████████████████| 145/145 [02:20<00:00,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total memory used: 2.28 MB\n",
            "Average memory per image: 0.00 MB\n",
            "Total time taken: 140.81 seconds\n",
            "Average time per image: 0.0305 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RpGvTTK5LeYM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}