{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMS5jH5VyhQh+t7LRB5N8+I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/FundusPhoto/blob/main/Metabo2024_tabular_local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fH9I44YAs3Bc",
        "outputId": "583766f1-3d75-4784-bf29-02813459f8be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available: True\n",
            "CUDA version: 12.4\n",
            "Number of CUDA devices: 1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import timm\n",
        "import copy\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from timm.scheduler import CosineLRScheduler\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.cuda.amp import GradScaler\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "%cd \"C:\\Users\\ykita\\Metabo2024\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLxfJoNgtEUZ",
        "outputId": "88889dcf-b377-41aa-9a20-a6fdcb67d882"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C:\\Users\\ykita\\Metabo2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル枠組み読み込み\n",
        "base_model = timm.create_model(model_name='swin_base_patch4_window12_384', num_classes=1, pretrained=False)\n",
        "\n",
        "# GPU使用する場合\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "base_model = base_model.to(device)\n",
        "\n",
        "# 学習済みモデル読み込み\n",
        "model_path = 'model_20220903.pth'\n",
        "#model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "base_model.load_state_dict(torch.load(model_path))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUb4IqQktGqP",
        "outputId": "10164681-40c5-4c44-c904-878f856b378d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\ykita\\AppData\\Local\\Temp\\ipykernel_21812\\2652723574.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import timm\n",
        "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
        "\n",
        "# CUDA設定\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "class FundusDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_dir, transform=None, use_tabular=True, tabular_features=None):\n",
        "        self.data = dataframe.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.use_tabular = use_tabular\n",
        "        self.tabular_features = tabular_features\n",
        "\n",
        "        if self.use_tabular and self.tabular_features is not None:\n",
        "            self.tabular_data = self.data[self.tabular_features].values.astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        label = self.data.iloc[idx, 8]  # METSカラム\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.use_tabular and self.tabular_features is not None:\n",
        "            tabular = torch.tensor(self.tabular_data[idx])\n",
        "        else:\n",
        "            tabular = None\n",
        "\n",
        "        return image, label, tabular\n",
        "\n",
        "\n",
        "class FundusModel(nn.Module):\n",
        "    def __init__(self, image_model, tabular_dim=7, num_classes=1, tabular_dropout_rate=0.3, final_dropout_rate=0.3):\n",
        "        super(FundusModel, self).__init__()\n",
        "        self.image_model = image_model\n",
        "        in_features = self.image_model.head.in_features\n",
        "\n",
        "        # タブラーブランチ（段階的にドロップアウト率を変更）\n",
        "        self.tabular_fc = nn.Sequential(\n",
        "            nn.Linear(tabular_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(tabular_dropout_rate)  # この値を段階的に上げていく\n",
        "        )\n",
        "\n",
        "        # 最終結合層（固定のドロップアウト率）\n",
        "        self.final_fc = nn.Sequential(\n",
        "            nn.Linear(in_features + 128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(final_dropout_rate),  # この値は0.3で固定\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, tabular=None):\n",
        "        img_features = self.image_model.forward_features(image)\n",
        "        img_features = self.image_model.global_pool(img_features)\n",
        "        img_features = img_features.view(img_features.size(0), -1)\n",
        "\n",
        "        if tabular is not None:\n",
        "            tab_features = self.tabular_fc(tabular)\n",
        "            combined = torch.cat((img_features, tab_features), dim=1)\n",
        "        else:\n",
        "            combined = torch.cat((img_features, torch.zeros(img_features.size(0), 128).to(image.device)), dim=1)\n",
        "\n",
        "        out = self.final_fc(combined)\n",
        "        return out\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.best_model = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_model = copy.deepcopy(model.state_dict())\n",
        "            return True\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_model = copy.deepcopy(model.state_dict())\n",
        "            self.counter = 0\n",
        "            return True\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "            return False\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, device, scaler):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    for inputs, targets, tabular in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device).float().unsqueeze(1)\n",
        "        if tabular is not None:\n",
        "            tabular = tabular.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(inputs, tabular).squeeze(1)\n",
        "            loss = criterion(outputs, targets.squeeze(1))\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "        all_predictions.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
        "\n",
        "    metrics = calculate_metrics(all_targets, all_predictions)\n",
        "    metrics['loss'] = running_loss / len(train_loader)\n",
        "    return metrics\n",
        "\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets, tabular in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device).float().unsqueeze(1)\n",
        "            if tabular is not None:\n",
        "                tabular = tabular.to(device)\n",
        "\n",
        "            outputs = model(inputs, tabular).squeeze(1)\n",
        "            loss = criterion(outputs, targets.squeeze(1))\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_predictions.extend(torch.sigmoid(outputs).cpu().numpy())\n",
        "\n",
        "    metrics = calculate_metrics(all_targets, all_predictions)\n",
        "    metrics['loss'] = running_loss / len(val_loader)\n",
        "    return metrics\n",
        "\n",
        "def calculate_metrics(all_targets, all_predictions):\n",
        "    all_preds_binary = [1 if p >= 0.5 else 0 for p in all_predictions]\n",
        "    return {\n",
        "        'accuracy': accuracy_score(all_targets, all_preds_binary),\n",
        "        'precision': precision_score(all_targets, all_preds_binary, zero_division=0),\n",
        "        'recall': recall_score(all_targets, all_preds_binary, zero_division=0),\n",
        "        'f1': f1_score(all_targets, all_preds_binary, zero_division=0),\n",
        "        'auc': roc_auc_score(all_targets, all_predictions)\n",
        "    }\n",
        "\n",
        "\n",
        "def main():\n",
        "    # シード固定\n",
        "    seed_everything(42)\n",
        "\n",
        "    # 保存先ディレクトリの設定と作成\n",
        "    save_dir = r\"C:\\Users\\ykita\\Metabo2024\\tabular_prediction\"\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "        print(f\"Created directory: {save_dir}\")\n",
        "\n",
        "    # 元のデータの読み込み\n",
        "    original_csv_path = \"label_train.csv\"\n",
        "    original_df = pd.read_csv(original_csv_path)\n",
        "    print(f\"Original dataset size: {len(original_df)}\")\n",
        "\n",
        "    # 疾患データの読み込み\n",
        "    disease_csv_path = 'metabo_disease.csv'\n",
        "    disease_df = pd.read_csv(disease_csv_path)\n",
        "\n",
        "    # 除外する疾患の定義\n",
        "    exclude_conditions = [\n",
        "        'AH', 'Blur', 'ERM', \"Hemorrhage\", \"Coagulation\",\n",
        "        \"VO\", \"Degeneration\", \"AMD\", \"CRA\", \"Drusen\"\n",
        "    ]\n",
        "\n",
        "    # 除外するケースの抽出\n",
        "    exclude_df = disease_df[disease_df['reason'].isin(exclude_conditions)]\n",
        "    ah_blur_ids = exclude_df['id'].tolist()\n",
        "\n",
        "    # クリーニング済みデータフレームの作成\n",
        "    cleaned_df = original_df[~original_df['filename'].isin(ah_blur_ids)]\n",
        "    print(f\"Cleaned dataset size: {len(cleaned_df)}\")\n",
        "\n",
        "    # 表形式特徴量の定義\n",
        "    tabular_features = ['age', 'AC', 'SBP', 'DBP', 'HDLC', 'TG', 'BS']\n",
        "\n",
        "    # データの分割\n",
        "    train_df, val_df = train_test_split(\n",
        "        cleaned_df,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=cleaned_df['METS']\n",
        "    )\n",
        "    print(f\"Training set size: {len(train_df)}\")\n",
        "    print(f\"Validation set size: {len(val_df)}\")\n",
        "\n",
        "    # 表形式データの標準化\n",
        "    scaler = StandardScaler()\n",
        "    train_df[tabular_features] = scaler.fit_transform(train_df[tabular_features])\n",
        "    val_df[tabular_features] = scaler.transform(val_df[tabular_features])\n",
        "\n",
        "    # データ拡張の定義\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((384, 384)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((384, 384)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # データセットの作成\n",
        "    train_dataset = FundusDataset(\n",
        "        dataframe=train_df,\n",
        "        img_dir='cropped_images',\n",
        "        transform=train_transform,\n",
        "        tabular_features=tabular_features\n",
        "    )\n",
        "\n",
        "    val_dataset = FundusDataset(\n",
        "        dataframe=val_df,\n",
        "        img_dir='cropped_images',\n",
        "        transform=val_transform,\n",
        "        tabular_features=tabular_features\n",
        "    )\n",
        "\n",
        "    # データローダーの作成\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # 損失関数の定義\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Gradient scaler for mixed precision training\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    # ベースモデルの定義\n",
        "    base_model = timm.create_model(\n",
        "        model_name='swin_base_patch4_window12_384',\n",
        "        num_classes=1,\n",
        "        pretrained=False\n",
        "    )\n",
        "\n",
        "    # GPU使用する場合\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    base_model = base_model.to(device)\n",
        "\n",
        "    # 学習済みモデル読み込み\n",
        "    model_path = 'model_20220903.pth'\n",
        "    base_model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # フェーズの設定\n",
        "    phases = [\n",
        "        {\"name\": \"Phase1\", \"tabular_dropout\": 0.3, \"epochs\": 200},\n",
        "        {\"name\": \"Phase2\", \"tabular_dropout\": 0.5, \"epochs\": 200},\n",
        "        {\"name\": \"Phase3\", \"tabular_dropout\": 0.7, \"epochs\": 200},\n",
        "        {\"name\": \"Phase4\", \"tabular_dropout\": 0.8, \"epochs\": 200},\n",
        "        {\"name\": \"Phase5\", \"tabular_dropout\": 0.9, \"epochs\": 200},\n",
        "        {\"name\": \"Phase6\", \"tabular_dropout\": 0.95, \"epochs\": 200},\n",
        "        {\"name\": \"Phase7\", \"tabular_dropout\": 1.0, \"epochs\": 200}\n",
        "    ]\n",
        "\n",
        "    FINAL_DROPOUT_RATE = 0.3\n",
        "    base_metrics = None\n",
        "\n",
        "    # メトリクスの記録用ファイル\n",
        "    metrics_file = os.path.join(save_dir, \"training_metrics.txt\")\n",
        "    with open(metrics_file, 'w') as f:\n",
        "        f.write(\"Training started at: \" + time.strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\\n\")\n",
        "        f.write(f\"Original dataset size: {len(original_df)}\\n\")\n",
        "        f.write(f\"Cleaned dataset size: {len(cleaned_df)}\\n\")\n",
        "        f.write(f\"Training set size: {len(train_df)}\\n\")\n",
        "        f.write(f\"Validation set size: {len(val_df)}\\n\")\n",
        "        f.write(f\"Excluded conditions: {', '.join(exclude_conditions)}\\n\\n\")\n",
        "\n",
        "    # フェーズごとのトレーニング\n",
        "    for phase in phases:\n",
        "        print(f\"\\nStarting {phase['name']} with tabular dropout rate {phase['tabular_dropout']}\")\n",
        "        print(f\"Max epochs: {phase['epochs']}, Early stopping patience: 10\")\n",
        "\n",
        "        # モデルの初期化\n",
        "        model = FundusModel(\n",
        "            base_model,\n",
        "            tabular_dim=len(tabular_features),\n",
        "            tabular_dropout_rate=phase['tabular_dropout'],\n",
        "            final_dropout_rate=FINAL_DROPOUT_RATE\n",
        "        ).to(device)\n",
        "\n",
        "        # 前のフェーズの重みがある場合はロード\n",
        "        if phase != phases[0]:\n",
        "            prev_model_path = os.path.join(save_dir, f'fundus_model_{phases[phases.index(phase)-1][\"name\"]}.pth')\n",
        "            if os.path.exists(prev_model_path):\n",
        "                model.load_state_dict(torch.load(prev_model_path))\n",
        "                print(f\"Loaded weights from: {prev_model_path}\")\n",
        "\n",
        "        # トレーニング設定\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "        scheduler = CosineLRScheduler(\n",
        "            optimizer,\n",
        "            t_initial=phase['epochs'],\n",
        "            lr_min=1e-6,\n",
        "            warmup_t=5,\n",
        "            warmup_lr_init=1e-7,\n",
        "            warmup_prefix=True\n",
        "        )\n",
        "\n",
        "        early_stopping = EarlyStopping(patience=10, min_delta=1e-4)\n",
        "\n",
        "        # 現在のフェーズの保存パス\n",
        "        save_path = os.path.join(save_dir, f'fundus_model_{phase[\"name\"]}.pth')\n",
        "\n",
        "        # フェーズごとのトレーニング実行\n",
        "        metrics = train_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            scaler=scaler,\n",
        "            early_stopping=early_stopping,\n",
        "            num_epochs=phase['epochs'],\n",
        "            device=device,\n",
        "            save_path=save_path,\n",
        "            phase_name=phase['name']\n",
        "        )\n",
        "\n",
        "        # メトリクスの記録\n",
        "        if base_metrics is None:\n",
        "            base_metrics = metrics\n",
        "\n",
        "        # メトリクスをファイルに保存\n",
        "        with open(metrics_file, 'a') as f:\n",
        "            f.write(f\"\\n{phase['name']} Results:\\n\")\n",
        "            f.write(f\"Tabular Dropout Rate: {phase['tabular_dropout']}\\n\")\n",
        "            f.write(f\"Validation AUC: {metrics['auc']:.4f}\\n\")\n",
        "            f.write(f\"Relative to base phase: {(metrics['auc']/base_metrics['auc']-1)*100:.2f}% change\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "        print(f\"\\n{phase['name']} Final Results:\")\n",
        "        print(f\"Validation AUC: {metrics['auc']:.4f}\")\n",
        "        print(f\"Relative to base phase: {(metrics['auc']/base_metrics['auc']-1)*100:.2f}% change\")\n",
        "        print(f\"Results saved to: {metrics_file}\")\n",
        "        print(f\"Model saved to: {save_path}\")\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "                scaler, early_stopping, num_epochs, device, save_path, phase_name):\n",
        "    best_epoch = 0\n",
        "    history = {\n",
        "        'train_loss': [], 'train_metrics': [],\n",
        "        'val_loss': [], 'val_metrics': [],\n",
        "        'lr': []\n",
        "    }\n",
        "\n",
        "    # トレーニング履歴の保存先\n",
        "    history_file = os.path.join(os.path.dirname(save_path), f'history_{phase_name}.txt')\n",
        "\n",
        "    print(f\"\\nStarting training for {phase_name}\")\n",
        "    print(f\"Maximum epochs: {num_epochs}\")\n",
        "    print(f\"Early stopping patience: {early_stopping.patience}\")\n",
        "\n",
        "    with open(history_file, 'w') as f:\n",
        "        f.write(f\"Training history for {phase_name}\\n\")\n",
        "        f.write(\"Epoch,Train Loss,Train AUC,Val Loss,Val AUC,Learning Rate\\n\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # トレーニング\n",
        "        train_metrics = train(model, train_loader, criterion, optimizer, device, scaler)\n",
        "        val_metrics = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        scheduler.step(epoch + 1)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # 結果の記録\n",
        "        history['train_loss'].append(train_metrics['loss'])\n",
        "        history['train_metrics'].append(train_metrics)\n",
        "        history['val_loss'].append(val_metrics['loss'])\n",
        "        history['val_metrics'].append(val_metrics)\n",
        "        history['lr'].append(current_lr)\n",
        "\n",
        "        # エポックごとの結果をファイルに保存\n",
        "        with open(history_file, 'a') as f:\n",
        "            f.write(f\"{epoch+1},{train_metrics['loss']:.4f},{train_metrics['auc']:.4f},\"\n",
        "                   f\"{val_metrics['loss']:.4f},{val_metrics['auc']:.4f},{current_lr:.2e}\\n\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train - Loss: {train_metrics['loss']:.4f}, AUC: {train_metrics['auc']:.4f}\")\n",
        "        print(f\"Val - Loss: {val_metrics['loss']:.4f}, AUC: {val_metrics['auc']:.4f}\")\n",
        "        print(f\"Learning Rate: {current_lr:.2e}\")\n",
        "\n",
        "        if early_stopping(val_metrics['loss'], model):\n",
        "            best_epoch = epoch + 1\n",
        "            if early_stopping.early_stop:\n",
        "                print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
        "                print(f\"Best epoch was {best_epoch}\")\n",
        "                break\n",
        "\n",
        "    # 最良モデルの保存\n",
        "    model.load_state_dict(early_stopping.best_model)\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    print(f\"\\n{phase_name} Training Completed:\")\n",
        "    print(f\"Best epoch: {best_epoch}/{epoch+1}\")\n",
        "    print(f\"Training history saved to: {history_file}\")\n",
        "    print(f\"Model saved to: {save_path}\")\n",
        "\n",
        "    return val_metrics\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHGx1nR4tpSC",
        "outputId": "e7142b78-939d-404d-ee35-1b805bccad1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset size: 5000\n",
            "Cleaned dataset size: 4618\n",
            "Training set size: 3694\n",
            "Validation set size: 924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\ykita\\AppData\\Local\\Temp\\ipykernel_21812\\3967310642.py:292: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "C:\\Users\\ykita\\AppData\\Local\\Temp\\ipykernel_21812\\3967310642.py:307: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  base_model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Phase1 with tabular dropout rate 0.3\n",
            "Max epochs: 200, Early stopping patience: 10\n",
            "\n",
            "Starting training for Phase1\n",
            "Maximum epochs: 200\n",
            "Early stopping patience: 10\n"
          ]
        }
      ]
    }
  ]
}