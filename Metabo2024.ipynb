{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOqU5YG2lXz9v5fo+LX7/Ts",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c8943b556dd54f4091e927365df42c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea13308e0de041a8956a040f892f3b2e",
              "IPY_MODEL_a8a358abb4cf40cf939d1bb5bbefd8bb",
              "IPY_MODEL_a1f6eadcfa9f4bad86931d72b8f93315"
            ],
            "layout": "IPY_MODEL_af079f659aba438c82bff95ca9f07cb7"
          }
        },
        "ea13308e0de041a8956a040f892f3b2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a5b21b2e30142fcb42aa471d2da75c4",
            "placeholder": "​",
            "style": "IPY_MODEL_abcd783d8c34426eba410c3a6005d9b3",
            "value": "model.safetensors: 100%"
          }
        },
        "a8a358abb4cf40cf939d1bb5bbefd8bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54a702c7e6074ed0a81013d1b58df0d2",
            "max": 36565360,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_039d91ee8d6341d8b53281f07cd7798b",
            "value": 36565360
          }
        },
        "a1f6eadcfa9f4bad86931d72b8f93315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5058bf3472647088fcf055f78997a0b",
            "placeholder": "​",
            "style": "IPY_MODEL_b0128a0e9386420ca6c8777107525209",
            "value": " 36.6M/36.6M [00:00&lt;00:00, 147MB/s]"
          }
        },
        "af079f659aba438c82bff95ca9f07cb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a5b21b2e30142fcb42aa471d2da75c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abcd783d8c34426eba410c3a6005d9b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54a702c7e6074ed0a81013d1b58df0d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "039d91ee8d6341d8b53281f07cd7798b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5058bf3472647088fcf055f78997a0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0128a0e9386420ca6c8777107525209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41a6b16c207c421dbe8f1a093ee0d6a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_afb8f1a9c75e48b0a7399a50462ce163",
              "IPY_MODEL_5f70dcc1babd403395e0f8ceaf1a15f3",
              "IPY_MODEL_d824bf5c9ea248d1ac195b5bbf015424"
            ],
            "layout": "IPY_MODEL_1e4d9377e8da48978c046456e24a978f"
          }
        },
        "afb8f1a9c75e48b0a7399a50462ce163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b5d99bdbde14725b89847f4576f75dd",
            "placeholder": "​",
            "style": "IPY_MODEL_95a4f6cb9d614395952b4708850a763d",
            "value": "model.safetensors: 100%"
          }
        },
        "5f70dcc1babd403395e0f8ceaf1a15f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e1ddc70fe4a4c4fb0a1114aa7ed2bd4",
            "max": 36565360,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58a6aabd14a242dda9b12464de68182e",
            "value": 36565360
          }
        },
        "d824bf5c9ea248d1ac195b5bbf015424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4567ba105cac4f09996084c30fef170b",
            "placeholder": "​",
            "style": "IPY_MODEL_7ecc2cc205cc48ceb663d30c15942aaa",
            "value": " 36.6M/36.6M [00:00&lt;00:00, 150MB/s]"
          }
        },
        "1e4d9377e8da48978c046456e24a978f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b5d99bdbde14725b89847f4576f75dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95a4f6cb9d614395952b4708850a763d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e1ddc70fe4a4c4fb0a1114aa7ed2bd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58a6aabd14a242dda9b12464de68182e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4567ba105cac4f09996084c30fef170b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ecc2cc205cc48ceb663d30c15942aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46561741cddf4eb7a1b9042d7c887d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_858d23e287814f2991c2e7ca44925452",
              "IPY_MODEL_f6217a4337624bffb6bed385446c303d",
              "IPY_MODEL_a9b49a9b32b6420398904aebe1d91c4c"
            ],
            "layout": "IPY_MODEL_fcd403d437a94c68be3dfcd5ad9b59d9"
          }
        },
        "858d23e287814f2991c2e7ca44925452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f010b2687b54081a79559943fd2ca77",
            "placeholder": "​",
            "style": "IPY_MODEL_fa169862523040128d2941b47d6f996c",
            "value": "model.safetensors: 100%"
          }
        },
        "f6217a4337624bffb6bed385446c303d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36f5403463fe48ca950a9e8234febbdb",
            "max": 36565360,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc4d77565bea41f7b3aecd28cdca39da",
            "value": 36565360
          }
        },
        "a9b49a9b32b6420398904aebe1d91c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c98d465730724b9da4ba12c3f85e429d",
            "placeholder": "​",
            "style": "IPY_MODEL_e1eceebcd3a04eed987542144de3b258",
            "value": " 36.6M/36.6M [00:00&lt;00:00, 328MB/s]"
          }
        },
        "fcd403d437a94c68be3dfcd5ad9b59d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f010b2687b54081a79559943fd2ca77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa169862523040128d2941b47d6f996c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36f5403463fe48ca950a9e8234febbdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc4d77565bea41f7b3aecd28cdca39da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c98d465730724b9da4ba12c3f85e429d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1eceebcd3a04eed987542144de3b258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e66d32f45e35499989cb26af38307458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c35dc9cdadcd4753b916b94ba336f0bd",
              "IPY_MODEL_eb93b7de2172480683814627a1d2ab3c",
              "IPY_MODEL_ff45d1dc4dfa422d96c995fa9877b17d"
            ],
            "layout": "IPY_MODEL_48cae90ad5b346e6bc90dbeb818be5f9"
          }
        },
        "c35dc9cdadcd4753b916b94ba336f0bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b09ff0b6326a4d4685fb75792a691eb5",
            "placeholder": "​",
            "style": "IPY_MODEL_b184520123734ba5bbe0959c4d9944fe",
            "value": "model.safetensors: 100%"
          }
        },
        "eb93b7de2172480683814627a1d2ab3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1390cb797d64458abdeef8d840ee117b",
            "max": 214290028,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d35ea8d06fef43e58e5b383c46ca9274",
            "value": 214290028
          }
        },
        "ff45d1dc4dfa422d96c995fa9877b17d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f02d2c5cd064c018af06347190ca0db",
            "placeholder": "​",
            "style": "IPY_MODEL_f0a78e5a0f0543bf96eab3bed5cb8638",
            "value": " 214M/214M [00:05&lt;00:00, 39.5MB/s]"
          }
        },
        "48cae90ad5b346e6bc90dbeb818be5f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b09ff0b6326a4d4685fb75792a691eb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b184520123734ba5bbe0959c4d9944fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1390cb797d64458abdeef8d840ee117b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d35ea8d06fef43e58e5b383c46ca9274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f02d2c5cd064c018af06347190ca0db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0a78e5a0f0543bf96eab3bed5cb8638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/FundusPhoto/blob/main/Metabo2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Fundus_metabolic**"
      ],
      "metadata": {
        "id": "AHVcR-yjnL-8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFuOfjRGnI3m",
        "outputId": "25389eb6-5b9d-4eae-9de1-2ab2ea020033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Mount Google Drive if the CSV is in your drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**元データの分析**"
      ],
      "metadata": {
        "id": "ax3Car0Lncyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file (adjust the path if necessary)\n",
        "file_path = '/content/drive/MyDrive/Deep_learning/Fundus_metabolic/label_train.csv'  # Adjust this to your actual file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"Basic statistics of the dataset:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Drop non-numeric columns for correlation analysis\n",
        "numeric_data = data.drop(columns=[\"filename\"])\n",
        "\n",
        "# Calculate and display the correlation matrix\n",
        "correlation_matrix = numeric_data.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "plt.title('Correlation Matrix Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hAfVIiIinVpK",
        "outputId": "815e206d-0435-4b4f-d0ad-4ac96f2d685c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic statistics of the dataset:\n",
            "               age           AC          SBP          DBP         HDLC  \\\n",
            "count  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000   \n",
            "mean     46.856800    89.660860   132.864200    82.332600    54.038600   \n",
            "std      10.634873    10.709773    15.308855    12.034939    13.889395   \n",
            "min      18.000000    58.000000    89.000000    46.000000    20.000000   \n",
            "25%      39.000000    82.800000   123.000000    74.000000    44.000000   \n",
            "50%      48.000000    89.200000   132.000000    82.000000    52.000000   \n",
            "75%      55.000000    96.000000   141.000000    90.000000    62.000000   \n",
            "max      65.000000   157.800000   219.000000   139.000000   118.000000   \n",
            "\n",
            "                TG           BS        METS  \n",
            "count  5000.000000  5000.000000  5000.00000  \n",
            "mean    174.856000    96.231000     0.50000  \n",
            "std     152.419455    26.904461     0.50005  \n",
            "min      22.000000    44.000000     0.00000  \n",
            "25%      86.000000    83.000000     0.00000  \n",
            "50%     149.000000    88.000000     0.50000  \n",
            "75%     210.000000    99.000000     1.00000  \n",
            "max    2397.000000   385.000000     1.00000  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boostingを使用"
      ],
      "metadata": {
        "id": "WxvQdd2sToij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####\n",
        "# 他のデータからGradient Boostingを用いてMETSを予測すると、精度は99%以上とかなり高い\n",
        "####\n",
        "\n",
        "# Prepare the data for training (remove the filename and target column)\n",
        "X = numeric_data.drop(columns=[\"METS\"])\n",
        "y = numeric_data[\"METS\"]\n",
        "\n",
        "# Initialize the Gradient Boosting model\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_scores = cross_val_score(gb_model, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Display the cross-validation results\n",
        "print(\"5-Fold Cross-Validation Accuracy Scores:\", cv_scores)\n",
        "print(\"Mean Accuracy:\", cv_scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiTLTJvooQ5A",
        "outputId": "39c6b57a-7e4b-423b-99d5-56cc9afaabec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-Fold Cross-Validation Accuracy Scores: [0.993 0.991 1.    1.    1.   ]\n",
            "Mean Accuracy: 0.9968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########\n",
        "# 重要なTOP3指標を抜き出してみる --> AC（腹囲), TG（中性脂肪）, BS（空腹時血糖）\n",
        "#########\n",
        "\n",
        "# Train the Gradient Boosting model\n",
        "gb_model.fit(X, y)\n",
        "\n",
        "# Get feature importances from the Gradient Boosting model\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': gb_model.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display the feature importances\n",
        "print(\"Feature Importances for Gradient Boosting:\")\n",
        "print(feature_importances)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDcSrcYBorVV",
        "outputId": "5ffe5770-0ebb-495a-ac28-ad24fa847501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances for Gradient Boosting:\n",
            "  Feature  Importance\n",
            "1      AC    0.432137\n",
            "5      TG    0.242165\n",
            "6      BS    0.147832\n",
            "2     SBP    0.093333\n",
            "4    HDLC    0.046051\n",
            "3     DBP    0.038421\n",
            "0     age    0.000062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# このスクリプトは、メタボリックシンドローム（METS）の予測のために\n",
        "# 各健康指標のカットオフ値を決定し、それに基づいてデータを2値化します。\n",
        "#\n",
        "# 主な手順：\n",
        "# 1. Google Driveからデータを読み込む\n",
        "# 2. 各健康指標（年齢、腹囲、血圧など）に対して以下の処理を行う：\n",
        "#    a. ROC曲線を計算（HDLCの場合は値を反転）\n",
        "#    b. Youden index（感度+特異度-1）を計算\n",
        "#    c. Youden indexが最大となるポイントをカットオフ値として決定\n",
        "#    d. カットオフ値を基準にデータを2値化（HDLCは逆基準）\n",
        "# 3. 各指標のROC曲線をプロット\n",
        "# 4. 結果（カットオフ値、感度、特異度、Youden index、高低の症例数）を表示\n",
        "# 5. Youden indexの比較グラフを作成\n",
        "# 6. カットオフ値周りの症例分布を棒グラフで表示\n",
        "# 7. 2値化したデータを新しいデータフレームとして作成（filename, METSを含む）\n",
        "# 8. 2値化したデータをCSVファイルとして保存\n",
        "#\n",
        "# 注意点：\n",
        "# - HDLCは他の指標と逆の関係（高いほど良い）であるため、特別な処理を行っています。\n",
        "# - 2値化の基準は、カットオフ値より大きい場合を1、それ以外を0としています（HDLCは逆）。\n",
        "# - 結果のCSVファイルは元のファイルと同じディレクトリに保存されます。\n",
        "# - filenameとMETSは元のデータからそのまま引き継がれます。\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Google Driveからデータの読み込み\n",
        "data = pd.read_csv('/content/drive/MyDrive/Deep_learning/Fundus_metabolic/label_train.csv')\n",
        "\n",
        "# 分析対象の列\n",
        "features = ['age', 'AC', 'SBP', 'DBP', 'HDLC', 'TG', 'BS']\n",
        "\n",
        "# 結果を格納するリスト\n",
        "results_list = []\n",
        "\n",
        "# 2値化したデータを格納するデータフレーム\n",
        "binary_data = pd.DataFrame()\n",
        "\n",
        "# filenameとMETSを追加\n",
        "binary_data['filename'] = data['filename']\n",
        "binary_data['METS'] = data['METS']\n",
        "\n",
        "# 各特徴量に対して解析を行う\n",
        "for feature in features:\n",
        "    # HDLCの場合は値を反転させる（高値が望ましいため）\n",
        "    if feature == 'HDLC':\n",
        "        feature_values = -data[feature]\n",
        "    else:\n",
        "        feature_values = data[feature]\n",
        "\n",
        "    # ROC曲線の計算\n",
        "    fpr, tpr, thresholds = roc_curve(data['METS'], feature_values)\n",
        "\n",
        "    # Youden indexの計算\n",
        "    youden_index = tpr - fpr\n",
        "\n",
        "    # 最適なカットオフ値の決定\n",
        "    optimal_idx = np.argmax(youden_index)\n",
        "    optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "    # HDLCの場合はカットオフ値を元の尺度に戻す\n",
        "    if feature == 'HDLC':\n",
        "        optimal_threshold = -optimal_threshold\n",
        "        # HDLCの場合、\"高い\"と\"低い\"の意味が逆になる\n",
        "        high_count = (data[feature] <= optimal_threshold).sum()\n",
        "        low_count = (data[feature] > optimal_threshold).sum()\n",
        "        # 2値化（HDLCは逆の基準で2値化）\n",
        "        binary_data[f'{feature}_binary'] = (data[feature] <= optimal_threshold).astype(int)\n",
        "    else:\n",
        "        high_count = (data[feature] > optimal_threshold).sum()\n",
        "        low_count = (data[feature] <= optimal_threshold).sum()\n",
        "        # 2値化\n",
        "        binary_data[f'{feature}_binary'] = (data[feature] > optimal_threshold).astype(int)\n",
        "\n",
        "    # 結果の保存\n",
        "    results_list.append({\n",
        "        'Feature': feature,\n",
        "        'Cutoff': optimal_threshold,\n",
        "        'Sensitivity': tpr[optimal_idx],\n",
        "        'Specificity': 1 - fpr[optimal_idx],\n",
        "        'Youden Index': youden_index[optimal_idx],\n",
        "        'High Count': high_count,\n",
        "        'Low Count': low_count\n",
        "    })\n",
        "\n",
        "    # # ROC曲線のプロット\n",
        "    # plt.figure(figsize=(8, 6))\n",
        "    # plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc_score(data[\"METS\"], feature_values):.2f})')\n",
        "    # plt.plot([0, 1], [0, 1], 'k--')\n",
        "    # plt.xlim([0.0, 1.0])\n",
        "    # plt.ylim([0.0, 1.05])\n",
        "    # plt.xlabel('False Positive Rate')\n",
        "    # plt.ylabel('True Positive Rate')\n",
        "    # plt.title(f'ROC Curve for {feature}')\n",
        "    # plt.legend(loc=\"lower right\")\n",
        "    # plt.show()\n",
        "\n",
        "# 結果をデータフレームに変換\n",
        "results = pd.DataFrame(results_list)\n",
        "\n",
        "# 結果の表示\n",
        "print(results.to_string(index=False))\n",
        "\n",
        "# # 結果のプロット\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.bar(results['Feature'], results['Youden Index'])\n",
        "# plt.title('Youden Index for Each Feature')\n",
        "# plt.xlabel('Feature')\n",
        "# plt.ylabel('Youden Index')\n",
        "# plt.xticks(rotation=45)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# # 症例数の比較プロット\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# x = np.arange(len(features))\n",
        "# width = 0.35\n",
        "# plt.bar(x - width/2, results['High Count'], width, label='High')\n",
        "# plt.bar(x + width/2, results['Low Count'], width, label='Low')\n",
        "# plt.xlabel('Features')\n",
        "# plt.ylabel('Count')\n",
        "# plt.title('Distribution of Cases Around Cutoff')\n",
        "# plt.xticks(x, features, rotation=45)\n",
        "# plt.legend()\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# 2値化したデータフレームの先頭数行を表示\n",
        "print(\"\\n2値化したデータフレーム（先頭10行）:\")\n",
        "print(binary_data.head(10))\n",
        "\n",
        "# 2値化したデータフレームをCSVファイルとして保存\n",
        "binary_data.to_csv('/content/drive/MyDrive/Deep_learning/Fundus_metabolic/binary_features.csv', index=False)\n",
        "print(\"\\n2値化したデータをCSVファイルとして保存しました: '/content/drive/MyDrive/Deep_learning/Fundus_metabolic/binary_features.csv'\")"
      ],
      "metadata": {
        "id": "4OGfb_ZHCCrx",
        "outputId": "15499d12-8a30-473a-83a1-46ea5bbc6260",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature  Cutoff  Sensitivity  Specificity  Youden Index  High Count  Low Count\n",
            "    age    46.0       0.6972       0.5200        0.2172        2781       2219\n",
            "     AC    85.5       0.9844       0.6468        0.6312        3320       1680\n",
            "    SBP   130.0       0.8644       0.7040        0.5684        2715       2285\n",
            "    DBP    80.0       0.8212       0.6432        0.4644        2788       2212\n",
            "   HDLC    55.0       0.7648       0.5624        0.3272        3006       1994\n",
            "     TG   150.0       0.8316       0.8332        0.6648        2464       2536\n",
            "     BS    94.0       0.5088       0.8220        0.3308        1604       3396\n",
            "\n",
            "2値化したデータフレーム（先頭10行）:\n",
            "                filename  METS  age_binary  AC_binary  SBP_binary  DBP_binary  \\\n",
            "0  img75151275_00_1R.jpg     1           1          1           1           1   \n",
            "1  img96940480_00_1R.jpg     0           1          0           1           1   \n",
            "2  img15900654_00_1R.jpg     1           1          1           0           1   \n",
            "3  img74960801_00_1R.jpg     0           1          0           1           1   \n",
            "4  img37629509_00_1R.jpg     0           1          1           1           0   \n",
            "5  img44396554_00_1R.jpg     1           1          1           1           1   \n",
            "6  img26138665_00_1R.jpg     1           1          1           1           0   \n",
            "7  img54594181_00_1R.jpg     1           1          1           0           1   \n",
            "8  img26028628_00_1R.jpg     0           1          1           0           1   \n",
            "9  img83790287_00_1R.jpg     0           1          0           0           0   \n",
            "\n",
            "   HDLC_binary  TG_binary  BS_binary  \n",
            "0            0          1          1  \n",
            "1            0          0          0  \n",
            "2            1          1          1  \n",
            "3            0          0          1  \n",
            "4            1          0          0  \n",
            "5            1          1          1  \n",
            "6            0          1          1  \n",
            "7            1          1          1  \n",
            "8            1          1          1  \n",
            "9            0          0          1  \n",
            "\n",
            "2値化したデータをCSVファイルとして保存しました: '/content/drive/MyDrive/Deep_learning/Fundus_metabolic/binary_features.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NGBoostを使用"
      ],
      "metadata": {
        "id": "XpyZI8fVTteW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####\n",
        "# あとでCNNと合体することを想定し、NGBoostを使う\n",
        "#####\n",
        "!pip install ngboost\n",
        "from ngboost import NGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the data for training (remove the filename and target column)\n",
        "X = numeric_data.drop(columns=[\"METS\"])\n",
        "y = numeric_data[\"METS\"]\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'minibatch_frac': [1.0, 0.8, 0.6],\n",
        "    'col_sample': [1.0, 0.8, 0.6],\n",
        "}\n",
        "\n",
        "# Initialize the NGBoost model\n",
        "ngb_model = NGBClassifier(random_state=42)\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(estimator=ngb_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Get the best parameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Perform 5-fold cross-validation with the best model\n",
        "cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Display the results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"5-Fold Cross-Validation Accuracy Scores:\", cv_scores)\n",
        "print(\"Mean Accuracy:\", cv_scores.mean())\n",
        "print(\"Standard Deviation:\", cv_scores.std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAXjehHQRPox",
        "outputId": "68430e86-d8e7-4eca-b8b3-14bc12b573b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[iter 0] loss=0.6931 val_loss=0.0000 scale=8.0000 norm=16.0000\n",
            "[iter 100] loss=0.0206 val_loss=0.0000 scale=1.0000 norm=1.0232\n",
            "[iter 0] loss=0.6909 val_loss=0.0000 scale=8.0000 norm=16.0000\n",
            "[iter 100] loss=0.0149 val_loss=0.0000 scale=1.0000 norm=1.0166\n",
            "[iter 0] loss=0.6924 val_loss=0.0000 scale=16.0000 norm=32.0000\n",
            "[iter 100] loss=0.0160 val_loss=0.0000 scale=0.2500 norm=0.2544\n",
            "[iter 0] loss=0.6928 val_loss=0.0000 scale=8.0000 norm=16.0000\n",
            "[iter 100] loss=0.0178 val_loss=0.0000 scale=0.2500 norm=0.2549\n",
            "[iter 0] loss=0.6929 val_loss=0.0000 scale=8.0000 norm=16.0000\n",
            "[iter 100] loss=0.0205 val_loss=0.0000 scale=0.5000 norm=0.5115\n",
            "[iter 0] loss=0.6869 val_loss=0.0000 scale=16.0000 norm=32.0000\n",
            "[iter 100] loss=0.0216 val_loss=0.0000 scale=0.0625 norm=0.0640\n",
            "Best Parameters: {'col_sample': 0.8, 'learning_rate': 0.05, 'minibatch_frac': 1.0, 'n_estimators': 200}\n",
            "5-Fold Cross-Validation Accuracy Scores: [1.    0.996 1.    1.    1.   ]\n",
            "Mean Accuracy: 0.9992000000000001\n",
            "Standard Deviation: 0.0016000000000000012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## NGBooostを用いた場合の関連因子\n",
        "\n",
        "from ngboost import NGBClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prepare the data\n",
        "X = numeric_data.drop(columns=[\"METS\"])\n",
        "y = numeric_data[\"METS\"]\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_model = NGBClassifier(col_sample=0.8, learning_rate=0.05, minibatch_frac=1.0, n_estimators=200, random_state=42)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Perform permutation importance\n",
        "result = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=42)\n",
        "\n",
        "# Sort features by importance\n",
        "feature_importance = pd.DataFrame({'feature': X.columns, 'importance': result.importances_mean})\n",
        "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
        "\n",
        "# Display the top 10 most important features\n",
        "print(\"Top 10 most important features:\")\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(feature_importance['feature'][:10], feature_importance['importance'][:10])\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Top 10 Feature Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IkD_pgtMS6a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TOP3だけで予測すると物足りない\n",
        "# Select only the top 3 features: AC, TG, BS\n",
        "X_top3 = data[['AC', 'TG', 'BS']]\n",
        "\n",
        "# Perform 5-fold cross-validation using only these 3 features with Gradient Boosting\n",
        "cv_scores_top3 = cross_val_score(gb_model, X_top3, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Display the cross-validation results\n",
        "cv_scores_top3, cv_scores_top3.mean()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh1Ig09wpICH",
        "outputId": "6b1fa371-5b7e-4239-add0-aa6222844cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.939, 0.918, 0.951, 0.945, 0.932]), 0.9369999999999999)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### TOP1の腹囲をDropしてもそれなりに結果は良い\n",
        "# Drop the AC feature\n",
        "X_drop_AC = data.drop(columns=[\"filename\", \"METS\", \"AC\"])\n",
        "\n",
        "# Perform 5-fold cross-validation without AC using Gradient Boosting\n",
        "cv_scores_drop_AC = cross_val_score(gb_model, X_drop_AC, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Display the cross-validation results\n",
        "cv_scores_drop_AC, cv_scores_drop_AC.mean()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VVnqajcrFlQ",
        "outputId": "5c14bcbf-7d97-406c-8755-406a612a9f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.947, 0.945, 0.956, 0.955, 0.614]), 0.8834)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "## binary dataを用いたlogistic線形回帰\n",
        "####################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# CSVの読み込み\n",
        "file_path = '/content/drive/MyDrive/Deep_learning/Fundus_metabolic/binary_features.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 特徴量とターゲット変数を定義\n",
        "X = df[['age_binary', 'AC_binary', 'SBP_binary', 'DBP_binary', 'HDLC_binary', 'TG_binary', 'BS_binary']]\n",
        "y = df['METS']\n",
        "\n",
        "# トレーニングデータとテストデータに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ロジスティック回帰モデルの初期化\n",
        "log_model = LogisticRegression()\n",
        "\n",
        "# モデルを訓練データでフィッティング\n",
        "log_model.fit(X_train, y_train)\n",
        "\n",
        "# テストデータで予測\n",
        "y_pred = log_model.predict(X_test)\n",
        "\n",
        "# 精度の計算\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 5-Fold Cross-Validation\n",
        "cv_scores = cross_val_score(log_model, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# 結果を表示\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"5-Fold Cross-Validation Accuracy Scores: {cv_scores}\")\n",
        "print(f\"Mean Accuracy: {cv_scores.mean()}\")\n",
        "print(f\"Standard Deviation: {cv_scores.std()}\")\n"
      ],
      "metadata": {
        "id": "ujcvMrTWHbkR",
        "outputId": "64672c5d-44eb-4280-865e-0fe12cbaeadd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.888\n",
            "5-Fold Cross-Validation Accuracy Scores: [0.815 0.874 0.895 0.918 0.879]\n",
            "Mean Accuracy: 0.8762000000000001\n",
            "Standard Deviation: 0.03423098012035299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "## binary dataを用いたngboost回帰（線形とあまり変わらないが）\n",
        "####################\n",
        "\n",
        "!pip install ngboost --q\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from ngboost import NGBClassifier\n",
        "\n",
        "# CSVの読み込み\n",
        "file_path = '/content/drive/MyDrive/Deep_learning/Fundus_metabolic/binary_features.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 特徴量とターゲット変数を定義\n",
        "X = df[['age_binary', 'AC_binary', 'SBP_binary', 'DBP_binary', 'HDLC_binary', 'TG_binary', 'BS_binary']]\n",
        "y = df['METS']\n",
        "\n",
        "# トレーニングデータとテストデータに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# NGBoostモデルの初期化\n",
        "ngb = NGBClassifier()\n",
        "\n",
        "# パラメータグリッドの定義\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'minibatch_frac': [1.0, 0.8, 0.6],\n",
        "    'col_sample': [1.0, 0.8, 0.6],\n",
        "}\n",
        "\n",
        "# GridSearchCVのセットアップ (途中経過の表示に verbose=3 を使用)\n",
        "grid_search = GridSearchCV(estimator=ngb, param_grid=param_grid, cv=3, scoring='accuracy', verbose=3, n_jobs=-1)\n",
        "\n",
        "# トレーニングデータに対してグリッドサーチを実行\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 最適なパラメータを取得\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# 最適パラメータでテストデータに対して予測を行う\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# 精度の計算\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 結果を表示\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"Accuracy with best parameters: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "hG1UCfoPWTnR",
        "outputId": "eb3b483e-427e-4d6b-94ff-8b6d460ba073",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
            "[iter 0] loss=0.6931 val_loss=0.0000 scale=4.0000 norm=8.0000\n",
            "[iter 100] loss=0.2016 val_loss=0.0000 scale=1.0000 norm=1.3211\n",
            "Best parameters: {'col_sample': 0.8, 'learning_rate': 0.01, 'minibatch_frac': 0.8, 'n_estimators': 200}\n",
            "Accuracy with best parameters: 0.929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Strategy**\n",
        "\n",
        "1. 単純にbinaryでend-to-endする\n",
        "\n",
        "2. 各項目を別個に数値として回帰させて、gradient boostingで予測\n",
        "\n",
        "3. Multiple instance learning?\n",
        "\n",
        "#**気がついたこと**\n",
        "\n",
        "1. 病気の眼が混ざっている: DR, 黄斑上膜, 硝子体混濁, 白内障など。精度向上のため、Trainsetからは眼底が見えない症例は除外する。Testsetでも眼底が写っていない写真が3つぐらいあるが、これらの扱いについては後ほど考える。\n",
        "\n",
        "2. 今回はすべて右眼である\n",
        "\n",
        "3. 近視度数や白内障による白濁など、眼底の色が様々である。これらの情報はそれなりに役立ちそうなので、推測する項目によっては残しておいた方が良いような気がする。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dafzu4v7rTEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Image Preprocessing**"
      ],
      "metadata": {
        "id": "ARUuZnQtF_25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "img_path = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_train_1/img00032465_00_1R.jpg\"\n",
        "\n",
        "img = cv2.imread(img_path)\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "3rNnjp2OHDw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### simple scale down"
      ],
      "metadata": {
        "id": "vq7QD5HA2vW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 縮小のみ224px\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def crop_center_image(cv2_img, crop_size=900):\n",
        "    # 画像のサイズを取得\n",
        "    height, width, _ = cv2_img.shape\n",
        "\n",
        "    # 中心座標を計算\n",
        "    center_x = width // 2\n",
        "    center_y = height // 2\n",
        "\n",
        "    # 切り抜きの開始座標を計算\n",
        "    start_x = center_x - crop_size // 2\n",
        "    start_y = center_y - crop_size // 2\n",
        "\n",
        "    # 画像の切り抜き\n",
        "    cropped_img = cv2_img[start_y:start_y + crop_size, start_x:start_x + crop_size]\n",
        "\n",
        "    return cropped_img\n",
        "\n",
        "def resize_image(cv2_img, size=(224, 224)):\n",
        "    # リサイズ\n",
        "    resized_img = cv2.resize(cv2_img, size)\n",
        "    return resized_img\n",
        "\n",
        "# 使用例\n",
        "img = cv2.imread(img_path)  # 画像パスに合わせて指定\n",
        "cropped_img = crop_center_image(img)  # 900x900pxで切り抜き\n",
        "resized_img = resize_image(cropped_img)  # 224x224pxにリサイズ\n",
        "\n",
        "# 切り抜いてリサイズした画像の表示\n",
        "cv2_imshow(resized_img)\n"
      ],
      "metadata": {
        "id": "oWL_0mK7P_XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 輪郭強調バージョン"
      ],
      "metadata": {
        "id": "5O1pnR3a4pRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "このコードでは以下の処理を行っています：\n",
        "\n",
        "グレースケール変換: 色情報を除去し、輝度情報のみに注目します。\n",
        "CLAHE適用: コントラストを局所的に強調します。\n",
        "ガウシアンブラー: ノイズを軽減します。\n",
        "ラプラシアンフィルタ適用: エッジを検出し、血管構造を強調します。\n",
        "正規化と反転: エッジ情報を反転させ、血管を暗く表示します。\n",
        "再度のコントラスト強調: 処理された画像のコントラストを再度調整します。\n",
        "カラーマップ適用: グレースケール画像に疑似的な色をつけて可視化します。\n",
        "\n",
        "この処理により、元の画像から血管構造を強調した画像Bに近い効果を得ることができるはずです。ただし、完全に同じ結果を得るのは難しく、元の画像Bの作成に使用された正確な手法がわからないため、多少の違いは生じる可能性があります。\n",
        "このコードを実行して結果を確認し、必要に応じてパラメータ（例：CLAHEのclipLimit、ガウシアンブラーのカーネルサイズなど）を調整することで、より目的の効果に近づけることができます。\n",
        "\"\"\"\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "%matplotlib inline\n",
        "\n",
        "def download_image(url):\n",
        "    response = requests.get(url)\n",
        "    image = cv2.imdecode(np.frombuffer(response.content, np.uint8), cv2.IMREAD_COLOR)\n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def preprocess_fundus_image(image):\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Apply CLAHE for contrast enhancement\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    enhanced = clahe.apply(gray)\n",
        "\n",
        "    # Apply Gaussian blur to reduce noise\n",
        "    blurred = cv2.GaussianBlur(enhanced, (5, 5), 0)\n",
        "\n",
        "    # Apply Laplacian filter for edge detection\n",
        "    laplacian = cv2.Laplacian(blurred, cv2.CV_64F)\n",
        "\n",
        "    # Normalize and invert the Laplacian result\n",
        "    laplacian = (laplacian - np.min(laplacian)) / (np.max(laplacian) - np.min(laplacian))\n",
        "    inverted = 1 - laplacian\n",
        "\n",
        "    # Enhance contrast of the inverted image\n",
        "    enhanced_inverted = clahe.apply((inverted * 255).astype(np.uint8))\n",
        "\n",
        "    # Apply color map for visualization\n",
        "    colored = cv2.applyColorMap(enhanced_inverted, cv2.COLORMAP_BONE)\n",
        "    colored = cv2.cvtColor(colored, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return colored\n",
        "\n",
        "# Image URL\n",
        "image_url = \"https://www.central-cl.or.jp/wp-content/uploads/2015/07/8abf677e732a4de3ecb28c79ce082dff.jpg\"\n",
        "\n",
        "# Download and process the image\n",
        "original = download_image(image_url)\n",
        "processed = preprocess_fundus_image(original)\n",
        "\n",
        "# Display the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(original)\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(processed)\n",
        "plt.title('Processed Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s_BW9Dxvlipn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLAHE併用バージョン"
      ],
      "metadata": {
        "id": "RqDNMlIq4lUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# このスクリプトは以下の処理を行います：\n",
        "# 1. 指定されたURLから画像をダウンロード\n",
        "# 2. 画像の前処理と拡張：\n",
        "#    - 中心部分をクロップ（元のサイズの90%）\n",
        "#    - サイズを800x800にリサイズ\n",
        "#    - カスタム正規化処理（ガウシアンブラーを使用）\n",
        "#    - ランダムな明るさとコントラストの調整\n",
        "#    - コントラスト限界適応ヒストグラム平坦化（CLAHE）によるコントラスト強調\n",
        "# 3. 元の画像と処理後の画像を並べて表示\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from albumentations import (\n",
        "    Compose, Resize, RandomBrightnessContrast, ImageOnlyTransform, CenterCrop\n",
        ")\n",
        "\n",
        "def download_image(url):\n",
        "    response = requests.get(url)\n",
        "    image = cv2.imdecode(np.frombuffer(response.content, np.uint8), cv2.IMREAD_COLOR)\n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "class NormalizeImage(ImageOnlyTransform):\n",
        "    def __init__(self, alpha=2.0, beta=-1.0, gamma=0, always_apply=False, p=1.0):\n",
        "        super(NormalizeImage, self).__init__(always_apply, p)\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def apply(self, image, **params):\n",
        "        image = image.astype(np.float32)\n",
        "        gaussian = cv2.GaussianBlur(image, (0, 0), sigmaX=10)\n",
        "        normalized = cv2.addWeighted(image, self.alpha, gaussian, self.beta, self.gamma)\n",
        "        return np.clip(normalized, 0, 255).astype(np.uint8)\n",
        "\n",
        "def enhance_contrast(image):\n",
        "    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
        "    cl = clahe.apply(l)\n",
        "    limg = cv2.merge((cl,a,b))\n",
        "    enhanced = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
        "    return enhanced\n",
        "\n",
        "def preprocess_and_augment(image):\n",
        "    h, w = image.shape[:2]\n",
        "    crop_h, crop_w = int(h * 0.9), int(w * 0.9)  # Increase crop size to 80% of original\n",
        "\n",
        "    transform = Compose([\n",
        "        CenterCrop(crop_h, crop_w),\n",
        "        Resize(800, 800),\n",
        "        NormalizeImage(alpha=2.0, beta=-1.0, gamma=0),\n",
        "        RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1)\n",
        "    ])\n",
        "\n",
        "    augmented = transform(image=image)[\"image\"]\n",
        "    augmented = enhance_contrast(augmented)\n",
        "    return augmented\n",
        "\n",
        "# Image URL\n",
        "image_url = \"https://www.central-cl.or.jp/wp-content/uploads/2015/07/8abf677e732a4de3ecb28c79ce082dff.jpg\"\n",
        "\n",
        "# Download and process the image\n",
        "original = download_image(image_url)\n",
        "\n",
        "# Process the cropped image\n",
        "processed = preprocess_and_augment(original)\n",
        "\n",
        "# Display the results\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(original)\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(processed)\n",
        "plt.title('Processed Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z-mg6xUt3PUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Augmentationのサンプル\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "def download_image(url):\n",
        "    response = requests.get(url)\n",
        "    image = Image.open(requests.get(url, stream=True).raw)\n",
        "    return image\n",
        "\n",
        "def apply_augmentations(image):\n",
        "    # Define transformations\n",
        "    to_tensor = transforms.ToTensor()\n",
        "    to_pil = transforms.ToPILImage()\n",
        "\n",
        "    augmentations = [\n",
        "        (\"Original\", transforms.Lambda(lambda x: x)),\n",
        "        (\"Rotate\", transforms.RandomRotation(degrees=(-10, 10))),\n",
        "        (\"Flip\", transforms.RandomHorizontalFlip(p=1.0)),\n",
        "        (\"Color Jitter\", transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)),\n",
        "        (\"Gaussian Blur\", transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0))),\n",
        "        (\"Random Resized Crop\", transforms.RandomResizedCrop(size=image.size, scale=(0.8, 1.0), ratio=(0.75, 1.33))),\n",
        "        (\"Center Crop\", transforms.CenterCrop(size=min(image.size))),\n",
        "        (\"Grayscale\", transforms.RandomGrayscale(p=1.0))\n",
        "    ]\n",
        "\n",
        "    augmented_images = []\n",
        "    img_tensor = to_tensor(image)\n",
        "\n",
        "    for title, transform in augmentations:\n",
        "        augmented = transform(img_tensor)\n",
        "        augmented_images.append((title, to_pil(augmented)))\n",
        "\n",
        "    return augmented_images\n",
        "\n",
        "def display_images(images):\n",
        "    rows = (len(images) + 3) // 4  # Calculate number of rows needed\n",
        "    cols = min(len(images), 4)     # Use 4 columns or less if fewer images\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(20, 5 * rows))\n",
        "    fig.suptitle(\"Fundus Image Augmentations\", fontsize=16)\n",
        "\n",
        "    for i, (title, img) in enumerate(images):\n",
        "        ax = axes[i // cols, i % cols] if rows > 1 else axes[i]\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(title)\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for i in range(len(images), rows * cols):\n",
        "        ax = axes[i // cols, i % cols] if rows > 1 else axes[i]\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Image URL\n",
        "image_url = \"https://www.central-cl.or.jp/wp-content/uploads/2015/07/8abf677e732a4de3ecb28c79ce082dff.jpg\"\n",
        "\n",
        "# Download the image\n",
        "original_image = download_image(image_url)\n",
        "\n",
        "# Apply augmentations\n",
        "augmented_images = apply_augmentations(original_image)\n",
        "\n",
        "# Display the results\n",
        "display_images(augmented_images)"
      ],
      "metadata": {
        "id": "mofMh7nqP0MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**トレーニング画像をまとめて前処理**\n",
        "\n",
        "> 引用を追加\n",
        "\n"
      ],
      "metadata": {
        "id": "r57Dd7caT4dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "画像リサイズスクリプト simple 222px切り抜き\n",
        "\n",
        "このスクリプトは、指定された複数のディレクトリから全ての画像を読み込み、\n",
        "以下の処理を行います：\n",
        "1. 画像の中央部分を900x900ピクセルで切り抜き\n",
        "2. 切り抜いた画像を224x224ピクセルにリサイズ\n",
        "3. 処理した画像を新しい出力ディレクトリに保存\n",
        "\n",
        "主な機能：\n",
        "- 複数の入力ディレクトリから全ての画像を読み込む\n",
        "- 画像の中央部分を切り抜く（crop_center_image関数）\n",
        "- 画像をリサイズする（resize_image関数）\n",
        "- 処理済み画像を新しいディレクトリに保存\n",
        "- 詳細な進捗状況の表示\n",
        "\n",
        "注意：\n",
        "- 対応している画像形式は.png、.jpg、.jpegです\n",
        "- Google Colab環境での実行を想定しています\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "def crop_center_image(cv2_img, crop_size=900):\n",
        "    height, width, _ = cv2_img.shape\n",
        "    center_x = width // 2\n",
        "    center_y = height // 2\n",
        "    start_x = center_x - crop_size // 2\n",
        "    start_y = center_y - crop_size // 2\n",
        "    cropped_img = cv2_img[start_y:start_y + crop_size, start_x:start_x + crop_size]\n",
        "    return cropped_img\n",
        "\n",
        "def resize_image(cv2_img, size=(224, 224)):\n",
        "    resized_img = cv2.resize(cv2_img, size)\n",
        "    return resized_img\n",
        "\n",
        "# 入力ディレクトリと出力ディレクトリの設定\n",
        "input_dirs = [f\"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_train_{i}\" for i in range(1, 11)]\n",
        "output_dir = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_224px\"\n",
        "\n",
        "# 出力ディレクトリが存在しない場合は作成\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# 全画像ファイルのリストを作成\n",
        "all_image_files = []\n",
        "for input_dir in input_dirs:\n",
        "    image_files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    all_image_files.extend([(input_dir, f) for f in image_files])\n",
        "\n",
        "# 進捗バーの設定\n",
        "total_images = len(all_image_files)\n",
        "pbar = tqdm(total=total_images, desc=\"画像処理\", unit=\"枚\")\n",
        "\n",
        "# 全ての画像を処理\n",
        "for input_dir, filename in all_image_files:\n",
        "    input_path = os.path.join(input_dir, filename)\n",
        "    output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "    # 画像の読み込み、クロップ、リサイズ\n",
        "    img = cv2.imread(input_path)\n",
        "    cropped_img = crop_center_image(img)\n",
        "    resized_img = resize_image(cropped_img)\n",
        "\n",
        "    # 処理した画像を保存\n",
        "    cv2.imwrite(output_path, resized_img)\n",
        "\n",
        "    # 進捗バーの更新\n",
        "    pbar.update(1)\n",
        "\n",
        "# 進捗バーの終了\n",
        "pbar.close()\n",
        "\n",
        "print(f\"\\n処理完了: 合計 {total_images} 枚の画像を処理しました。\")"
      ],
      "metadata": {
        "id": "OZYIAOJMT3Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "画像リサイズスクリプト simple 600px切り抜き\n",
        "\n",
        "このスクリプトは、指定された複数のディレクトリから全ての画像を読み込み、\n",
        "以下の処理を行います：\n",
        "1. 画像の中央部分を900x900ピクセルで切り抜き\n",
        "2. 切り抜いた画像を600x600ピクセルにリサイズ（EfficientNetv2 XL用）\n",
        "3. 処理した画像を新しい出力ディレクトリに保存\n",
        "\n",
        "主な機能：\n",
        "- 複数の入力ディレクトリから全ての画像を読み込む\n",
        "- 画像の中央部分を切り抜く（crop_center_image関数）\n",
        "- 画像をリサイズする（resize_image関数）\n",
        "- 処理済み画像を新しいディレクトリに保存\n",
        "- 詳細な進捗状況の表示\n",
        "\n",
        "注意：\n",
        "- 対応している画像形式は.png、.jpg、.jpegです\n",
        "- Google Colab環境での実行を想定しています\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "def crop_center_image(cv2_img, crop_size=900):\n",
        "    height, width, _ = cv2_img.shape\n",
        "    center_x = width // 2\n",
        "    center_y = height // 2\n",
        "    start_x = center_x - crop_size // 2\n",
        "    start_y = center_y - crop_size // 2\n",
        "    cropped_img = cv2_img[start_y:start_y + crop_size, start_x:start_x + crop_size]\n",
        "    return cropped_img\n",
        "\n",
        "def resize_image(cv2_img, size=(600, 600)):\n",
        "    resized_img = cv2.resize(cv2_img, size)\n",
        "    return resized_img\n",
        "\n",
        "# 入力ディレクトリと出力ディレクトリの設定\n",
        "input_dirs = [f\"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_train_{i}\" for i in range(1, 11)]\n",
        "output_dir = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_600px\"\n",
        "\n",
        "# 出力ディレクトリが存在しない場合は作成\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# 全画像ファイルのリストを作成\n",
        "all_image_files = []\n",
        "for input_dir in input_dirs:\n",
        "    image_files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    all_image_files.extend([(input_dir, f) for f in image_files])\n",
        "\n",
        "# 進捗バーの設定\n",
        "total_images = len(all_image_files)\n",
        "pbar = tqdm(total=total_images, desc=\"画像処理\", unit=\"枚\")\n",
        "\n",
        "# 全ての画像を処理\n",
        "for input_dir, filename in all_image_files:\n",
        "    input_path = os.path.join(input_dir, filename)\n",
        "    output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "    # 画像の読み込み、クロップ、リサイズ\n",
        "    img = cv2.imread(input_path)\n",
        "    cropped_img = crop_center_image(img)\n",
        "    resized_img = resize_image(cropped_img)\n",
        "\n",
        "    # 処理した画像を保存\n",
        "    cv2.imwrite(output_path, resized_img)\n",
        "\n",
        "    # 進捗バーの更新\n",
        "    pbar.update(1)\n",
        "\n",
        "# 進捗バーの終了\n",
        "pbar.close()\n",
        "\n",
        "print(f\"\\n処理完了: 合計 {total_images} 枚の画像を処理しました。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BomgEhEflqky",
        "outputId": "be02b54b-3d0b-4c36-9f48-364bf23775a2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "画像処理: 100%|██████████| 5000/5000 [1:46:18<00:00,  1.28s/枚]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "処理完了: 合計 5000 枚の画像を処理しました。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "画像リサイズスクリプト simple 600px切り抜き\n",
        "\n",
        "このスクリプトは、指定された複数のディレクトリから全ての画像を読み込み、\n",
        "以下の処理を行います：\n",
        "1. 画像の中央部分を900x900ピクセルで切り抜き\n",
        "2. 切り抜いた画像を600x600ピクセルにリサイズ（EfficientNetv2 XL用）\n",
        "3. 処理した画像を新しい出力ディレクトリに保存\n",
        "\n",
        "主な機能：\n",
        "- 複数の入力ディレクトリから全ての画像を読み込む\n",
        "- 画像の中央部分を切り抜く（crop_center_image関数）\n",
        "- 画像をリサイズする（resize_image関数）\n",
        "- 処理済み画像を新しいディレクトリに保存\n",
        "- 詳細な進捗状況の表示\n",
        "\n",
        "注意：\n",
        "- 対応している画像形式は.png、.jpg、.jpegです\n",
        "- Google Colab環境での実行を想定しています\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "def crop_center_image(cv2_img, crop_size=900):\n",
        "    height, width, _ = cv2_img.shape\n",
        "    center_x = width // 2\n",
        "    center_y = height // 2\n",
        "    start_x = center_x - crop_size // 2\n",
        "    start_y = center_y - crop_size // 2\n",
        "    cropped_img = cv2_img[start_y:start_y + crop_size, start_x:start_x + crop_size]\n",
        "    return cropped_img\n",
        "\n",
        "def resize_image(cv2_img, size=(480, 480)):\n",
        "    resized_img = cv2.resize(cv2_img, size)\n",
        "    return resized_img\n",
        "\n",
        "# 入力ディレクトリと出力ディレクトリの設定\n",
        "input_dirs = [f\"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_train_{i}\" for i in range(1, 11)]\n",
        "output_dir = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_480px\"\n",
        "\n",
        "# 出力ディレクトリが存在しない場合は作成\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# 全画像ファイルのリストを作成\n",
        "all_image_files = []\n",
        "for input_dir in input_dirs:\n",
        "    image_files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    all_image_files.extend([(input_dir, f) for f in image_files])\n",
        "\n",
        "# 進捗バーの設定\n",
        "total_images = len(all_image_files)\n",
        "pbar = tqdm(total=total_images, desc=\"画像処理\", unit=\"枚\")\n",
        "\n",
        "# 全ての画像を処理\n",
        "for input_dir, filename in all_image_files:\n",
        "    input_path = os.path.join(input_dir, filename)\n",
        "    output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "    # 画像の読み込み、クロップ、リサイズ\n",
        "    img = cv2.imread(input_path)\n",
        "    cropped_img = crop_center_image(img)\n",
        "    resized_img = resize_image(cropped_img)\n",
        "\n",
        "    # 処理した画像を保存\n",
        "    cv2.imwrite(output_path, resized_img)\n",
        "\n",
        "    # 進捗バーの更新\n",
        "    pbar.update(1)\n",
        "\n",
        "# 進捗バーの終了\n",
        "pbar.close()\n",
        "\n",
        "print(f\"\\n処理完了: 合計 {total_images} 枚の画像を処理しました。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRbu-plWX_nU",
        "outputId": "bb6d0188-ea13-45a2-873c-56cd9842b77b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "画像処理: 100%|██████████| 5000/5000 [02:28<00:00, 33.63枚/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "処理完了: 合計 5000 枚の画像を処理しました。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**simple classification**\n",
        "\n",
        "まずは、中央を900x900で切り抜いた後に224x224に縮小した画像でbinaryな分類を試みる"
      ],
      "metadata": {
        "id": "GKdnhQpiHSCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "XkVIKok9X2N-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ca1c104-65d9-4b80-8665-f45ed8855664"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "Successfully installed timm-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "\n",
        "# 利用可能なモデルをリストとして取得\n",
        "model_list = timm.list_models(pretrained=True)\n",
        "# モデル一覧を表示\n",
        "model_list\n"
      ],
      "metadata": {
        "id": "KlC0rSFyfDCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#試してみるモデル\n",
        "#224px\n",
        "#ResNet50 --> 60%ぐらいが限界\n",
        "#EfficitnetNetv2 --> 60%\n",
        "#RepVGGA0 --> 60%弱\n",
        "\n",
        "#480px\n",
        "#EfficientNetv2 --> 60%\n",
        "\n",
        "\"\"\"\n",
        "EfficientNetV2-B0: 入力サイズは224x224ピクセル\n",
        "EfficientNetV2-B1: 入力サイズは240x240ピクセル\n",
        "EfficientNetV2-B2: 入力サイズは260x260ピクセル\n",
        "EfficientNetV2-B3: 入力サイズは300x300ピクセル\n",
        "EfficientNetV2-S: 入力サイズは384x384ピクセル\n",
        "EfficientNetV2-M: 入力サイズは512x512ピクセル\n",
        "EfficientNetV2-L: 入力サイズは640x640ピクセル\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rOko9ioe3Lkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 眼底画像を用いた代謝症候群分類モデル（エラー修正版）\n",
        "#\n",
        "# このスクリプトは以下の機能を実装しています：\n",
        "# 1. 眼底画像データセットの読み込みと前処理\n",
        "# 2. データ拡張（Image Augmentation）の適用（Torchvision最新版に対応）\n",
        "# 3. ResNet50を用いた転移学習モデルの構築\n",
        "# 4. モデルのトレーニングと評価（各エポックの実行時間を表示）\n",
        "# 5. Early Stoppingの実装\n",
        "# 6. 最終的な性能評価とレポート生成\n",
        "# 7. トレーニング過程の可視化\n",
        "# 8. 総実行時間の表示\n",
        "\n",
        "# 必要なライブラリのインストール\n",
        "!pip install timm --q\n",
        "!pip install ranger-adabelief==0.1.0 --q\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "# Google Driveのマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Training settings\n",
        "num_epochs = 50\n",
        "patience = 10\n",
        "seed = 42\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(seed)\n",
        "\n",
        "# データセットクラスの定義\n",
        "class FundusDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        label = self.data.iloc[idx, 8]  # METSカラムのインデックス\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# データの前処理とオーグメンテーション\n",
        "transform_train = transforms.Compose([\n",
        "    #transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
        "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomGrayscale(p=0.1),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 検証用の変換（オーグメンテーションなし）\n",
        "transform_val = transforms.Compose([\n",
        "    #transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# データセットとデータローダーの作成\n",
        "csv_file = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/label_train.csv\"\n",
        "img_dir = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_480px\"\n",
        "#img_dir = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_224px\"\n",
        "\n",
        "full_dataset = FundusDataset(csv_file, img_dir)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_dataset.dataset.transform = transform_train\n",
        "val_dataset.dataset.transform = transform_val\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# モデルの定義\n",
        "class ResNet50Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet50Model, self).__init__()\n",
        "        self.resnet = timm.create_model('resnet50', pretrained=True)\n",
        "        in_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(in_features, 2)  # 2クラス分類\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "\n",
        "class EfficientNetV2s(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EfficientNetV2s, self).__init__()\n",
        "        self.efficientnet = timm.create_model('tf_efficientnetv2_s.in21k_ft_in1k', pretrained=True)\n",
        "\n",
        "        # Get the number of features from the last layer\n",
        "        in_features = self.efficientnet.get_classifier().in_features\n",
        "\n",
        "        # Replace the classifier with a new one for 2-class classification\n",
        "        self.efficientnet.classifier = nn.Linear(in_features, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.efficientnet(x)\n",
        "\n",
        "\n",
        "class CoAtNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CoAtNet2, self).__init__()\n",
        "        self.coatnet = timm.create_model('coatnet_2_rw_224.sw_in12k_ft_in1k', pretrained=True)\n",
        "\n",
        "        # Get the number of features from the last layer\n",
        "        in_features = self.coatnet.head.fc.in_features\n",
        "\n",
        "        # Replace the classifier with a new one for 2-class classification\n",
        "        self.coatnet.head.fc = nn.Linear(in_features, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.coatnet(x)\n",
        "\n",
        "class RepVGGA0(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RepVGGA0, self).__init__()\n",
        "        self.repvgg = timm.create_model('repvgg_a0.rvgg_in1k', pretrained=True)\n",
        "\n",
        "        # Get the number of features from the last layer\n",
        "        in_features = self.repvgg.head.fc.in_features\n",
        "\n",
        "        # Replace the classifier with a new one for 2-class classification\n",
        "        self.repvgg.head.fc = nn.Linear(in_features, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.repvgg(x)\n",
        "\n",
        "class RepViTM1_0(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(RepViTM1_0, self).__init__()\n",
        "        self.repvit = timm.create_model('repvit_m1_0.dist_450e_in1k', pretrained=True)\n",
        "\n",
        "        # Get the number of features from the last layer\n",
        "        in_features = self.repvit.head.head.l.in_features\n",
        "\n",
        "        # Replace the classifier with a new one for num_classes classification\n",
        "        self.repvit.head.head.l = nn.Linear(in_features, num_classes)\n",
        "        self.repvit.head.head_dist.l = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.repvit(x)\n",
        "\n",
        "\n",
        "\n",
        "# モデル、損失関数、オプティマイザーの設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model = ResNet50Model().to(device)\n",
        "#model = EfficientNetV2s().to(device)\n",
        "#model = CoAtNet2().to(device)\n",
        "#model = RepVGGA0().to(device)\n",
        "#model = RepViTM1_0().to(device)\n",
        "#model = timm.create_model('swin_base_patch4_window7_224_in22k', pretrained=True, num_classes=2).to(device)\n",
        "#model = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=2).to(device)\n",
        "#model = timm.create_model('efficientnetv2_xl', pretrained=False, num_classes=2).to(device) #画像サイズ600ｘ600を推奨,Pretrainedなし, out of memoryになる\n",
        "model = timm.create_model('efficientnetv2_rw_m.agc_in1k', pretrained=True, num_classes=2).to(device) #画像サイズ480ｘ480を推奨\n",
        "#model = timm.create_model('repvgg_a0.rvgg_in1k', pretrained=True, num_classes=2).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "#optimizer = RangerAdaBelief(model.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999), weight_decouple=True)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "# トレーニング関数\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# 評価関数\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Early Stopping クラス\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "# トレーニングループ\n",
        "num_epochs = num_epochs\n",
        "early_stopping = EarlyStopping(patience=patience)\n",
        "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    # スケジューラの更新\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "    print(f\"Epoch duration: {epoch_duration:.2f} seconds\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    early_stopping(val_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "end_time = time.time()\n",
        "total_duration = end_time - start_time\n",
        "\n",
        "print(f\"Total training time: {total_duration:.2f} seconds\")\n",
        "\n",
        "\n",
        "\n",
        "# 最終評価\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = outputs.max(1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "final_accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Final Accuracy: {final_accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "\n",
        "# 訓練結果のグラフ表示\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['train_acc'], label='Train Accuracy')\n",
        "plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4bRqls1DHWeV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234,
          "referenced_widgets": [
            "e66d32f45e35499989cb26af38307458",
            "c35dc9cdadcd4753b916b94ba336f0bd",
            "eb93b7de2172480683814627a1d2ab3c",
            "ff45d1dc4dfa422d96c995fa9877b17d",
            "48cae90ad5b346e6bc90dbeb818be5f9",
            "b09ff0b6326a4d4685fb75792a691eb5",
            "b184520123734ba5bbe0959c4d9944fe",
            "1390cb797d64458abdeef8d840ee117b",
            "d35ea8d06fef43e58e5b383c46ca9274",
            "7f02d2c5cd064c018af06347190ca0db",
            "f0a78e5a0f0543bf96eab3bed5cb8638"
          ]
        },
        "outputId": "c3eda198-c59b-4471-c23d-20151d362af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/214M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e66d32f45e35499989cb26af38307458"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ij8kK9FCtIOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Small datasetを用いたテスト**"
      ],
      "metadata": {
        "id": "I6xmXhXncD-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 眼底画像を用いたメタボ分類\n",
        "#\n",
        "# このスクリプトは、眼底画像を使用して代謝症候群の二値分類を行います。\n",
        "# 事前学習済みのResNet50モデルを使用し、300画像の縮小データセットで微調整を行います。\n",
        "#\n",
        "# 主要コンポーネント:\n",
        "# 1. データ準備:\n",
        "#    - 元のデータセットを300サンプルに縮小\n",
        "#    - トレーニングセットと検証セットに分割\n",
        "# 2. モデル:\n",
        "#    - timmライブラリから事前学習済みResNet50モデルを使用\n",
        "#    - 最終の全結合層を二値分類用に変更\n",
        "# 3. トレーニング:\n",
        "#    - カスタムトレーニングループとEarly Stoppingを実装\n",
        "#    - Adamオプティマイザとクロスエントロピー損失を使用\n",
        "# 4. 評価:\n",
        "#    - 検証セットで最終評価を実施\n",
        "#    - 精度と分類レポートを出力\n",
        "# 5. 可視化:\n",
        "#    - トレーニングと検証の損失/精度曲線をプロット\n",
        "#\n",
        "# 注意: このバージョンは、迅速なテストと反復のために縮小されたデータセットを使用しています。\n",
        "#       必要に応じて、データセットのサイズ、モデルのパラメータ、トレーニング設定を調整してください。\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# データセットクラスの定義\n",
        "class FundusDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        label = self.data.iloc[idx, 8]  # METSカラムのインデックス\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# データの前処理\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# データセットとデータローダーの作成\n",
        "csv_file = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/label_train.csv\"\n",
        "img_dir = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_224px\"\n",
        "\n",
        "# CSVファイルを読み込み、最初の300行を抽出\n",
        "full_data = pd.read_csv(csv_file)\n",
        "reduced_data = full_data.head(300)\n",
        "reduced_csv_file = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/label_train_reduced.csv\"\n",
        "reduced_data.to_csv(reduced_csv_file, index=False)\n",
        "\n",
        "# 縮小されたデータセットを使用\n",
        "dataset = FundusDataset(reduced_csv_file, img_dir, transform=transform)\n",
        "train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=8, shuffle=False)\n",
        "\n",
        "print(f\"Total samples in reduced dataset: {len(dataset)}\")\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "\n",
        "# モデルの定義\n",
        "class ResNet50Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet50Model, self).__init__()\n",
        "        self.resnet = timm.create_model('resnet50', pretrained=True)\n",
        "        in_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(in_features, 2)  # 2クラス分類\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# モデル、損失関数、オプティマイザーの設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet50Model().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# トレーニング関数\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# 評価関数\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Early Stopping クラス\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "# トレーニングループ\n",
        "num_epochs = 50\n",
        "early_stopping = EarlyStopping(patience=10)\n",
        "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    early_stopping(val_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "# 最終評価\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = outputs.max(1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "final_accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Final Accuracy: {final_accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "\n",
        "# 訓練結果のグラフ表示\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['train_acc'], label='Train Accuracy')\n",
        "plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "FO4dTXF0Mw-g",
        "outputId": "07fb25ce-fc93-437b-8a8b-1b675bd1f0f9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b22d940bc89c>\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# 縮小されたデータセットを使用\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFundusDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_csv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     )\n\u001b[1;32m    213\u001b[0m                 ):\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2670\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2672\u001b[0;31m     return list(\n\u001b[0m\u001b[1;32m   2673\u001b[0m         chain.from_iterable(\n\u001b[1;32m   2674\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2672\u001b[0m     return list(\n\u001b[1;32m   2673\u001b[0m         chain.from_iterable(\n\u001b[0;32m-> 2674\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m         )\n\u001b[1;32m   2676\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_list_indexing\u001b[0;34m(X, key, key_dtype)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# key is a integer array-like of key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# key is a integer array-like of key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-b22d940bc89c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# METSカラムのインデックス\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3440\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3442\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3444\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**周辺項目から攻めるバージョン**\n",
        "\n",
        "\n",
        "-----------\n",
        "###説明変数\n",
        "\n",
        "age: 年齢\n",
        "\n",
        "AC: 腹囲\n",
        "\n",
        "SBP: 収縮期血圧\n",
        "\n",
        "DBP: 拡張期血圧\n",
        "\n",
        "HDLC: HDLコレステロール\n",
        "\n",
        "TG: 中性脂肪\n",
        "\n",
        "BS: 血糖\n",
        "\n",
        "---------------\n",
        "\n",
        "###目的変数\n",
        "\n",
        "METS: メタボリックシンドロームの有無\n",
        "\n",
        "---------------\n",
        "\n",
        "1. label_train.csvから標準化した説明変数をピックアップする\n",
        "\n",
        "2. ResNet50で、7つの説明変数を同時に回帰するモデルを作成する。Lossは最小二乗誤差とする\n",
        "\n",
        "3. まずはこのモデルがうまく作成できるかを検討\n",
        "\n",
        "4. ある程度モデルの精度が上がったら、7つの変数からNGBoostを用いてMETsを予測するモデルを結合して、METsを目的変数としてファインチューニングする\n"
      ],
      "metadata": {
        "id": "SqCve6HqKdWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeTmeCEiVeeK",
        "outputId": "04a784ae-38d5-4081-8c08-cc070dcb10aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.6)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "Successfully installed timm-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install ranger-adabelief==0.1.0\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "\n",
        "# データセットクラスの定義\n",
        "class FundusDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # 説明変数の標準化\n",
        "        self.scaler = StandardScaler()\n",
        "        self.features = ['age', 'AC', 'SBP', 'DBP', 'HDLC', 'TG', 'BS']\n",
        "        self.data[self.features] = self.scaler.fit_transform(self.data[self.features])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        # 説明変数を取得\n",
        "        features = self.data.iloc[idx][self.features].values.astype(np.float32)\n",
        "\n",
        "        # METSラベルを取得\n",
        "        mets_label = self.data.iloc[idx]['METS']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, features, mets_label\n",
        "\n",
        "# データの前処理\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# データセットとデータローダーの作成\n",
        "csv_file = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/label_train.csv\"\n",
        "img_dir = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_224px\"\n",
        "\n",
        "full_dataset = FundusDataset(csv_file, img_dir, transform=transform)\n",
        "train_dataset, val_dataset = train_test_split(full_dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# マルチ出力回帰モデルの定義\n",
        "# class MultiOutputRegression(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(MultiOutputRegression, self).__init__()\n",
        "#         self.resnet = timm.create_model('resnet50', pretrained=True)\n",
        "#         in_features = self.resnet.fc.in_features\n",
        "#         self.resnet.fc = nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "#         self.regression_head = nn.Sequential(\n",
        "#             nn.Linear(in_features, 256),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(256, 7)  # 7 outputs for regression\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         features = self.resnet(x)\n",
        "#         return self.regression_head(features)\n",
        "\n",
        "class MultiOutputRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiOutputRegression, self).__init__()\n",
        "        self.repvgg = timm.create_model('repvgg_a0.rvgg_in1k', pretrained=True)\n",
        "        in_features = self.repvgg.head.fc.in_features\n",
        "        self.repvgg.head.fc = nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "        self.regression_head = nn.Sequential(\n",
        "            nn.Linear(in_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 7)  # 7 outputs for regression\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.repvgg(x)\n",
        "        return self.regression_head(features)\n",
        "\n",
        "# モデル、損失関数、オプティマイザーの設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultiOutputRegression().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0002)\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999), weight_decouple = True)\n",
        "\n",
        "\n",
        "# トレーニングループ\n",
        "num_epochs = 50\n",
        "history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, features, _ in train_loader:\n",
        "        images, features = images.to(device), features.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, features)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, features, _ in val_loader:\n",
        "            images, features = images.to(device), features.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, features)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# 学習曲線のプロット\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# モデルの評価\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, features, _ in val_loader:\n",
        "        images, features = images.to(device), features.to(device)\n",
        "        outputs = model(images)\n",
        "        all_preds.extend(outputs.cpu().numpy())\n",
        "        all_true.extend(features.cpu().numpy())\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_true = np.array(all_true)\n",
        "\n",
        "# R-squared score for each feature\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "feature_names = ['age', 'AC', 'SBP', 'DBP', 'HDLC', 'TG', 'BS']\n",
        "for i, feature in enumerate(feature_names):\n",
        "    r2 = r2_score(all_true[:, i], all_preds[:, i])\n",
        "    print(f\"R-squared score for {feature}: {r2:.4f}\")\n",
        "\n",
        "# Scatter plots for each feature\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, feature in enumerate(feature_names):\n",
        "    axes[i].scatter(all_true[:, i], all_preds[:, i], alpha=0.5)\n",
        "    axes[i].plot([all_true[:, i].min(), all_true[:, i].max()], [all_true[:, i].min(), all_true[:, i].max()], 'r--', lw=2)\n",
        "    axes[i].set_xlabel(f'True {feature}')\n",
        "    axes[i].set_ylabel(f'Predicted {feature}')\n",
        "    axes[i].set_title(f'{feature}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sUFJxMVRU33-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリのインストール\n",
        "!pip install timm\n",
        "!pip install ranger-adabelief==0.1.0\n",
        "\n",
        "# Google Driveのマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "\n",
        "# データセットクラスの定義\n",
        "class FundusDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # 説明変数の標準化\n",
        "        self.scaler = StandardScaler()\n",
        "        self.features = ['age', 'AC', 'SBP', 'DBP', 'HDLC', 'TG', 'BS']\n",
        "        self.data[self.features] = self.scaler.fit_transform(self.data[self.features])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        # 説明変数を取得\n",
        "        features = self.data.iloc[idx][self.features].values.astype(np.float32)\n",
        "\n",
        "        # METSラベルを取得\n",
        "        mets_label = self.data.iloc[idx]['METS']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, features, mets_label\n",
        "\n",
        "# データの前処理とオーグメンテーション\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 検証用の変換（オーグメンテーションなし）\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# データセットとデータローダーの作成\n",
        "csv_file = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/label_train.csv\"\n",
        "img_dir = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_224px\"\n",
        "\n",
        "full_dataset = FundusDataset(csv_file, img_dir)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_dataset.dataset.transform = transform_train\n",
        "val_dataset.dataset.transform = transform_val\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# RepVGG A0を使用したマルチ出力回帰モデルの定義\n",
        "class MultiOutputRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiOutputRegression, self).__init__()\n",
        "        self.repvgg = timm.create_model('repvgg_a0.rvgg_in1k', pretrained=True)\n",
        "        in_features = self.repvgg.head.fc.in_features\n",
        "        self.repvgg.head.fc = nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "        self.regression_head = nn.Sequential(\n",
        "            nn.Linear(in_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 7)  # 7 outputs for regression\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.repvgg(x)\n",
        "        return self.regression_head(features)\n",
        "\n",
        "# モデル、損失関数、オプティマイザーの設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultiOutputRegression().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = RangerAdaBelief(model.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999), weight_decouple=True)\n",
        "\n",
        "# トレーニングループ\n",
        "num_epochs = 50\n",
        "history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, features, _ in train_loader:\n",
        "        images, features = images.to(device), features.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, features)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, features, _ in val_loader:\n",
        "            images, features = images.to(device), features.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, features)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# 学習曲線のプロット\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# モデルの評価\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, features, _ in val_loader:\n",
        "        images, features = images.to(device), features.to(device)\n",
        "        outputs = model(images)\n",
        "        all_preds.extend(outputs.cpu().numpy())\n",
        "        all_true.extend(features.cpu().numpy())\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_true = np.array(all_true)\n",
        "\n",
        "# R-squared score for each feature\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "feature_names = ['age', 'AC', 'SBP', 'DBP', 'HDLC', 'TG', 'BS']\n",
        "for i, feature in enumerate(feature_names):\n",
        "    r2 = r2_score(all_true[:, i], all_preds[:, i])\n",
        "    print(f\"R-squared score for {feature}: {r2:.4f}\")\n",
        "\n",
        "# Scatter plots for each feature\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, feature in enumerate(feature_names):\n",
        "    axes[i].scatter(all_true[:, i], all_preds[:, i], alpha=0.5)\n",
        "    axes[i].plot([all_true[:, i].min(), all_true[:, i].max()], [all_true[:, i].min(), all_true[:, i].max()], 'r--', lw=2)\n",
        "    axes[i].set_xlabel(f'True {feature}')\n",
        "    axes[i].set_ylabel(f'Predicted {feature}')\n",
        "    axes[i].set_title(f'{feature}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Deep_learning/Fundus_metabolic/repvgg_regression_model.pth')\n",
        "print(\"Model saved successfully.\")"
      ],
      "metadata": {
        "id": "Qh7q9Fa1eSaG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c8943b556dd54f4091e927365df42c73",
            "ea13308e0de041a8956a040f892f3b2e",
            "a8a358abb4cf40cf939d1bb5bbefd8bb",
            "a1f6eadcfa9f4bad86931d72b8f93315",
            "af079f659aba438c82bff95ca9f07cb7",
            "4a5b21b2e30142fcb42aa471d2da75c4",
            "abcd783d8c34426eba410c3a6005d9b3",
            "54a702c7e6074ed0a81013d1b58df0d2",
            "039d91ee8d6341d8b53281f07cd7798b",
            "c5058bf3472647088fcf055f78997a0b",
            "b0128a0e9386420ca6c8777107525209"
          ]
        },
        "outputId": "660f54e1-5e4d-4fa8-e686-70524d612b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.6)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "Successfully installed timm-1.0.9\n",
            "Collecting ranger-adabelief==0.1.0\n",
            "  Downloading ranger_adabelief-0.1.0-py3-none-any.whl.metadata (549 bytes)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from ranger-adabelief==0.1.0) (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger-adabelief==0.1.0) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger-adabelief==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger-adabelief==0.1.0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger-adabelief==0.1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger-adabelief==0.1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->ranger-adabelief==0.1.0) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->ranger-adabelief==0.1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->ranger-adabelief==0.1.0) (1.3.0)\n",
            "Downloading ranger_adabelief-0.1.0-py3-none-any.whl (5.6 kB)\n",
            "Installing collected packages: ranger-adabelief\n",
            "Successfully installed ranger-adabelief-0.1.0\n",
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/36.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8943b556dd54f4091e927365df42c73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 上位1/4と下位1/4を見分けられるか？？\n",
        "\n",
        "SBP: accuracy 0.57"
      ],
      "metadata": {
        "id": "6O_woJdu7fpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリのインストール\n",
        "!pip install timm --q\n",
        "!pip install ranger-adabelief==0.1.0 --q\n",
        "\n",
        "# Google Driveのマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "# Load the uploaded CSV file\n",
        "file_path = '/content/drive/MyDrive/Deep_learning/Fundus_metabolic/label_train.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Check the columns in the CSV to understand the structure\n",
        "data.columns\n",
        "\n",
        "######################\n",
        "## 計算する項目を指定\n",
        "column = 'DBP'\n",
        "######################\n",
        "\n",
        "# Calculate the 25th and 75th percentiles for the SBP column\n",
        "sbp_25th = data[column].quantile(0.25)\n",
        "sbp_75th = data[column].quantile(0.75)\n",
        "\n",
        "# Filter the data for filenames where SBP is in the lower 1/4 and upper 1/4\n",
        "lower_quartile = data[data[column] <= sbp_25th]['filename'].tolist()\n",
        "upper_quartile = data[data[column] >= sbp_75th]['filename'].tolist()\n",
        "\n",
        "print(f\"lower_quartile: {len(lower_quartile)}, upper_quartile: {len(upper_quartile)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYg7pmDs82mZ",
        "outputId": "99ba88db-a4af-487c-a30c-b45ff5c77ae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "lower_quartile: 1312, upper_quartile: 1309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the dataset to only include rows with filenames in the lower and upper quartiles\n",
        "selected_filenames = lower_quartile + upper_quartile\n",
        "filtered_data = data[data['filename'].isin(selected_filenames)]\n",
        "\n",
        "# Split the filtered dataset into training and validation sets\n",
        "train_data, val_data = train_test_split(filtered_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save the filtered train and validation datasets to new CSV files for model use\n",
        "filtered_train_csv = '/content/filtered_train.csv'\n",
        "filtered_val_csv = '/content/filtered_val.csv'\n",
        "\n",
        "train_data.to_csv(filtered_train_csv, index=False)\n",
        "val_data.to_csv(filtered_val_csv, index=False)\n",
        "\n",
        "# Now we will set up the deep learning dataset and dataloaders using these filtered datasets\n",
        "class FilteredFundusDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        # Assuming 'METS' is the label as in the original model script\n",
        "        label = self.data.iloc[idx, 8]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# データの前処理とオーグメンテーション\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 検証用の変換（オーグメンテーションなし）\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Early Stopping クラス\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "# Paths for the filtered datasets\n",
        "train_csv_file = filtered_train_csv\n",
        "val_csv_file = filtered_val_csv\n",
        "img_dir = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_224px\"\n",
        "\n",
        "# Create dataset objects for the filtered data\n",
        "train_dataset_filtered = FilteredFundusDataset(train_csv_file, img_dir, transform_train)\n",
        "val_dataset_filtered = FilteredFundusDataset(val_csv_file, img_dir, transform_val)\n",
        "\n",
        "# Create data loaders for the filtered datasets\n",
        "train_loader_filtered = DataLoader(train_dataset_filtered, batch_size=8, shuffle=True)\n",
        "val_loader_filtered = DataLoader(val_dataset_filtered, batch_size=8, shuffle=False)\n",
        "\n",
        "# The filtered datasets are now ready. You can now use the existing deep learning model code to train and evaluate the model.\n",
        "filtered_train_csv, filtered_val_csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGUA4PL5-Fr8",
        "outputId": "6d95ae26-ce63-42c9-cb30-8c408973cb97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/filtered_train.csv', '/content/filtered_val.csv')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will now proceed with training the model using the filtered data.\n",
        "# Reusing the model and training functions from the provided deep learning code\n",
        "\n",
        "# Set device for training (use GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model setup (using the selected model: RepVGG A0)\n",
        "# モデル定義の一部にドロップアウトを追加\n",
        "model = nn.Sequential(\n",
        "    timm.create_model('repvgg_a0.rvgg_in1k', pretrained=True, num_classes=1000),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(1000, 2)\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.002)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "\n",
        "# Initialize training\n",
        "num_epochs = 200\n",
        "patience = 10\n",
        "early_stopping = EarlyStopping(patience=patience)\n",
        "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_loader_filtered, criterion, optimizer, device)\n",
        "    val_loss, val_acc = evaluate(model, val_loader_filtered, criterion, device)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "    print(f\"Epoch duration: {epoch_duration:.2f} seconds\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    early_stopping(val_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "end_time = time.time()\n",
        "total_duration = end_time - start_time\n",
        "\n",
        "# Final evaluation on validation set\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader_filtered:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = outputs.max(1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "final_accuracy = accuracy_score(all_labels, all_preds)\n",
        "classification_rep = classification_report(all_labels, all_preds)\n",
        "\n",
        "final_accuracy, classification_rep\n"
      ],
      "metadata": {
        "id": "U9P4O6Yo7i0-",
        "outputId": "7fe425ad-e696-44cb-bf4a-f611ff33986a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "Train Loss: 0.9079, Train Acc: 0.5162\n",
            "Val Loss: 0.7046, Val Acc: 0.5124\n",
            "Epoch duration: 33.75 seconds\n",
            "----------------------------------------\n",
            "Epoch 2/50\n",
            "Train Loss: 0.7191, Train Acc: 0.5367\n",
            "Val Loss: 0.7437, Val Acc: 0.5410\n",
            "Epoch duration: 31.30 seconds\n",
            "----------------------------------------\n",
            "Epoch 3/50\n",
            "Train Loss: 0.7147, Train Acc: 0.5396\n",
            "Val Loss: 0.6830, Val Acc: 0.5371\n",
            "Epoch duration: 32.02 seconds\n",
            "----------------------------------------\n",
            "Epoch 4/50\n",
            "Train Loss: 0.7222, Train Acc: 0.5301\n",
            "Val Loss: 0.8476, Val Acc: 0.5410\n",
            "Epoch duration: 30.97 seconds\n",
            "----------------------------------------\n",
            "Epoch 5/50\n",
            "Train Loss: 0.7143, Train Acc: 0.5382\n",
            "Val Loss: 0.6599, Val Acc: 0.6171\n",
            "Epoch duration: 31.07 seconds\n",
            "----------------------------------------\n",
            "Epoch 6/50\n",
            "Train Loss: 0.6941, Train Acc: 0.5630\n",
            "Val Loss: 0.6614, Val Acc: 0.5543\n",
            "Epoch duration: 30.70 seconds\n",
            "----------------------------------------\n",
            "Epoch 7/50\n",
            "Train Loss: 0.7020, Train Acc: 0.5592\n",
            "Val Loss: 0.6611, Val Acc: 0.6133\n",
            "Epoch duration: 31.16 seconds\n",
            "----------------------------------------\n",
            "Epoch 8/50\n",
            "Train Loss: 0.6952, Train Acc: 0.5577\n",
            "Val Loss: 1.5309, Val Acc: 0.4533\n",
            "Epoch duration: 31.28 seconds\n",
            "----------------------------------------\n",
            "Epoch 9/50\n",
            "Train Loss: 0.6952, Train Acc: 0.5639\n",
            "Val Loss: 0.6501, Val Acc: 0.6076\n",
            "Epoch duration: 31.02 seconds\n",
            "----------------------------------------\n",
            "Epoch 10/50\n",
            "Train Loss: 0.6870, Train Acc: 0.5620\n",
            "Val Loss: 0.6387, Val Acc: 0.6457\n",
            "Epoch duration: 30.92 seconds\n",
            "----------------------------------------\n",
            "Epoch 11/50\n",
            "Train Loss: 0.6814, Train Acc: 0.5792\n",
            "Val Loss: 0.6494, Val Acc: 0.6248\n",
            "Epoch duration: 31.19 seconds\n",
            "----------------------------------------\n",
            "Epoch 12/50\n",
            "Train Loss: 0.6773, Train Acc: 0.5759\n",
            "Val Loss: 0.6398, Val Acc: 0.6305\n",
            "Epoch duration: 31.20 seconds\n",
            "----------------------------------------\n",
            "Epoch 13/50\n",
            "Train Loss: 0.6733, Train Acc: 0.5864\n",
            "Val Loss: 0.6558, Val Acc: 0.5695\n",
            "Epoch duration: 31.06 seconds\n",
            "----------------------------------------\n",
            "Epoch 14/50\n",
            "Train Loss: 0.6770, Train Acc: 0.5840\n",
            "Val Loss: 0.6612, Val Acc: 0.6038\n",
            "Epoch duration: 30.95 seconds\n",
            "----------------------------------------\n",
            "Epoch 15/50\n",
            "Train Loss: 0.6773, Train Acc: 0.5854\n",
            "Val Loss: 0.6455, Val Acc: 0.5905\n",
            "Epoch duration: 31.17 seconds\n",
            "----------------------------------------\n",
            "Epoch 16/50\n",
            "Train Loss: 0.6825, Train Acc: 0.5802\n",
            "Val Loss: 0.6701, Val Acc: 0.5505\n",
            "Epoch duration: 30.53 seconds\n",
            "----------------------------------------\n",
            "Epoch 17/50\n",
            "Train Loss: 0.6746, Train Acc: 0.5625\n",
            "Val Loss: 0.6360, Val Acc: 0.6419\n",
            "Epoch duration: 31.03 seconds\n",
            "----------------------------------------\n",
            "Epoch 18/50\n",
            "Train Loss: 0.6622, Train Acc: 0.5873\n",
            "Val Loss: 0.6408, Val Acc: 0.5981\n",
            "Epoch duration: 31.10 seconds\n",
            "----------------------------------------\n",
            "Epoch 19/50\n",
            "Train Loss: 0.6616, Train Acc: 0.6031\n",
            "Val Loss: 0.6387, Val Acc: 0.5886\n",
            "Epoch duration: 31.14 seconds\n",
            "----------------------------------------\n",
            "Epoch 20/50\n",
            "Train Loss: 0.6530, Train Acc: 0.6183\n",
            "Val Loss: 0.6298, Val Acc: 0.6248\n",
            "Epoch duration: 30.96 seconds\n",
            "----------------------------------------\n",
            "Epoch 21/50\n",
            "Train Loss: 0.6561, Train Acc: 0.6217\n",
            "Val Loss: 0.6363, Val Acc: 0.6438\n",
            "Epoch duration: 31.15 seconds\n",
            "----------------------------------------\n",
            "Epoch 22/50\n",
            "Train Loss: 0.6549, Train Acc: 0.6059\n",
            "Val Loss: 0.6350, Val Acc: 0.6610\n",
            "Epoch duration: 30.90 seconds\n",
            "----------------------------------------\n",
            "Epoch 23/50\n",
            "Train Loss: 0.6490, Train Acc: 0.6250\n",
            "Val Loss: 0.6315, Val Acc: 0.6248\n",
            "Epoch duration: 30.94 seconds\n",
            "----------------------------------------\n",
            "Epoch 24/50\n",
            "Train Loss: 0.6476, Train Acc: 0.6183\n",
            "Val Loss: 0.6307, Val Acc: 0.6362\n",
            "Epoch duration: 30.90 seconds\n",
            "----------------------------------------\n",
            "Epoch 25/50\n",
            "Train Loss: 0.6493, Train Acc: 0.6217\n",
            "Val Loss: 0.6450, Val Acc: 0.6248\n",
            "Epoch duration: 31.13 seconds\n",
            "----------------------------------------\n",
            "Epoch 26/50\n",
            "Train Loss: 0.6505, Train Acc: 0.6126\n",
            "Val Loss: 0.6289, Val Acc: 0.6400\n",
            "Epoch duration: 30.67 seconds\n",
            "----------------------------------------\n",
            "Epoch 27/50\n",
            "Train Loss: 0.6493, Train Acc: 0.6174\n",
            "Val Loss: 0.6493, Val Acc: 0.6286\n",
            "Epoch duration: 30.82 seconds\n",
            "----------------------------------------\n",
            "Epoch 28/50\n",
            "Train Loss: 0.6477, Train Acc: 0.6264\n",
            "Val Loss: 0.6413, Val Acc: 0.6248\n",
            "Epoch duration: 30.86 seconds\n",
            "----------------------------------------\n",
            "Epoch 29/50\n",
            "Train Loss: 0.6481, Train Acc: 0.6207\n",
            "Val Loss: 0.6424, Val Acc: 0.6552\n",
            "Epoch duration: 30.90 seconds\n",
            "----------------------------------------\n",
            "Epoch 30/50\n",
            "Train Loss: 0.6441, Train Acc: 0.6245\n",
            "Val Loss: 0.6306, Val Acc: 0.6590\n",
            "Epoch duration: 30.60 seconds\n",
            "----------------------------------------\n",
            "Epoch 31/50\n",
            "Train Loss: 0.6465, Train Acc: 0.6279\n",
            "Val Loss: 0.6250, Val Acc: 0.6438\n",
            "Epoch duration: 30.75 seconds\n",
            "----------------------------------------\n",
            "Epoch 32/50\n",
            "Train Loss: 0.6461, Train Acc: 0.6236\n",
            "Val Loss: 0.6304, Val Acc: 0.6171\n",
            "Epoch duration: 30.77 seconds\n",
            "----------------------------------------\n",
            "Epoch 33/50\n",
            "Train Loss: 0.6421, Train Acc: 0.6298\n",
            "Val Loss: 0.6348, Val Acc: 0.6514\n",
            "Epoch duration: 31.00 seconds\n",
            "----------------------------------------\n",
            "Epoch 34/50\n",
            "Train Loss: 0.6431, Train Acc: 0.6221\n",
            "Val Loss: 0.6293, Val Acc: 0.6571\n",
            "Epoch duration: 30.46 seconds\n",
            "----------------------------------------\n",
            "Epoch 35/50\n",
            "Train Loss: 0.6403, Train Acc: 0.6255\n",
            "Val Loss: 0.6265, Val Acc: 0.6571\n",
            "Epoch duration: 31.70 seconds\n",
            "----------------------------------------\n",
            "Epoch 36/50\n",
            "Train Loss: 0.6435, Train Acc: 0.6240\n",
            "Val Loss: 0.6233, Val Acc: 0.6419\n",
            "Epoch duration: 31.56 seconds\n",
            "----------------------------------------\n",
            "Epoch 37/50\n",
            "Train Loss: 0.6407, Train Acc: 0.6331\n",
            "Val Loss: 0.6418, Val Acc: 0.6533\n",
            "Epoch duration: 31.97 seconds\n",
            "----------------------------------------\n",
            "Epoch 38/50\n",
            "Train Loss: 0.6402, Train Acc: 0.6345\n",
            "Val Loss: 0.6405, Val Acc: 0.6590\n",
            "Epoch duration: 32.33 seconds\n",
            "----------------------------------------\n",
            "Epoch 39/50\n",
            "Train Loss: 0.6327, Train Acc: 0.6374\n",
            "Val Loss: 0.6286, Val Acc: 0.6590\n",
            "Epoch duration: 31.71 seconds\n",
            "----------------------------------------\n",
            "Epoch 40/50\n",
            "Train Loss: 0.6356, Train Acc: 0.6355\n",
            "Val Loss: 0.6324, Val Acc: 0.6667\n",
            "Epoch duration: 31.93 seconds\n",
            "----------------------------------------\n",
            "Epoch 41/50\n",
            "Train Loss: 0.6403, Train Acc: 0.6369\n",
            "Val Loss: 0.6378, Val Acc: 0.6648\n",
            "Epoch duration: 31.70 seconds\n",
            "----------------------------------------\n",
            "Epoch 42/50\n",
            "Train Loss: 0.6405, Train Acc: 0.6365\n",
            "Val Loss: 0.6265, Val Acc: 0.6648\n",
            "Epoch duration: 31.86 seconds\n",
            "----------------------------------------\n",
            "Epoch 43/50\n",
            "Train Loss: 0.6308, Train Acc: 0.6469\n",
            "Val Loss: 0.6276, Val Acc: 0.6590\n",
            "Epoch duration: 31.65 seconds\n",
            "----------------------------------------\n",
            "Epoch 44/50\n",
            "Train Loss: 0.6348, Train Acc: 0.6584\n",
            "Val Loss: 0.6219, Val Acc: 0.6610\n",
            "Epoch duration: 31.97 seconds\n",
            "----------------------------------------\n",
            "Epoch 45/50\n",
            "Train Loss: 0.6307, Train Acc: 0.6608\n",
            "Val Loss: 0.6248, Val Acc: 0.6724\n",
            "Epoch duration: 32.12 seconds\n",
            "----------------------------------------\n",
            "Epoch 46/50\n",
            "Train Loss: 0.6335, Train Acc: 0.6369\n",
            "Val Loss: 0.6236, Val Acc: 0.6667\n",
            "Epoch duration: 31.58 seconds\n",
            "----------------------------------------\n",
            "Epoch 47/50\n",
            "Train Loss: 0.6275, Train Acc: 0.6512\n",
            "Val Loss: 0.6247, Val Acc: 0.6552\n",
            "Epoch duration: 31.89 seconds\n",
            "----------------------------------------\n",
            "Epoch 48/50\n",
            "Train Loss: 0.6300, Train Acc: 0.6598\n",
            "Val Loss: 0.6246, Val Acc: 0.6667\n",
            "Epoch duration: 31.91 seconds\n",
            "----------------------------------------\n",
            "Epoch 49/50\n",
            "Train Loss: 0.6341, Train Acc: 0.6427\n",
            "Val Loss: 0.6238, Val Acc: 0.6571\n",
            "Epoch duration: 31.90 seconds\n",
            "----------------------------------------\n",
            "Epoch 50/50\n",
            "Train Loss: 0.6257, Train Acc: 0.6551\n",
            "Val Loss: 0.6255, Val Acc: 0.6476\n",
            "Epoch duration: 31.80 seconds\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6476190476190476,\n",
              " '              precision    recall  f1-score   support\\n\\n           0       0.69      0.63      0.66       284\\n           1       0.60      0.67      0.64       241\\n\\n    accuracy                           0.65       525\\n   macro avg       0.65      0.65      0.65       525\\nweighted avg       0.65      0.65      0.65       525\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 上位1/4と下位1/4を見分けられるか？？\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Age: 0.64\n",
        "\n",
        "AC: 0.63\n",
        "\n",
        "SBP: 0.66\n",
        "\n",
        "DBP: 0.63\n",
        "\n",
        "HDLC: 0.55\n",
        "\n",
        "TG: 0.62\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HqpA7sm1CiTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "このスクリプトは眼底画像と代謝関連データを用いた深層学習モデルの作成と学習を行います。\n",
        "主な機能：\n",
        "1. 必要なライブラリのインポートとセットアップ\n",
        "2. Google Driveのマウントとデータの読み込み\n",
        "3. カスタムデータセットクラスの定義\n",
        "4. データ拡張とテンソル変換の設定\n",
        "5. モデルのトレーニングと評価関数の定義\n",
        "6. 早期終了機能の実装\n",
        "7. 特定のカラムに対するモデル作成・トレーニング関数\n",
        "   - 四分位数に基づくデータフィルタリング\n",
        "   - データセットの分割とデータローダーの作成\n",
        "   - モデルのセットアップ、損失関数、最適化アルゴリズムの定義\n",
        "   - トレーニングループ（早期終了を含む）\n",
        "8. メインループ：各カラム（年齢、AC、SBP、DBP、HDLC、TG、BS）に対して\n",
        "   モデルを作成・トレーニングし、結果を保存\n",
        "\n",
        "このスクリプトは、各代謝指標の上位25%と下位25%のデータを使用して\n",
        "バイナリ分類モデルを学習し、眼底画像からこれらの代謝異常を\n",
        "予測することを目的としています。\n",
        "\"\"\"\n",
        "\n",
        "このコメントアウトは、スクリプト全体の目的と主な機能を簡潔に説明しています。\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import timm\n",
        "import time\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Google Driveのマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 必要なライブラリのインストール\n",
        "!pip install timm --quiet\n",
        "!pip install ranger-adabelief==0.1.0 --quiet\n",
        "\n",
        "# Set device for training (use GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/drive/MyDrive/Deep_learning/Fundus_metabolic/label_train.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define the columns to process\n",
        "columns = ['age', 'AC', 'SBP', 'DBP', 'HDLC', 'TG', 'BS']\n",
        "\n",
        "# Dataset class\n",
        "class FilteredFundusDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        label = self.data.iloc[idx, 8]  # Assuming 'METS' is the label as in the original model script\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Data transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Early Stopping class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "# Function to create and train model for a specific column\n",
        "def create_and_train_model(column):\n",
        "    # Calculate the 25th and 75th percentiles for the column\n",
        "    lower_quartile = data[column].quantile(0.25)\n",
        "    upper_quartile = data[column].quantile(0.75)\n",
        "\n",
        "    # Filter the data for filenames where the column value is in the lower 1/4 and upper 1/4\n",
        "    lower_quartile_files = data[data[column] <= lower_quartile]['filename'].tolist()\n",
        "    upper_quartile_files = data[data[column] >= upper_quartile]['filename'].tolist()\n",
        "\n",
        "    # Combine the filtered filenames\n",
        "    selected_filenames = lower_quartile_files + upper_quartile_files\n",
        "    filtered_data = data[data['filename'].isin(selected_filenames)]\n",
        "\n",
        "    # Split the filtered dataset into training and validation sets\n",
        "    train_data, val_data = train_test_split(filtered_data, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Save the filtered train and validation datasets to new CSV files\n",
        "    filtered_train_csv = f'/content/filtered_train_{column}.csv'\n",
        "    filtered_val_csv = f'/content/filtered_val_{column}.csv'\n",
        "    train_data.to_csv(filtered_train_csv, index=False)\n",
        "    val_data.to_csv(filtered_val_csv, index=False)\n",
        "\n",
        "    # Create dataset objects for the filtered data\n",
        "    train_dataset = FilteredFundusDataset(filtered_train_csv, img_dir, transform_train)\n",
        "    val_dataset = FilteredFundusDataset(filtered_val_csv, img_dir, transform_val)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "    # Model setup\n",
        "    model = nn.Sequential(\n",
        "        timm.create_model('repvgg_a0.rvgg_in1k', pretrained=True, num_classes=1000),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(1000, 2)\n",
        "    ).to(device)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.002)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 200\n",
        "    patience = 10\n",
        "    early_stopping = EarlyStopping(patience=patience)\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        early_stopping(val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Set the image directory\n",
        "img_dir = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_224px\"\n",
        "\n",
        "# Main loop to process each column\n",
        "for column in columns:\n",
        "    print(f\"Processing column: {column}\")\n",
        "    model, history = create_and_train_model(column)\n",
        "\n",
        "    # Save the model\n",
        "    save_path = f\"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/quartile/model_{column}.pth\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model for {column} saved to {save_path}\")\n",
        "\n",
        "    # Save the history as CSV\n",
        "    history_df = pd.DataFrame(history)\n",
        "    history_path = os.path.join(save_path, f\"history_{column}.csv\")\n",
        "    history_df.to_csv(history_path, index=False)\n",
        "    print(f\"Training history for {column} saved to {history_path}\")\n",
        "\n",
        "    print(f\"Completed processing for {column}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "print(\"All columns processed and models saved.\")\n"
      ],
      "metadata": {
        "id": "5i_1jpEiX9sD",
        "outputId": "1b321d88-93e8-48e0-c4ec-c974ef43d37c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "41a6b16c207c421dbe8f1a093ee0d6a2",
            "afb8f1a9c75e48b0a7399a50462ce163",
            "5f70dcc1babd403395e0f8ceaf1a15f3",
            "d824bf5c9ea248d1ac195b5bbf015424",
            "1e4d9377e8da48978c046456e24a978f",
            "8b5d99bdbde14725b89847f4576f75dd",
            "95a4f6cb9d614395952b4708850a763d",
            "6e1ddc70fe4a4c4fb0a1114aa7ed2bd4",
            "58a6aabd14a242dda9b12464de68182e",
            "4567ba105cac4f09996084c30fef170b",
            "7ecc2cc205cc48ceb663d30c15942aaa"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "Processing column: age\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/36.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41a6b16c207c421dbe8f1a093ee0d6a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "Train Loss: 1.1398, Train Acc: 0.5171\n",
            "Val Loss: 1.0725, Val Acc: 0.5232\n",
            "Epoch 2/200\n",
            "Train Loss: 0.7312, Train Acc: 0.5152\n",
            "Val Loss: 0.6960, Val Acc: 0.5058\n",
            "Epoch 3/200\n",
            "Train Loss: 0.7310, Train Acc: 0.5331\n",
            "Val Loss: 0.6908, Val Acc: 0.5290\n",
            "Epoch 4/200\n",
            "Train Loss: 0.7292, Train Acc: 0.5200\n",
            "Val Loss: 0.6767, Val Acc: 0.5425\n",
            "Epoch 5/200\n",
            "Train Loss: 0.7207, Train Acc: 0.5321\n",
            "Val Loss: 0.6842, Val Acc: 0.5328\n",
            "Epoch 6/200\n",
            "Train Loss: 0.7250, Train Acc: 0.5162\n",
            "Val Loss: 0.6721, Val Acc: 0.6062\n",
            "Epoch 7/200\n",
            "Train Loss: 0.7116, Train Acc: 0.5321\n",
            "Val Loss: 0.7278, Val Acc: 0.5541\n",
            "Epoch 8/200\n",
            "Train Loss: 0.6989, Train Acc: 0.5509\n",
            "Val Loss: 0.7527, Val Acc: 0.6004\n",
            "Epoch 9/200\n",
            "Train Loss: 0.7124, Train Acc: 0.5437\n",
            "Val Loss: 0.6861, Val Acc: 0.5309\n",
            "Epoch 10/200\n",
            "Train Loss: 0.6897, Train Acc: 0.5519\n",
            "Val Loss: 0.7061, Val Acc: 0.5251\n",
            "Epoch 11/200\n",
            "Train Loss: 0.6990, Train Acc: 0.5408\n",
            "Val Loss: 0.7380, Val Acc: 0.5251\n",
            "Epoch 12/200\n",
            "Train Loss: 0.7032, Train Acc: 0.5674\n",
            "Val Loss: 0.6737, Val Acc: 0.5792\n",
            "Epoch 13/200\n",
            "Train Loss: 0.6757, Train Acc: 0.5901\n",
            "Val Loss: 0.6665, Val Acc: 0.6081\n",
            "Epoch 14/200\n",
            "Train Loss: 0.6661, Train Acc: 0.6079\n",
            "Val Loss: 0.6608, Val Acc: 0.6216\n",
            "Epoch 15/200\n",
            "Train Loss: 0.6625, Train Acc: 0.6127\n",
            "Val Loss: 0.6610, Val Acc: 0.6081\n",
            "Epoch 16/200\n",
            "Train Loss: 0.6674, Train Acc: 0.5973\n",
            "Val Loss: 0.6531, Val Acc: 0.6371\n",
            "Epoch 17/200\n",
            "Train Loss: 0.6620, Train Acc: 0.6166\n",
            "Val Loss: 0.6536, Val Acc: 0.6293\n",
            "Epoch 18/200\n",
            "Train Loss: 0.6558, Train Acc: 0.6137\n",
            "Val Loss: 0.6535, Val Acc: 0.6409\n",
            "Epoch 19/200\n",
            "Train Loss: 0.6565, Train Acc: 0.6229\n",
            "Val Loss: 0.6538, Val Acc: 0.6274\n",
            "Epoch 20/200\n",
            "Train Loss: 0.6581, Train Acc: 0.6229\n",
            "Val Loss: 0.6572, Val Acc: 0.6313\n",
            "Epoch 21/200\n",
            "Train Loss: 0.6538, Train Acc: 0.6243\n",
            "Val Loss: 0.6539, Val Acc: 0.6274\n",
            "Epoch 22/200\n",
            "Train Loss: 0.6548, Train Acc: 0.6234\n",
            "Val Loss: 0.6552, Val Acc: 0.6351\n",
            "Epoch 23/200\n",
            "Train Loss: 0.6526, Train Acc: 0.6282\n",
            "Val Loss: 0.6547, Val Acc: 0.6371\n",
            "Epoch 24/200\n",
            "Train Loss: 0.6537, Train Acc: 0.6335\n",
            "Val Loss: 0.6513, Val Acc: 0.6293\n",
            "Epoch 25/200\n",
            "Train Loss: 0.6489, Train Acc: 0.6234\n",
            "Val Loss: 0.6528, Val Acc: 0.6293\n",
            "Epoch 26/200\n",
            "Train Loss: 0.6446, Train Acc: 0.6422\n",
            "Val Loss: 0.6553, Val Acc: 0.6274\n",
            "Epoch 27/200\n",
            "Train Loss: 0.6476, Train Acc: 0.6350\n",
            "Val Loss: 0.6554, Val Acc: 0.6313\n",
            "Epoch 28/200\n",
            "Train Loss: 0.6551, Train Acc: 0.6219\n",
            "Val Loss: 0.6497, Val Acc: 0.6255\n",
            "Epoch 29/200\n",
            "Train Loss: 0.6417, Train Acc: 0.6412\n",
            "Val Loss: 0.6476, Val Acc: 0.6293\n",
            "Epoch 30/200\n",
            "Train Loss: 0.6518, Train Acc: 0.6345\n",
            "Val Loss: 0.6481, Val Acc: 0.6293\n",
            "Epoch 31/200\n",
            "Train Loss: 0.6520, Train Acc: 0.6287\n",
            "Val Loss: 0.6513, Val Acc: 0.6371\n",
            "Epoch 32/200\n",
            "Train Loss: 0.6534, Train Acc: 0.6296\n",
            "Val Loss: 0.6521, Val Acc: 0.6255\n",
            "Epoch 33/200\n",
            "Train Loss: 0.6545, Train Acc: 0.6118\n",
            "Val Loss: 0.6488, Val Acc: 0.6332\n",
            "Epoch 34/200\n",
            "Train Loss: 0.6511, Train Acc: 0.6330\n",
            "Val Loss: 0.6485, Val Acc: 0.6293\n",
            "Epoch 35/200\n",
            "Train Loss: 0.6510, Train Acc: 0.6296\n",
            "Val Loss: 0.6489, Val Acc: 0.6293\n",
            "Epoch 36/200\n",
            "Train Loss: 0.6491, Train Acc: 0.6359\n",
            "Val Loss: 0.6476, Val Acc: 0.6351\n",
            "Epoch 37/200\n",
            "Train Loss: 0.6493, Train Acc: 0.6301\n",
            "Val Loss: 0.6484, Val Acc: 0.6351\n",
            "Epoch 38/200\n",
            "Train Loss: 0.6523, Train Acc: 0.6301\n",
            "Val Loss: 0.6474, Val Acc: 0.6351\n",
            "Epoch 39/200\n",
            "Train Loss: 0.6447, Train Acc: 0.6272\n",
            "Val Loss: 0.6508, Val Acc: 0.6332\n",
            "Epoch 40/200\n",
            "Train Loss: 0.6498, Train Acc: 0.6383\n",
            "Val Loss: 0.6504, Val Acc: 0.6274\n",
            "Epoch 41/200\n",
            "Train Loss: 0.6544, Train Acc: 0.6234\n",
            "Val Loss: 0.6493, Val Acc: 0.6313\n",
            "Epoch 42/200\n",
            "Train Loss: 0.6471, Train Acc: 0.6369\n",
            "Val Loss: 0.6500, Val Acc: 0.6371\n",
            "Epoch 43/200\n",
            "Train Loss: 0.6505, Train Acc: 0.6258\n",
            "Val Loss: 0.6493, Val Acc: 0.6351\n",
            "Epoch 44/200\n",
            "Train Loss: 0.6440, Train Acc: 0.6408\n",
            "Val Loss: 0.6476, Val Acc: 0.6332\n",
            "Epoch 45/200\n",
            "Train Loss: 0.6490, Train Acc: 0.6258\n",
            "Val Loss: 0.6506, Val Acc: 0.6274\n",
            "Epoch 46/200\n",
            "Train Loss: 0.6446, Train Acc: 0.6325\n",
            "Val Loss: 0.6478, Val Acc: 0.6293\n",
            "Epoch 47/200\n",
            "Train Loss: 0.6481, Train Acc: 0.6325\n",
            "Val Loss: 0.6502, Val Acc: 0.6332\n",
            "Epoch 48/200\n",
            "Train Loss: 0.6499, Train Acc: 0.6210\n",
            "Val Loss: 0.6515, Val Acc: 0.6255\n",
            "Early stopping triggered\n",
            "Model for age saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/quartile/model_age.pth\n",
            "Training history for age saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/quartile/history_age.pth\n",
            "Completed processing for age\n",
            "----------------------------------------\n",
            "Processing column: AC\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "Train Loss: 1.0656, Train Acc: 0.5338\n",
            "Val Loss: 0.7459, Val Acc: 0.5258\n",
            "Epoch 2/200\n",
            "Train Loss: 0.7121, Train Acc: 0.5586\n",
            "Val Loss: 0.6932, Val Acc: 0.5298\n",
            "Epoch 3/200\n",
            "Train Loss: 0.6983, Train Acc: 0.5616\n",
            "Val Loss: 0.6921, Val Acc: 0.5337\n",
            "Epoch 4/200\n",
            "Train Loss: 0.7244, Train Acc: 0.5412\n",
            "Val Loss: 0.7124, Val Acc: 0.5615\n",
            "Epoch 5/200\n",
            "Train Loss: 0.7087, Train Acc: 0.5492\n",
            "Val Loss: 0.7014, Val Acc: 0.5615\n",
            "Epoch 6/200\n",
            "Train Loss: 0.7364, Train Acc: 0.5392\n",
            "Val Loss: 0.6882, Val Acc: 0.5615\n",
            "Epoch 7/200\n",
            "Train Loss: 0.7033, Train Acc: 0.5487\n",
            "Val Loss: 0.7554, Val Acc: 0.5615\n",
            "Epoch 8/200\n",
            "Train Loss: 0.6961, Train Acc: 0.5655\n",
            "Val Loss: 0.6886, Val Acc: 0.5615\n",
            "Epoch 9/200\n",
            "Train Loss: 0.6948, Train Acc: 0.5745\n",
            "Val Loss: 0.7398, Val Acc: 0.5615\n",
            "Epoch 10/200\n",
            "Train Loss: 0.6961, Train Acc: 0.5531\n",
            "Val Loss: 0.6879, Val Acc: 0.5615\n",
            "Epoch 11/200\n",
            "Train Loss: 0.6932, Train Acc: 0.5675\n",
            "Val Loss: 1.8177, Val Acc: 0.5615\n",
            "Epoch 12/200\n",
            "Train Loss: 0.6940, Train Acc: 0.5531\n",
            "Val Loss: 0.7184, Val Acc: 0.5615\n",
            "Epoch 13/200\n",
            "Train Loss: 0.6918, Train Acc: 0.5660\n",
            "Val Loss: 0.6808, Val Acc: 0.5615\n",
            "Epoch 14/200\n",
            "Train Loss: 0.6805, Train Acc: 0.5725\n",
            "Val Loss: 0.6802, Val Acc: 0.5774\n",
            "Epoch 15/200\n",
            "Train Loss: 0.6735, Train Acc: 0.5829\n",
            "Val Loss: 0.6664, Val Acc: 0.5913\n",
            "Epoch 16/200\n",
            "Train Loss: 0.6842, Train Acc: 0.5799\n",
            "Val Loss: 0.6713, Val Acc: 0.5813\n",
            "Epoch 17/200\n",
            "Train Loss: 0.6794, Train Acc: 0.5765\n",
            "Val Loss: 0.7322, Val Acc: 0.5615\n",
            "Epoch 18/200\n",
            "Train Loss: 0.6772, Train Acc: 0.5799\n",
            "Val Loss: 0.6801, Val Acc: 0.5615\n",
            "Epoch 19/200\n",
            "Train Loss: 0.6769, Train Acc: 0.5874\n",
            "Val Loss: 0.6743, Val Acc: 0.5615\n",
            "Epoch 20/200\n",
            "Train Loss: 0.6772, Train Acc: 0.5909\n",
            "Val Loss: 0.6664, Val Acc: 0.5933\n",
            "Epoch 21/200\n",
            "Train Loss: 0.6774, Train Acc: 0.5859\n",
            "Val Loss: 0.7585, Val Acc: 0.5615\n",
            "Epoch 22/200\n",
            "Train Loss: 0.6660, Train Acc: 0.5988\n",
            "Val Loss: 0.6636, Val Acc: 0.5952\n",
            "Epoch 23/200\n",
            "Train Loss: 0.6611, Train Acc: 0.5973\n",
            "Val Loss: 0.6649, Val Acc: 0.5794\n",
            "Epoch 24/200\n",
            "Train Loss: 0.6627, Train Acc: 0.5973\n",
            "Val Loss: 0.6548, Val Acc: 0.6190\n",
            "Epoch 25/200\n",
            "Train Loss: 0.6585, Train Acc: 0.6013\n",
            "Val Loss: 0.6617, Val Acc: 0.5754\n",
            "Epoch 26/200\n",
            "Train Loss: 0.6570, Train Acc: 0.6142\n",
            "Val Loss: 0.6525, Val Acc: 0.6210\n",
            "Epoch 27/200\n",
            "Train Loss: 0.6569, Train Acc: 0.6038\n",
            "Val Loss: 0.6585, Val Acc: 0.6091\n",
            "Epoch 28/200\n",
            "Train Loss: 0.6542, Train Acc: 0.6167\n",
            "Val Loss: 0.6547, Val Acc: 0.5952\n",
            "Epoch 29/200\n",
            "Train Loss: 0.6535, Train Acc: 0.6127\n",
            "Val Loss: 0.6533, Val Acc: 0.5952\n",
            "Epoch 30/200\n",
            "Train Loss: 0.6530, Train Acc: 0.6192\n",
            "Val Loss: 0.6606, Val Acc: 0.5913\n",
            "Epoch 31/200\n",
            "Train Loss: 0.6548, Train Acc: 0.6053\n",
            "Val Loss: 0.6607, Val Acc: 0.5933\n",
            "Epoch 32/200\n",
            "Train Loss: 0.6520, Train Acc: 0.6182\n",
            "Val Loss: 0.6487, Val Acc: 0.6290\n",
            "Epoch 33/200\n",
            "Train Loss: 0.6514, Train Acc: 0.6182\n",
            "Val Loss: 0.6525, Val Acc: 0.6171\n",
            "Epoch 34/200\n",
            "Train Loss: 0.6506, Train Acc: 0.6182\n",
            "Val Loss: 0.6523, Val Acc: 0.5992\n",
            "Epoch 35/200\n",
            "Train Loss: 0.6507, Train Acc: 0.6137\n",
            "Val Loss: 0.6552, Val Acc: 0.6032\n",
            "Epoch 36/200\n",
            "Train Loss: 0.6478, Train Acc: 0.6207\n",
            "Val Loss: 0.6576, Val Acc: 0.6171\n",
            "Epoch 37/200\n",
            "Train Loss: 0.6519, Train Acc: 0.6202\n",
            "Val Loss: 0.6527, Val Acc: 0.6052\n",
            "Epoch 38/200\n",
            "Train Loss: 0.6506, Train Acc: 0.6246\n",
            "Val Loss: 0.6519, Val Acc: 0.6091\n",
            "Epoch 39/200\n",
            "Train Loss: 0.6490, Train Acc: 0.6182\n",
            "Val Loss: 0.6493, Val Acc: 0.6230\n",
            "Epoch 40/200\n",
            "Train Loss: 0.6456, Train Acc: 0.6226\n",
            "Val Loss: 0.6492, Val Acc: 0.6111\n",
            "Epoch 41/200\n",
            "Train Loss: 0.6458, Train Acc: 0.6236\n",
            "Val Loss: 0.6489, Val Acc: 0.6111\n",
            "Epoch 42/200\n",
            "Train Loss: 0.6499, Train Acc: 0.6202\n",
            "Val Loss: 0.6485, Val Acc: 0.6190\n",
            "Epoch 43/200\n",
            "Train Loss: 0.6457, Train Acc: 0.6216\n",
            "Val Loss: 0.6476, Val Acc: 0.6310\n",
            "Epoch 44/200\n",
            "Train Loss: 0.6448, Train Acc: 0.6221\n",
            "Val Loss: 0.6486, Val Acc: 0.6250\n",
            "Epoch 45/200\n",
            "Train Loss: 0.6431, Train Acc: 0.6221\n",
            "Val Loss: 0.6506, Val Acc: 0.6151\n",
            "Epoch 46/200\n",
            "Train Loss: 0.6453, Train Acc: 0.6207\n",
            "Val Loss: 0.6487, Val Acc: 0.6190\n",
            "Epoch 47/200\n",
            "Train Loss: 0.6420, Train Acc: 0.6266\n",
            "Val Loss: 0.6496, Val Acc: 0.6190\n",
            "Epoch 48/200\n",
            "Train Loss: 0.6448, Train Acc: 0.6261\n",
            "Val Loss: 0.6492, Val Acc: 0.6250\n",
            "Epoch 49/200\n",
            "Train Loss: 0.6500, Train Acc: 0.6202\n",
            "Val Loss: 0.6491, Val Acc: 0.6250\n",
            "Epoch 50/200\n",
            "Train Loss: 0.6420, Train Acc: 0.6246\n",
            "Val Loss: 0.6491, Val Acc: 0.6250\n",
            "Epoch 51/200\n",
            "Train Loss: 0.6451, Train Acc: 0.6241\n",
            "Val Loss: 0.6497, Val Acc: 0.6171\n",
            "Epoch 52/200\n",
            "Train Loss: 0.6353, Train Acc: 0.6306\n",
            "Val Loss: 0.6517, Val Acc: 0.6210\n",
            "Epoch 53/200\n",
            "Train Loss: 0.6374, Train Acc: 0.6261\n",
            "Val Loss: 0.6497, Val Acc: 0.6310\n",
            "Early stopping triggered\n",
            "Model for AC saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/quartile/model_AC.pth\n",
            "Training history for AC saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/quartile/history_AC.pth\n",
            "Completed processing for AC\n",
            "----------------------------------------\n",
            "Processing column: SBP\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "Train Loss: 1.0266, Train Acc: 0.5116\n",
            "Val Loss: 0.7275, Val Acc: 0.5590\n",
            "Epoch 2/200\n",
            "Train Loss: 0.7159, Train Acc: 0.5252\n",
            "Val Loss: 0.7708, Val Acc: 0.5551\n",
            "Epoch 3/200\n",
            "Train Loss: 0.7114, Train Acc: 0.5353\n",
            "Val Loss: 0.6925, Val Acc: 0.5667\n",
            "Epoch 4/200\n",
            "Train Loss: 0.7232, Train Acc: 0.5271\n",
            "Val Loss: 0.6934, Val Acc: 0.5590\n",
            "Epoch 5/200\n",
            "Train Loss: 0.7042, Train Acc: 0.5518\n",
            "Val Loss: 0.6952, Val Acc: 0.5609\n",
            "Epoch 6/200\n",
            "Train Loss: 0.7015, Train Acc: 0.5499\n",
            "Val Loss: 0.6528, Val Acc: 0.6383\n",
            "Epoch 7/200\n",
            "Train Loss: 0.6983, Train Acc: 0.5528\n",
            "Val Loss: 0.6624, Val Acc: 0.6170\n",
            "Epoch 8/200\n",
            "Train Loss: 0.6925, Train Acc: 0.5474\n",
            "Val Loss: 0.6666, Val Acc: 0.5880\n",
            "Epoch 9/200\n",
            "Train Loss: 0.6910, Train Acc: 0.5653\n",
            "Val Loss: 0.7880, Val Acc: 0.5841\n",
            "Epoch 10/200\n",
            "Train Loss: 0.6986, Train Acc: 0.5503\n",
            "Val Loss: 0.6786, Val Acc: 0.5764\n",
            "Epoch 11/200\n",
            "Train Loss: 0.6975, Train Acc: 0.5687\n",
            "Val Loss: 0.6504, Val Acc: 0.6422\n",
            "Epoch 12/200\n",
            "Train Loss: 0.6951, Train Acc: 0.5649\n",
            "Val Loss: 0.6500, Val Acc: 0.6190\n",
            "Epoch 13/200\n",
            "Train Loss: 0.6886, Train Acc: 0.5581\n",
            "Val Loss: 0.6674, Val Acc: 0.5996\n",
            "Epoch 14/200\n",
            "Train Loss: 0.6887, Train Acc: 0.5663\n",
            "Val Loss: 0.6592, Val Acc: 0.5880\n",
            "Epoch 15/200\n",
            "Train Loss: 0.6869, Train Acc: 0.5774\n",
            "Val Loss: 0.6875, Val Acc: 0.5590\n",
            "Epoch 16/200\n",
            "Train Loss: 0.6814, Train Acc: 0.5678\n",
            "Val Loss: 0.6463, Val Acc: 0.6093\n",
            "Epoch 17/200\n",
            "Train Loss: 0.6785, Train Acc: 0.5779\n",
            "Val Loss: 0.6480, Val Acc: 0.6383\n",
            "Epoch 18/200\n",
            "Train Loss: 0.6705, Train Acc: 0.5934\n",
            "Val Loss: 0.6781, Val Acc: 0.5899\n",
            "Epoch 19/200\n",
            "Train Loss: 0.6731, Train Acc: 0.5954\n",
            "Val Loss: 0.6807, Val Acc: 0.5590\n",
            "Epoch 20/200\n",
            "Train Loss: 0.6755, Train Acc: 0.5731\n",
            "Val Loss: 0.6532, Val Acc: 0.5841\n",
            "Epoch 21/200\n",
            "Train Loss: 0.6831, Train Acc: 0.5668\n",
            "Val Loss: 0.6410, Val Acc: 0.6151\n",
            "Epoch 22/200\n",
            "Train Loss: 0.6769, Train Acc: 0.5886\n",
            "Val Loss: 0.6479, Val Acc: 0.6035\n",
            "Epoch 23/200\n",
            "Train Loss: 0.6735, Train Acc: 0.5997\n",
            "Val Loss: 0.6461, Val Acc: 0.6364\n",
            "Epoch 24/200\n",
            "Train Loss: 0.6647, Train Acc: 0.6007\n",
            "Val Loss: 0.6516, Val Acc: 0.6015\n",
            "Epoch 25/200\n",
            "Train Loss: 0.6745, Train Acc: 0.5721\n",
            "Val Loss: 0.6386, Val Acc: 0.6538\n",
            "Epoch 26/200\n",
            "Train Loss: 0.6594, Train Acc: 0.6026\n",
            "Val Loss: 0.6660, Val Acc: 0.6209\n",
            "Epoch 27/200\n",
            "Train Loss: 0.6656, Train Acc: 0.6016\n",
            "Val Loss: 0.6414, Val Acc: 0.6518\n",
            "Epoch 28/200\n",
            "Train Loss: 0.6642, Train Acc: 0.6070\n",
            "Val Loss: 0.6542, Val Acc: 0.5803\n",
            "Epoch 29/200\n",
            "Train Loss: 0.6665, Train Acc: 0.5813\n",
            "Val Loss: 0.6254, Val Acc: 0.6634\n",
            "Epoch 30/200\n",
            "Train Loss: 0.6723, Train Acc: 0.6012\n",
            "Val Loss: 0.6558, Val Acc: 0.6248\n",
            "Epoch 31/200\n",
            "Train Loss: 0.6708, Train Acc: 0.5949\n",
            "Val Loss: 0.6605, Val Acc: 0.5687\n",
            "Epoch 32/200\n",
            "Train Loss: 0.6584, Train Acc: 0.6205\n",
            "Val Loss: 0.6871, Val Acc: 0.6151\n",
            "Epoch 33/200\n",
            "Train Loss: 0.6646, Train Acc: 0.6123\n",
            "Val Loss: 0.6449, Val Acc: 0.6190\n",
            "Epoch 34/200\n",
            "Train Loss: 0.6604, Train Acc: 0.6099\n",
            "Val Loss: 0.6246, Val Acc: 0.6692\n",
            "Epoch 35/200\n",
            "Train Loss: 0.6600, Train Acc: 0.6104\n",
            "Val Loss: 0.6211, Val Acc: 0.6557\n",
            "Epoch 36/200\n",
            "Train Loss: 0.6784, Train Acc: 0.5954\n",
            "Val Loss: 0.6569, Val Acc: 0.6325\n",
            "Epoch 37/200\n",
            "Train Loss: 0.6625, Train Acc: 0.6012\n",
            "Val Loss: 0.6214, Val Acc: 0.6615\n",
            "Epoch 38/200\n",
            "Train Loss: 0.6627, Train Acc: 0.6036\n",
            "Val Loss: 0.6939, Val Acc: 0.5648\n",
            "Epoch 39/200\n",
            "Train Loss: 0.6585, Train Acc: 0.6171\n",
            "Val Loss: 1.7975, Val Acc: 0.5551\n",
            "Epoch 40/200\n",
            "Train Loss: 0.6726, Train Acc: 0.5963\n",
            "Val Loss: 0.6165, Val Acc: 0.6750\n",
            "Epoch 41/200\n",
            "Train Loss: 0.6760, Train Acc: 0.5910\n",
            "Val Loss: 0.6427, Val Acc: 0.6576\n",
            "Epoch 42/200\n",
            "Train Loss: 0.6604, Train Acc: 0.6016\n",
            "Val Loss: 0.6618, Val Acc: 0.6074\n",
            "Epoch 43/200\n",
            "Train Loss: 0.6545, Train Acc: 0.6225\n",
            "Val Loss: 0.6664, Val Acc: 0.5880\n",
            "Epoch 44/200\n",
            "Train Loss: 0.6598, Train Acc: 0.6128\n",
            "Val Loss: 0.6384, Val Acc: 0.6209\n",
            "Epoch 45/200\n",
            "Train Loss: 0.6417, Train Acc: 0.6346\n",
            "Val Loss: 0.7193, Val Acc: 0.6190\n",
            "Epoch 46/200\n",
            "Train Loss: 0.6512, Train Acc: 0.6118\n",
            "Val Loss: 0.6418, Val Acc: 0.6151\n",
            "Epoch 47/200\n",
            "Train Loss: 0.6448, Train Acc: 0.6249\n",
            "Val Loss: 0.6201, Val Acc: 0.6596\n",
            "Epoch 48/200\n",
            "Train Loss: 0.6327, Train Acc: 0.6481\n",
            "Val Loss: 0.6188, Val Acc: 0.6576\n",
            "Epoch 49/200\n",
            "Train Loss: 0.6330, Train Acc: 0.6515\n",
            "Val Loss: 0.6162, Val Acc: 0.6596\n",
            "Epoch 50/200\n",
            "Train Loss: 0.6293, Train Acc: 0.6534\n",
            "Val Loss: 0.6143, Val Acc: 0.6673\n",
            "Epoch 51/200\n",
            "Train Loss: 0.6292, Train Acc: 0.6476\n",
            "Val Loss: 0.6160, Val Acc: 0.6770\n",
            "Epoch 52/200\n",
            "Train Loss: 0.6242, Train Acc: 0.6621\n",
            "Val Loss: 0.6156, Val Acc: 0.6576\n",
            "Epoch 53/200\n",
            "Train Loss: 0.6298, Train Acc: 0.6331\n",
            "Val Loss: 0.6129, Val Acc: 0.6750\n",
            "Epoch 54/200\n",
            "Train Loss: 0.6205, Train Acc: 0.6520\n",
            "Val Loss: 0.6074, Val Acc: 0.6867\n",
            "Epoch 55/200\n",
            "Train Loss: 0.6227, Train Acc: 0.6394\n",
            "Val Loss: 0.6144, Val Acc: 0.6615\n",
            "Epoch 56/200\n",
            "Train Loss: 0.6194, Train Acc: 0.6505\n",
            "Val Loss: 0.6086, Val Acc: 0.6750\n",
            "Epoch 57/200\n",
            "Train Loss: 0.6280, Train Acc: 0.6462\n",
            "Val Loss: 0.6093, Val Acc: 0.6712\n",
            "Epoch 58/200\n",
            "Train Loss: 0.6181, Train Acc: 0.6665\n",
            "Val Loss: 0.6062, Val Acc: 0.6750\n",
            "Epoch 59/200\n",
            "Train Loss: 0.6259, Train Acc: 0.6438\n",
            "Val Loss: 0.6100, Val Acc: 0.6731\n",
            "Epoch 60/200\n",
            "Train Loss: 0.6234, Train Acc: 0.6515\n",
            "Val Loss: 0.6082, Val Acc: 0.6731\n",
            "Epoch 61/200\n",
            "Train Loss: 0.6113, Train Acc: 0.6684\n",
            "Val Loss: 0.6107, Val Acc: 0.6731\n",
            "Epoch 62/200\n",
            "Train Loss: 0.6157, Train Acc: 0.6510\n",
            "Val Loss: 0.6116, Val Acc: 0.6654\n",
            "Epoch 63/200\n",
            "Train Loss: 0.6124, Train Acc: 0.6588\n",
            "Val Loss: 0.6248, Val Acc: 0.6557\n",
            "Epoch 64/200\n",
            "Train Loss: 0.6164, Train Acc: 0.6559\n",
            "Val Loss: 0.6162, Val Acc: 0.6538\n",
            "Epoch 65/200\n",
            "Train Loss: 0.6098, Train Acc: 0.6588\n",
            "Val Loss: 0.6087, Val Acc: 0.6789\n",
            "Epoch 66/200\n",
            "Train Loss: 0.6099, Train Acc: 0.6617\n",
            "Val Loss: 0.6157, Val Acc: 0.6615\n",
            "Epoch 67/200\n",
            "Train Loss: 0.6116, Train Acc: 0.6626\n",
            "Val Loss: 0.6073, Val Acc: 0.6750\n",
            "Epoch 68/200\n",
            "Train Loss: 0.6141, Train Acc: 0.6525\n",
            "Val Loss: 0.6082, Val Acc: 0.6673\n",
            "Early stopping triggered\n",
            "Model for SBP saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/quartile/model_SBP.pth\n",
            "Training history for SBP saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/quartile/history_SBP.pth\n",
            "Completed processing for SBP\n",
            "----------------------------------------\n",
            "Processing column: DBP\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "Train Loss: 0.9607, Train Acc: 0.5091\n",
            "Val Loss: 0.8096, Val Acc: 0.5410\n",
            "Epoch 2/200\n",
            "Train Loss: 0.7456, Train Acc: 0.5315\n",
            "Val Loss: 0.6875, Val Acc: 0.5638\n",
            "Epoch 3/200\n",
            "Train Loss: 0.7549, Train Acc: 0.5134\n",
            "Val Loss: 0.6877, Val Acc: 0.5410\n",
            "Epoch 4/200\n",
            "Train Loss: 0.7298, Train Acc: 0.5243\n",
            "Val Loss: 0.6768, Val Acc: 0.5733\n",
            "Epoch 5/200\n",
            "Train Loss: 0.7452, Train Acc: 0.5115\n",
            "Val Loss: 0.7639, Val Acc: 0.4590\n",
            "Epoch 6/200\n",
            "Train Loss: 0.7050, Train Acc: 0.5324\n",
            "Val Loss: 0.6852, Val Acc: 0.5676\n",
            "Epoch 7/200\n",
            "Train Loss: 0.7135, Train Acc: 0.5224\n",
            "Val Loss: 0.6509, Val Acc: 0.6152\n",
            "Epoch 8/200\n",
            "Train Loss: 0.7110, Train Acc: 0.5363\n",
            "Val Loss: 0.6709, Val Acc: 0.5810\n",
            "Epoch 9/200\n",
            "Train Loss: 0.7104, Train Acc: 0.5472\n",
            "Val Loss: 0.7491, Val Acc: 0.4590\n",
            "Epoch 10/200\n",
            "Train Loss: 0.7190, Train Acc: 0.5358\n",
            "Val Loss: 0.6913, Val Acc: 0.5410\n",
            "Epoch 11/200\n",
            "Train Loss: 0.7125, Train Acc: 0.5200\n",
            "Val Loss: 0.6929, Val Acc: 0.5410\n",
            "Epoch 12/200\n",
            "Train Loss: 0.7171, Train Acc: 0.5186\n",
            "Val Loss: 0.7391, Val Acc: 0.5410\n",
            "Epoch 13/200\n",
            "Train Loss: 0.7125, Train Acc: 0.5205\n",
            "Val Loss: 0.6849, Val Acc: 0.5562\n",
            "Epoch 14/200\n",
            "Train Loss: 0.6761, Train Acc: 0.5849\n",
            "Val Loss: 0.6517, Val Acc: 0.5676\n",
            "Epoch 15/200\n",
            "Train Loss: 0.6679, Train Acc: 0.5964\n",
            "Val Loss: 0.6389, Val Acc: 0.6000\n",
            "Epoch 16/200\n",
            "Train Loss: 0.6645, Train Acc: 0.6078\n",
            "Val Loss: 0.6369, Val Acc: 0.6019\n",
            "Epoch 17/200\n",
            "Train Loss: 0.6688, Train Acc: 0.5945\n",
            "Val Loss: 0.6355, Val Acc: 0.6400\n",
            "Epoch 18/200\n",
            "Train Loss: 0.6620, Train Acc: 0.6054\n",
            "Val Loss: 0.6333, Val Acc: 0.6362\n",
            "Epoch 19/200\n",
            "Train Loss: 0.6602, Train Acc: 0.6240\n",
            "Val Loss: 0.6318, Val Acc: 0.6438\n",
            "Epoch 20/200\n",
            "Train Loss: 0.6664, Train Acc: 0.6040\n",
            "Val Loss: 0.6336, Val Acc: 0.6552\n",
            "Epoch 21/200\n",
            "Train Loss: 0.6572, Train Acc: 0.5988\n",
            "Val Loss: 0.6351, Val Acc: 0.6286\n",
            "Epoch 22/200\n",
            "Train Loss: 0.6632, Train Acc: 0.6064\n",
            "Val Loss: 0.6261, Val Acc: 0.6305\n",
            "Epoch 23/200\n",
            "Train Loss: 0.6592, Train Acc: 0.6135\n",
            "Val Loss: 0.6276, Val Acc: 0.6400\n",
            "Epoch 24/200\n",
            "Train Loss: 0.6607, Train Acc: 0.6078\n",
            "Val Loss: 0.6375, Val Acc: 0.6324\n",
            "Epoch 25/200\n",
            "Train Loss: 0.6547, Train Acc: 0.6326\n",
            "Val Loss: 0.6273, Val Acc: 0.6705\n",
            "Epoch 26/200\n",
            "Train Loss: 0.6627, Train Acc: 0.6021\n",
            "Val Loss: 0.6246, Val Acc: 0.6590\n",
            "Epoch 27/200\n",
            "Train Loss: 0.6559, Train Acc: 0.6198\n",
            "Val Loss: 0.6392, Val Acc: 0.6171\n",
            "Epoch 28/200\n",
            "Train Loss: 0.6534, Train Acc: 0.6250\n",
            "Val Loss: 0.6298, Val Acc: 0.6343\n",
            "Epoch 29/200\n",
            "Train Loss: 0.6576, Train Acc: 0.6078\n",
            "Val Loss: 0.6374, Val Acc: 0.6229\n",
            "Epoch 30/200\n",
            "Train Loss: 0.6614, Train Acc: 0.6073\n",
            "Val Loss: 0.6381, Val Acc: 0.6133\n",
            "Epoch 31/200\n",
            "Train Loss: 0.6522, Train Acc: 0.6193\n",
            "Val Loss: 0.6251, Val Acc: 0.6552\n",
            "Epoch 32/200\n",
            "Train Loss: 0.6593, Train Acc: 0.6102\n",
            "Val Loss: 0.6196, Val Acc: 0.6610\n",
            "Epoch 33/200\n",
            "Train Loss: 0.6593, Train Acc: 0.6116\n",
            "Val Loss: 0.6244, Val Acc: 0.6533\n",
            "Epoch 34/200\n",
            "Train Loss: 0.6603, Train Acc: 0.6107\n",
            "Val Loss: 0.6230, Val Acc: 0.6533\n",
            "Epoch 35/200\n",
            "Train Loss: 0.6532, Train Acc: 0.6221\n",
            "Val Loss: 0.6274, Val Acc: 0.6438\n",
            "Epoch 36/200\n",
            "Train Loss: 0.6562, Train Acc: 0.6155\n",
            "Val Loss: 0.6421, Val Acc: 0.6152\n",
            "Epoch 37/200\n",
            "Train Loss: 0.6528, Train Acc: 0.6202\n",
            "Val Loss: 0.6230, Val Acc: 0.6419\n",
            "Epoch 38/200\n",
            "Train Loss: 0.6544, Train Acc: 0.6240\n",
            "Val Loss: 0.6339, Val Acc: 0.6381\n",
            "Epoch 39/200\n",
            "Train Loss: 0.6519, Train Acc: 0.6140\n",
            "Val Loss: 0.6257, Val Acc: 0.6324\n",
            "Epoch 40/200\n",
            "Train Loss: 0.6511, Train Acc: 0.5997\n",
            "Val Loss: 0.6234, Val Acc: 0.6533\n",
            "Epoch 41/200\n",
            "Train Loss: 0.6547, Train Acc: 0.6121\n",
            "Val Loss: 0.6239, Val Acc: 0.6381\n",
            "Epoch 42/200\n",
            "Train Loss: 0.6539, Train Acc: 0.6264\n",
            "Val Loss: 0.6234, Val Acc: 0.6343\n",
            "Early stopping triggered\n",
            "Model for DBP saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/quartile/model_DBP.pth\n",
            "Training history for DBP saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/quartile/history_DBP.pth\n",
            "Completed processing for DBP\n",
            "----------------------------------------\n",
            "Processing column: HDLC\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "Train Loss: 1.1638, Train Acc: 0.5154\n",
            "Val Loss: 1.1115, Val Acc: 0.4887\n",
            "Epoch 2/200\n",
            "Train Loss: 0.7337, Train Acc: 0.5087\n",
            "Val Loss: 0.8904, Val Acc: 0.4943\n",
            "Epoch 3/200\n",
            "Train Loss: 0.7184, Train Acc: 0.5144\n",
            "Val Loss: 0.7465, Val Acc: 0.5075\n",
            "Epoch 4/200\n",
            "Train Loss: 0.7235, Train Acc: 0.5149\n",
            "Val Loss: 0.7350, Val Acc: 0.4962\n",
            "Epoch 5/200\n",
            "Train Loss: 0.7163, Train Acc: 0.5106\n",
            "Val Loss: 0.7800, Val Acc: 0.5132\n",
            "Epoch 6/200\n",
            "Train Loss: 0.7352, Train Acc: 0.5102\n",
            "Val Loss: 0.7507, Val Acc: 0.4830\n",
            "Epoch 7/200\n",
            "Train Loss: 0.7198, Train Acc: 0.5305\n",
            "Val Loss: 0.7774, Val Acc: 0.5019\n",
            "Epoch 8/200\n",
            "Train Loss: 0.7218, Train Acc: 0.5050\n",
            "Val Loss: 0.7059, Val Acc: 0.5038\n",
            "Epoch 9/200\n",
            "Train Loss: 0.7152, Train Acc: 0.5338\n",
            "Val Loss: 0.6834, Val Acc: 0.5585\n",
            "Epoch 10/200\n",
            "Train Loss: 0.7036, Train Acc: 0.5470\n",
            "Val Loss: 0.7165, Val Acc: 0.5377\n",
            "Epoch 11/200\n",
            "Train Loss: 0.7059, Train Acc: 0.5536\n",
            "Val Loss: 0.7293, Val Acc: 0.4811\n",
            "Epoch 12/200\n",
            "Train Loss: 0.7114, Train Acc: 0.5338\n",
            "Val Loss: 0.7308, Val Acc: 0.5283\n",
            "Epoch 13/200\n",
            "Train Loss: 0.7050, Train Acc: 0.5338\n",
            "Val Loss: 0.7138, Val Acc: 0.5302\n",
            "Epoch 14/200\n",
            "Train Loss: 0.7104, Train Acc: 0.5073\n",
            "Val Loss: 0.8154, Val Acc: 0.5226\n",
            "Epoch 15/200\n",
            "Train Loss: 0.7018, Train Acc: 0.5239\n",
            "Val Loss: 1.3352, Val Acc: 0.5245\n",
            "Epoch 16/200\n",
            "Train Loss: 0.7026, Train Acc: 0.5286\n",
            "Val Loss: 397.8565, Val Acc: 0.5321\n",
            "Epoch 17/200\n",
            "Train Loss: 0.6906, Train Acc: 0.5394\n",
            "Val Loss: 203.3998, Val Acc: 0.5208\n",
            "Epoch 18/200\n",
            "Train Loss: 0.6868, Train Acc: 0.5626\n",
            "Val Loss: 46.4397, Val Acc: 0.5566\n",
            "Epoch 19/200\n",
            "Train Loss: 0.6871, Train Acc: 0.5522\n",
            "Val Loss: 228.3355, Val Acc: 0.5509\n",
            "Early stopping triggered\n",
            "Model for HDLC saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/quartile/model_HDLC.pth\n",
            "Training history for HDLC saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/quartile/history_HDLC.pth\n",
            "Completed processing for HDLC\n",
            "----------------------------------------\n",
            "Processing column: TG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "Train Loss: 1.2906, Train Acc: 0.4980\n",
            "Val Loss: 0.8541, Val Acc: 0.5138\n",
            "Epoch 2/200\n",
            "Train Loss: 0.7287, Train Acc: 0.4980\n",
            "Val Loss: 0.6923, Val Acc: 0.5553\n",
            "Epoch 3/200\n",
            "Train Loss: 0.7127, Train Acc: 0.5247\n",
            "Val Loss: 0.6792, Val Acc: 0.5771\n",
            "Epoch 4/200\n",
            "Train Loss: 0.7233, Train Acc: 0.5282\n",
            "Val Loss: 0.7117, Val Acc: 0.5613\n",
            "Epoch 5/200\n",
            "Train Loss: 0.7044, Train Acc: 0.5474\n",
            "Val Loss: 0.7209, Val Acc: 0.5119\n",
            "Epoch 6/200\n",
            "Train Loss: 0.7023, Train Acc: 0.5440\n",
            "Val Loss: 0.6725, Val Acc: 0.5791\n",
            "Epoch 7/200\n",
            "Train Loss: 0.7096, Train Acc: 0.5375\n",
            "Val Loss: 0.6747, Val Acc: 0.5810\n",
            "Epoch 8/200\n",
            "Train Loss: 0.6944, Train Acc: 0.5311\n",
            "Val Loss: 0.6738, Val Acc: 0.5830\n",
            "Epoch 9/200\n",
            "Train Loss: 0.6943, Train Acc: 0.5568\n",
            "Val Loss: 0.9884, Val Acc: 0.4862\n",
            "Epoch 10/200\n",
            "Train Loss: 0.6999, Train Acc: 0.5445\n",
            "Val Loss: 0.7202, Val Acc: 0.5652\n",
            "Epoch 11/200\n",
            "Train Loss: 0.6961, Train Acc: 0.5548\n",
            "Val Loss: 0.6723, Val Acc: 0.6126\n",
            "Epoch 12/200\n",
            "Train Loss: 0.6849, Train Acc: 0.5657\n",
            "Val Loss: 0.7343, Val Acc: 0.5000\n",
            "Epoch 13/200\n",
            "Train Loss: 0.6858, Train Acc: 0.5593\n",
            "Val Loss: 0.6817, Val Acc: 0.5731\n",
            "Epoch 14/200\n",
            "Train Loss: 0.6824, Train Acc: 0.5707\n",
            "Val Loss: 0.7200, Val Acc: 0.5099\n",
            "Epoch 15/200\n",
            "Train Loss: 0.6867, Train Acc: 0.5637\n",
            "Val Loss: 0.7815, Val Acc: 0.4901\n",
            "Epoch 16/200\n",
            "Train Loss: 0.6842, Train Acc: 0.5623\n",
            "Val Loss: 0.6898, Val Acc: 0.5988\n",
            "Epoch 17/200\n",
            "Train Loss: 0.6886, Train Acc: 0.5529\n",
            "Val Loss: 1.4579, Val Acc: 0.6225\n",
            "Epoch 18/200\n",
            "Train Loss: 0.6707, Train Acc: 0.5815\n",
            "Val Loss: 1.0121, Val Acc: 0.6067\n",
            "Epoch 19/200\n",
            "Train Loss: 0.6574, Train Acc: 0.5954\n",
            "Val Loss: 0.6900, Val Acc: 0.6146\n",
            "Epoch 20/200\n",
            "Train Loss: 0.6638, Train Acc: 0.5894\n",
            "Val Loss: 0.6522, Val Acc: 0.5988\n",
            "Epoch 21/200\n",
            "Train Loss: 0.6558, Train Acc: 0.6112\n",
            "Val Loss: 0.6543, Val Acc: 0.6206\n",
            "Epoch 22/200\n",
            "Train Loss: 0.6528, Train Acc: 0.6072\n",
            "Val Loss: 0.6555, Val Acc: 0.6206\n",
            "Epoch 23/200\n",
            "Train Loss: 0.6613, Train Acc: 0.6003\n",
            "Val Loss: 0.6529, Val Acc: 0.6265\n",
            "Epoch 24/200\n",
            "Train Loss: 0.6582, Train Acc: 0.6062\n",
            "Val Loss: 0.6636, Val Acc: 0.6107\n",
            "Epoch 25/200\n",
            "Train Loss: 0.6498, Train Acc: 0.6072\n",
            "Val Loss: 0.6614, Val Acc: 0.6186\n",
            "Epoch 26/200\n",
            "Train Loss: 0.6486, Train Acc: 0.6206\n",
            "Val Loss: 0.6586, Val Acc: 0.6028\n",
            "Epoch 27/200\n",
            "Train Loss: 0.6559, Train Acc: 0.6097\n",
            "Val Loss: 0.6569, Val Acc: 0.6126\n",
            "Epoch 28/200\n",
            "Train Loss: 0.6539, Train Acc: 0.6161\n",
            "Val Loss: 0.6559, Val Acc: 0.6225\n",
            "Epoch 29/200\n",
            "Train Loss: 0.6510, Train Acc: 0.6082\n",
            "Val Loss: 0.6554, Val Acc: 0.6126\n",
            "Epoch 30/200\n",
            "Train Loss: 0.6482, Train Acc: 0.6186\n",
            "Val Loss: 0.6546, Val Acc: 0.6245\n",
            "Early stopping triggered\n",
            "Model for TG saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/quartile/model_TG.pth\n",
            "Training history for TG saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/quartile/history_TG.pth\n",
            "Completed processing for TG\n",
            "----------------------------------------\n",
            "Processing column: BS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "Train Loss: 0.9251, Train Acc: 0.5032\n",
            "Val Loss: 0.6884, Val Acc: 0.5204\n",
            "Epoch 2/200\n",
            "Train Loss: 0.7228, Train Acc: 0.5371\n",
            "Val Loss: 0.7030, Val Acc: 0.4630\n",
            "Epoch 3/200\n",
            "Train Loss: 0.7415, Train Acc: 0.5116\n",
            "Val Loss: 0.7216, Val Acc: 0.5426\n",
            "Epoch 4/200\n",
            "Train Loss: 0.7197, Train Acc: 0.5385\n",
            "Val Loss: 0.7039, Val Acc: 0.5426\n",
            "Epoch 5/200\n",
            "Train Loss: 0.7174, Train Acc: 0.5390\n",
            "Val Loss: 0.7086, Val Acc: 0.4963\n",
            "Epoch 6/200\n",
            "Train Loss: 0.7262, Train Acc: 0.5246\n",
            "Val Loss: 0.6867, Val Acc: 0.5333\n",
            "Epoch 7/200\n",
            "Train Loss: 0.7288, Train Acc: 0.5167\n",
            "Val Loss: 0.7029, Val Acc: 0.4648\n",
            "Epoch 8/200\n",
            "Train Loss: 0.7050, Train Acc: 0.5255\n",
            "Val Loss: 0.6864, Val Acc: 0.5556\n",
            "Epoch 9/200\n",
            "Train Loss: 0.6946, Train Acc: 0.5496\n",
            "Val Loss: 0.6804, Val Acc: 0.6056\n",
            "Epoch 10/200\n",
            "Train Loss: 0.6898, Train Acc: 0.5557\n",
            "Val Loss: 0.7247, Val Acc: 0.5426\n",
            "Epoch 11/200\n",
            "Train Loss: 0.6975, Train Acc: 0.5404\n",
            "Val Loss: 0.7536, Val Acc: 0.5426\n",
            "Epoch 12/200\n",
            "Train Loss: 0.6928, Train Acc: 0.5557\n",
            "Val Loss: 0.6923, Val Acc: 0.5426\n",
            "Epoch 13/200\n",
            "Train Loss: 0.6795, Train Acc: 0.5765\n",
            "Val Loss: 0.7076, Val Acc: 0.5500\n",
            "Epoch 14/200\n",
            "Train Loss: 0.6821, Train Acc: 0.5863\n",
            "Val Loss: 0.6716, Val Acc: 0.6056\n",
            "Epoch 15/200\n",
            "Train Loss: 0.6959, Train Acc: 0.5686\n",
            "Val Loss: 1.3682, Val Acc: 0.5426\n",
            "Epoch 16/200\n",
            "Train Loss: 0.6912, Train Acc: 0.5677\n",
            "Val Loss: 0.6987, Val Acc: 0.5222\n",
            "Epoch 17/200\n",
            "Train Loss: 0.6956, Train Acc: 0.5547\n",
            "Val Loss: 0.6922, Val Acc: 0.5815\n",
            "Epoch 18/200\n",
            "Train Loss: 0.6824, Train Acc: 0.5826\n",
            "Val Loss: 0.7064, Val Acc: 0.5963\n",
            "Epoch 19/200\n",
            "Train Loss: 0.6697, Train Acc: 0.6048\n",
            "Val Loss: 0.6573, Val Acc: 0.6241\n",
            "Epoch 20/200\n",
            "Train Loss: 0.6627, Train Acc: 0.6109\n",
            "Val Loss: 0.6684, Val Acc: 0.5852\n",
            "Epoch 21/200\n",
            "Train Loss: 0.6604, Train Acc: 0.6109\n",
            "Val Loss: 0.6830, Val Acc: 0.6222\n",
            "Epoch 22/200\n",
            "Train Loss: 0.6609, Train Acc: 0.6090\n",
            "Val Loss: 0.6785, Val Acc: 0.5926\n",
            "Epoch 23/200\n",
            "Train Loss: 0.6761, Train Acc: 0.6071\n",
            "Val Loss: 1.3958, Val Acc: 0.4574\n",
            "Epoch 24/200\n",
            "Train Loss: 0.6582, Train Acc: 0.6104\n",
            "Val Loss: 0.6828, Val Acc: 0.6074\n",
            "Epoch 25/200\n",
            "Train Loss: 0.6607, Train Acc: 0.6136\n",
            "Val Loss: 0.7081, Val Acc: 0.5722\n",
            "Epoch 26/200\n",
            "Train Loss: 0.6547, Train Acc: 0.6136\n",
            "Val Loss: 0.6509, Val Acc: 0.6315\n",
            "Epoch 27/200\n",
            "Train Loss: 0.6455, Train Acc: 0.6280\n",
            "Val Loss: 0.6526, Val Acc: 0.6185\n",
            "Epoch 28/200\n",
            "Train Loss: 0.6444, Train Acc: 0.6354\n",
            "Val Loss: 0.6511, Val Acc: 0.6463\n",
            "Epoch 29/200\n",
            "Train Loss: 0.6379, Train Acc: 0.6299\n",
            "Val Loss: 0.6547, Val Acc: 0.6444\n",
            "Epoch 30/200\n",
            "Train Loss: 0.6386, Train Acc: 0.6433\n",
            "Val Loss: 0.6523, Val Acc: 0.6407\n",
            "Epoch 31/200\n",
            "Train Loss: 0.6373, Train Acc: 0.6396\n",
            "Val Loss: 0.6607, Val Acc: 0.6259\n",
            "Epoch 32/200\n",
            "Train Loss: 0.6421, Train Acc: 0.6354\n",
            "Val Loss: 0.6473, Val Acc: 0.6481\n",
            "Epoch 33/200\n",
            "Train Loss: 0.6427, Train Acc: 0.6313\n",
            "Val Loss: 0.6523, Val Acc: 0.6389\n",
            "Epoch 34/200\n",
            "Train Loss: 0.6342, Train Acc: 0.6452\n",
            "Val Loss: 0.6620, Val Acc: 0.6444\n",
            "Epoch 35/200\n",
            "Train Loss: 0.6434, Train Acc: 0.6331\n",
            "Val Loss: 0.6506, Val Acc: 0.6333\n",
            "Epoch 36/200\n",
            "Train Loss: 0.6392, Train Acc: 0.6313\n",
            "Val Loss: 0.6493, Val Acc: 0.6352\n",
            "Epoch 37/200\n",
            "Train Loss: 0.6358, Train Acc: 0.6484\n",
            "Val Loss: 0.6593, Val Acc: 0.6426\n",
            "Epoch 38/200\n",
            "Train Loss: 0.6419, Train Acc: 0.6350\n",
            "Val Loss: 0.6549, Val Acc: 0.6241\n",
            "Epoch 39/200\n",
            "Train Loss: 0.6377, Train Acc: 0.6424\n",
            "Val Loss: 0.6509, Val Acc: 0.6352\n",
            "Epoch 40/200\n",
            "Train Loss: 0.6349, Train Acc: 0.6405\n",
            "Val Loss: 0.6487, Val Acc: 0.6407\n",
            "Epoch 41/200\n",
            "Train Loss: 0.6336, Train Acc: 0.6535\n",
            "Val Loss: 0.6472, Val Acc: 0.6407\n",
            "Epoch 42/200\n",
            "Train Loss: 0.6292, Train Acc: 0.6489\n",
            "Val Loss: 0.6473, Val Acc: 0.6389\n",
            "Epoch 43/200\n",
            "Train Loss: 0.6314, Train Acc: 0.6475\n",
            "Val Loss: 0.6511, Val Acc: 0.6407\n",
            "Epoch 44/200\n",
            "Train Loss: 0.6374, Train Acc: 0.6415\n",
            "Val Loss: 0.6467, Val Acc: 0.6426\n",
            "Epoch 45/200\n",
            "Train Loss: 0.6352, Train Acc: 0.6452\n",
            "Val Loss: 0.6496, Val Acc: 0.6407\n",
            "Epoch 46/200\n",
            "Train Loss: 0.6341, Train Acc: 0.6447\n",
            "Val Loss: 0.6449, Val Acc: 0.6407\n",
            "Epoch 47/200\n",
            "Train Loss: 0.6419, Train Acc: 0.6303\n",
            "Val Loss: 0.6481, Val Acc: 0.6407\n",
            "Epoch 48/200\n",
            "Train Loss: 0.6358, Train Acc: 0.6391\n",
            "Val Loss: 0.6475, Val Acc: 0.6389\n",
            "Epoch 49/200\n",
            "Train Loss: 0.6370, Train Acc: 0.6419\n",
            "Val Loss: 0.6471, Val Acc: 0.6370\n",
            "Epoch 50/200\n",
            "Train Loss: 0.6322, Train Acc: 0.6461\n",
            "Val Loss: 0.6494, Val Acc: 0.6333\n",
            "Epoch 51/200\n",
            "Train Loss: 0.6333, Train Acc: 0.6438\n",
            "Val Loss: 0.6454, Val Acc: 0.6407\n",
            "Epoch 52/200\n",
            "Train Loss: 0.6338, Train Acc: 0.6512\n",
            "Val Loss: 0.6483, Val Acc: 0.6352\n",
            "Epoch 53/200\n",
            "Train Loss: 0.6308, Train Acc: 0.6489\n",
            "Val Loss: 0.6462, Val Acc: 0.6370\n",
            "Epoch 54/200\n",
            "Train Loss: 0.6311, Train Acc: 0.6517\n",
            "Val Loss: 0.6482, Val Acc: 0.6333\n",
            "Epoch 55/200\n",
            "Train Loss: 0.6286, Train Acc: 0.6461\n",
            "Val Loss: 0.6480, Val Acc: 0.6370\n",
            "Epoch 56/200\n",
            "Train Loss: 0.6315, Train Acc: 0.6480\n",
            "Val Loss: 0.6508, Val Acc: 0.6315\n",
            "Early stopping triggered\n",
            "Model for BS saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/quartile/model_BS.pth\n",
            "Training history for BS saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/quartile/history_BS.pth\n",
            "Completed processing for BS\n",
            "----------------------------------------\n",
            "All columns processed and models saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "このスクリプトは眼底画像を用いて代謝関連データの回帰分析を行う深層学習モデルを作成・学習します。\n",
        "主な機能：\n",
        "1. 必要なライブラリのインポートとGPU設定\n",
        "2. CSVファイルからのデータ読み込みと処理対象カラムの定義\n",
        "3. 回帰分析用のカスタムデータセットクラスの定義（StandardScalerを使用）\n",
        "4. データ拡張とテンソル変換の設定\n",
        "5. モデルのトレーニングと評価関数の定義\n",
        "6. 早期終了機能の実装\n",
        "7. 特定のカラムに対する回帰モデル作成・トレーニング関数\n",
        "   - データセットの分割とデータローダーの作成\n",
        "   - 事前学習済みモデルの読み込みと回帰用への修正\n",
        "   - 損失関数（MSE）、最適化アルゴリズム、学習率スケジューラの定義. RepVGGA0, RangerAdaBrief, Dropout\n",
        "   - トレーニングループ（早期終了を含む）\n",
        "8. メインループ：各カラム（年齢、AC、SBP、DBP、HDLC、TG、BS）に対して\n",
        "   回帰モデルを作成・トレーニングし、結果を保存\n",
        "\n",
        "このスクリプトは、先行して学習された分類モデルを基に、\n",
        "眼底画像から直接各代謝指標の値を予測する回帰モデルを\n",
        "ファインチューニングすることを目的としています。\n",
        "\"\"\"\n",
        "# 必要なライブラリのインストール\n",
        "!pip install timm --quiet\n",
        "!pip install ranger-adabelief==0.1.0 --quiet\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import timm\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Google Driveのマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set device for training (use GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/drive/MyDrive/Deep_learning/Fundus_metabolic/label_train.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define the columns to process\n",
        "#columns = ['age', 'AC', 'SBP', 'DBP', 'HDLC', 'TG', 'BS']\n",
        "columns = ['DBP', 'HDLC', 'TG', 'BS']\n",
        "\n",
        "\n",
        "# Dataset class for regression\n",
        "class FundusRegressionDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, target_column, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_column = target_column\n",
        "        self.scaler = StandardScaler()\n",
        "        self.data[target_column] = self.scaler.fit_transform(self.data[[target_column]])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        target = self.data.iloc[idx][self.target_column]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(target, dtype=torch.float32)\n",
        "\n",
        "# Data transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs).squeeze()\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    return epoch_loss\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    return epoch_loss\n",
        "\n",
        "# Early Stopping class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "# Function to create and train regression model for a specific column\n",
        "def create_and_train_regression_model(column):\n",
        "    # Split the dataset into training and validation sets\n",
        "    train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Save the train and validation datasets to new CSV files\n",
        "    train_csv = f'/content/train_{column}.csv'\n",
        "    val_csv = f'/content/val_{column}.csv'\n",
        "    train_data.to_csv(train_csv, index=False)\n",
        "    val_data.to_csv(val_csv, index=False)\n",
        "\n",
        "    # Create dataset objects\n",
        "    train_dataset = FundusRegressionDataset(train_csv, img_dir, column, transform_train)\n",
        "    val_dataset = FundusRegressionDataset(val_csv, img_dir, column, transform_val)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "    # Load the pre-trained model\n",
        "    pretrained_model_path = f\"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/quartile/model_{column}.pth\"\n",
        "    pretrained_model = nn.Sequential(\n",
        "        timm.create_model('repvgg_a0.rvgg_in1k', pretrained=True, num_classes=1000),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(1000, 2)\n",
        "    )\n",
        "    pretrained_model.load_state_dict(torch.load(pretrained_model_path))\n",
        "\n",
        "    # Modify the model for regression\n",
        "    model = nn.Sequential(\n",
        "        *list(pretrained_model.children())[:-1],  # Remove the last layer\n",
        "        nn.Linear(1000, 1)  # Add a new layer for regression\n",
        "    ).to(device)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 100\n",
        "    patience = 10\n",
        "    early_stopping = EarlyStopping(patience=patience)\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss = evaluate(model, val_loader, criterion, device)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        early_stopping(val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Set the image directory\n",
        "img_dir = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_224px\"\n",
        "\n",
        "# Set the save directory for the new models\n",
        "save_dir = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/regression_finetuned_from_quartile\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Main loop to process each column\n",
        "for column in columns:\n",
        "    print(f\"Processing column: {column}\")\n",
        "    model, history = create_and_train_regression_model(column)\n",
        "\n",
        "    # Save the model\n",
        "    save_path = os.path.join(save_dir, f\"model_{column}.pth\")\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model for {column} saved to {save_path}\")\n",
        "\n",
        "    # Save the history as CSV\n",
        "    history_df = pd.DataFrame(history)\n",
        "    history_path = os.path.join(save_dir, f\"history_{column}.csv\")\n",
        "    history_df.to_csv(history_path, index=False)\n",
        "    print(f\"Training history for {column} saved to {history_path}\")\n",
        "\n",
        "    print(f\"Completed processing for {column}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "print(\"All columns processed and models saved.\")"
      ],
      "metadata": {
        "id": "Lv9bQpt7GhYw",
        "outputId": "39abe7e5-ac3c-4642-ef10-162cb89b9aea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "46561741cddf4eb7a1b9042d7c887d47",
            "858d23e287814f2991c2e7ca44925452",
            "f6217a4337624bffb6bed385446c303d",
            "a9b49a9b32b6420398904aebe1d91c4c",
            "fcd403d437a94c68be3dfcd5ad9b59d9",
            "5f010b2687b54081a79559943fd2ca77",
            "fa169862523040128d2941b47d6f996c",
            "36f5403463fe48ca950a9e8234febbdb",
            "bc4d77565bea41f7b3aecd28cdca39da",
            "c98d465730724b9da4ba12c3f85e429d",
            "e1eceebcd3a04eed987542144de3b258"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Processing column: DBP\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/36.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46561741cddf4eb7a1b9042d7c887d47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-071268bee341>:176: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_model.load_state_dict(torch.load(pretrained_model_path))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Train Loss: 0.9883\n",
            "Val Loss: 0.9438\n",
            "Epoch 2/100\n",
            "Train Loss: 0.9842\n",
            "Val Loss: 0.9576\n",
            "Epoch 3/100\n",
            "Train Loss: 0.9872\n",
            "Val Loss: 1.1142\n",
            "Epoch 4/100\n",
            "Train Loss: 0.9718\n",
            "Val Loss: 0.9381\n",
            "Epoch 5/100\n",
            "Train Loss: 0.9749\n",
            "Val Loss: 0.9959\n",
            "Epoch 6/100\n",
            "Train Loss: 0.9838\n",
            "Val Loss: 0.9545\n",
            "Epoch 7/100\n",
            "Train Loss: 0.9631\n",
            "Val Loss: 0.9787\n",
            "Epoch 8/100\n",
            "Train Loss: 0.9542\n",
            "Val Loss: 0.9291\n",
            "Epoch 9/100\n",
            "Train Loss: 0.9574\n",
            "Val Loss: 0.9138\n",
            "Epoch 10/100\n",
            "Train Loss: 0.9586\n",
            "Val Loss: 0.9365\n",
            "Epoch 11/100\n",
            "Train Loss: 0.9576\n",
            "Val Loss: 0.9747\n",
            "Epoch 12/100\n",
            "Train Loss: 0.9471\n",
            "Val Loss: 0.9297\n",
            "Epoch 13/100\n",
            "Train Loss: 0.9499\n",
            "Val Loss: 0.9296\n",
            "Epoch 14/100\n",
            "Train Loss: 0.9406\n",
            "Val Loss: 0.9026\n",
            "Epoch 15/100\n",
            "Train Loss: 0.9431\n",
            "Val Loss: 0.9459\n",
            "Epoch 16/100\n",
            "Train Loss: 0.9328\n",
            "Val Loss: 0.9722\n",
            "Epoch 17/100\n",
            "Train Loss: 0.9381\n",
            "Val Loss: 0.9043\n",
            "Epoch 18/100\n",
            "Train Loss: 0.9362\n",
            "Val Loss: 0.8991\n",
            "Epoch 19/100\n",
            "Train Loss: 0.9375\n",
            "Val Loss: 0.9170\n",
            "Epoch 20/100\n",
            "Train Loss: 0.9351\n",
            "Val Loss: 0.8959\n",
            "Epoch 21/100\n",
            "Train Loss: 0.9414\n",
            "Val Loss: 0.9221\n",
            "Epoch 22/100\n",
            "Train Loss: 0.9395\n",
            "Val Loss: 0.9273\n",
            "Epoch 23/100\n",
            "Train Loss: 0.9519\n",
            "Val Loss: 0.8888\n",
            "Epoch 24/100\n",
            "Train Loss: 0.9352\n",
            "Val Loss: 0.8960\n",
            "Epoch 25/100\n",
            "Train Loss: 0.9265\n",
            "Val Loss: 0.8722\n",
            "Epoch 26/100\n",
            "Train Loss: 0.9146\n",
            "Val Loss: 0.8778\n",
            "Epoch 27/100\n",
            "Train Loss: 0.9271\n",
            "Val Loss: 1.0694\n",
            "Epoch 28/100\n",
            "Train Loss: 0.9276\n",
            "Val Loss: 0.9004\n",
            "Epoch 29/100\n",
            "Train Loss: 0.9267\n",
            "Val Loss: 0.9310\n",
            "Epoch 30/100\n",
            "Train Loss: 0.9267\n",
            "Val Loss: 0.8829\n",
            "Epoch 31/100\n",
            "Train Loss: 0.9105\n",
            "Val Loss: 0.8870\n",
            "Epoch 32/100\n",
            "Train Loss: 0.8984\n",
            "Val Loss: 0.8803\n",
            "Epoch 33/100\n",
            "Train Loss: 0.8902\n",
            "Val Loss: 0.8753\n",
            "Epoch 34/100\n",
            "Train Loss: 0.8913\n",
            "Val Loss: 0.8714\n",
            "Epoch 35/100\n",
            "Train Loss: 0.8931\n",
            "Val Loss: 0.8716\n",
            "Epoch 36/100\n",
            "Train Loss: 0.8875\n",
            "Val Loss: 0.8663\n",
            "Epoch 37/100\n",
            "Train Loss: 0.8908\n",
            "Val Loss: 0.8678\n",
            "Epoch 38/100\n",
            "Train Loss: 0.8812\n",
            "Val Loss: 0.9030\n",
            "Epoch 39/100\n",
            "Train Loss: 0.8956\n",
            "Val Loss: 0.9072\n",
            "Epoch 40/100\n",
            "Train Loss: 0.8721\n",
            "Val Loss: 0.9117\n",
            "Epoch 41/100\n",
            "Train Loss: 0.8781\n",
            "Val Loss: 0.8645\n",
            "Epoch 42/100\n",
            "Train Loss: 0.8773\n",
            "Val Loss: 0.8610\n",
            "Epoch 43/100\n",
            "Train Loss: 0.8838\n",
            "Val Loss: 0.8985\n",
            "Epoch 44/100\n",
            "Train Loss: 0.8869\n",
            "Val Loss: 0.8630\n",
            "Epoch 45/100\n",
            "Train Loss: 0.8798\n",
            "Val Loss: 0.8665\n",
            "Epoch 46/100\n",
            "Train Loss: 0.8739\n",
            "Val Loss: 0.8680\n",
            "Epoch 47/100\n",
            "Train Loss: 0.8745\n",
            "Val Loss: 0.8794\n",
            "Epoch 48/100\n",
            "Train Loss: 0.8766\n",
            "Val Loss: 0.8665\n",
            "Epoch 49/100\n",
            "Train Loss: 0.8694\n",
            "Val Loss: 0.8802\n",
            "Epoch 50/100\n",
            "Train Loss: 0.8624\n",
            "Val Loss: 0.8605\n",
            "Epoch 51/100\n",
            "Train Loss: 0.8693\n",
            "Val Loss: 0.8685\n",
            "Epoch 52/100\n",
            "Train Loss: 0.8725\n",
            "Val Loss: 0.8692\n",
            "Epoch 53/100\n",
            "Train Loss: 0.8667\n",
            "Val Loss: 0.8616\n",
            "Epoch 54/100\n",
            "Train Loss: 0.8749\n",
            "Val Loss: 0.8849\n",
            "Epoch 55/100\n",
            "Train Loss: 0.8715\n",
            "Val Loss: 0.8640\n",
            "Epoch 56/100\n",
            "Train Loss: 0.8689\n",
            "Val Loss: 0.8640\n",
            "Epoch 57/100\n",
            "Train Loss: 0.8712\n",
            "Val Loss: 0.8607\n",
            "Epoch 58/100\n",
            "Train Loss: 0.8769\n",
            "Val Loss: 0.8839\n",
            "Epoch 59/100\n",
            "Train Loss: 0.8634\n",
            "Val Loss: 0.8651\n",
            "Epoch 60/100\n",
            "Train Loss: 0.8648\n",
            "Val Loss: 0.8643\n",
            "Early stopping triggered\n",
            "Model for DBP saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/regression_finetuned_from_quartile/model_DBP.pth\n",
            "Training history for DBP saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/regression_finetuned_from_quartile/history_DBP.csv\n",
            "Completed processing for DBP\n",
            "----------------------------------------\n",
            "Processing column: HDLC\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-071268bee341>:176: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_model.load_state_dict(torch.load(pretrained_model_path))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Train Loss: 1.0437\n",
            "Val Loss: 1.0487\n",
            "Epoch 2/100\n",
            "Train Loss: 1.0220\n",
            "Val Loss: 1.0215\n",
            "Epoch 3/100\n",
            "Train Loss: 1.0158\n",
            "Val Loss: 1.0059\n",
            "Epoch 4/100\n",
            "Train Loss: 1.0186\n",
            "Val Loss: 0.9998\n",
            "Epoch 5/100\n",
            "Train Loss: 1.0292\n",
            "Val Loss: 1.0002\n",
            "Epoch 6/100\n",
            "Train Loss: 1.0143\n",
            "Val Loss: 1.0201\n",
            "Epoch 7/100\n",
            "Train Loss: 1.0101\n",
            "Val Loss: 1.0013\n",
            "Epoch 8/100\n",
            "Train Loss: 1.0127\n",
            "Val Loss: 1.0015\n",
            "Epoch 9/100\n",
            "Train Loss: 1.0083\n",
            "Val Loss: 1.0029\n",
            "Epoch 10/100\n",
            "Train Loss: 1.0120\n",
            "Val Loss: 1.0153\n",
            "Epoch 11/100\n",
            "Train Loss: 1.0060\n",
            "Val Loss: 1.0147\n",
            "Epoch 12/100\n",
            "Train Loss: 1.0039\n",
            "Val Loss: 1.0192\n",
            "Epoch 13/100\n",
            "Train Loss: 1.0049\n",
            "Val Loss: 1.0196\n",
            "Epoch 14/100\n",
            "Train Loss: 1.0054\n",
            "Val Loss: 1.0161\n",
            "Early stopping triggered\n",
            "Model for HDLC saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/regression_finetuned_from_quartile/model_HDLC.pth\n",
            "Training history for HDLC saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/regression_finetuned_from_quartile/history_HDLC.csv\n",
            "Completed processing for HDLC\n",
            "----------------------------------------\n",
            "Processing column: TG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-071268bee341>:176: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_model.load_state_dict(torch.load(pretrained_model_path))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Train Loss: 1.0205\n",
            "Val Loss: 1.0158\n",
            "Epoch 2/100\n",
            "Train Loss: 1.0257\n",
            "Val Loss: 73446329.4195\n",
            "Epoch 3/100\n",
            "Train Loss: 1.0161\n",
            "Val Loss: 17947.6695\n",
            "Epoch 4/100\n",
            "Train Loss: 1.0119\n",
            "Val Loss: 1.0351\n",
            "Epoch 5/100\n",
            "Train Loss: 1.0161\n",
            "Val Loss: 1.0760\n",
            "Epoch 6/100\n",
            "Train Loss: 1.0194\n",
            "Val Loss: 2.0506\n",
            "Epoch 7/100\n",
            "Train Loss: 1.0133\n",
            "Val Loss: 1.1397\n",
            "Epoch 8/100\n",
            "Train Loss: 1.0014\n",
            "Val Loss: 4.6598\n",
            "Epoch 9/100\n",
            "Train Loss: 1.0080\n",
            "Val Loss: 990.5645\n",
            "Epoch 10/100\n",
            "Train Loss: 1.0048\n",
            "Val Loss: 515.1161\n",
            "Epoch 11/100\n",
            "Train Loss: 1.0032\n",
            "Val Loss: 4867.9381\n",
            "Early stopping triggered\n",
            "Model for TG saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/regression_finetuned_from_quartile/model_TG.pth\n",
            "Training history for TG saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/regression_finetuned_from_quartile/history_TG.csv\n",
            "Completed processing for TG\n",
            "----------------------------------------\n",
            "Processing column: BS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-071268bee341>:176: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_model.load_state_dict(torch.load(pretrained_model_path))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Train Loss: 0.9639\n",
            "Val Loss: 0.9357\n",
            "Epoch 2/100\n",
            "Train Loss: 0.9606\n",
            "Val Loss: 0.9105\n",
            "Epoch 3/100\n",
            "Train Loss: 0.9498\n",
            "Val Loss: 0.9436\n",
            "Epoch 4/100\n",
            "Train Loss: 0.9551\n",
            "Val Loss: 0.9159\n",
            "Epoch 5/100\n",
            "Train Loss: 0.9464\n",
            "Val Loss: 0.9067\n",
            "Epoch 6/100\n",
            "Train Loss: 0.9387\n",
            "Val Loss: 0.9077\n",
            "Epoch 7/100\n",
            "Train Loss: 0.9531\n",
            "Val Loss: 0.9309\n",
            "Epoch 8/100\n",
            "Train Loss: 0.9468\n",
            "Val Loss: 0.9117\n",
            "Epoch 9/100\n",
            "Train Loss: 0.9445\n",
            "Val Loss: 0.9351\n",
            "Epoch 10/100\n",
            "Train Loss: 0.9562\n",
            "Val Loss: 18.2601\n",
            "Epoch 11/100\n",
            "Train Loss: 0.9557\n",
            "Val Loss: 1.0150\n",
            "Epoch 12/100\n",
            "Train Loss: 0.9357\n",
            "Val Loss: 1.5251\n",
            "Epoch 13/100\n",
            "Train Loss: 0.9243\n",
            "Val Loss: 1.3617\n",
            "Epoch 14/100\n",
            "Train Loss: 0.9260\n",
            "Val Loss: 0.9782\n",
            "Epoch 15/100\n",
            "Train Loss: 0.9352\n",
            "Val Loss: 1.0648\n",
            "Early stopping triggered\n",
            "Model for BS saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/regression_finetuned_from_quartile/model_BS.pth\n",
            "Training history for BS saved to /content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/regression_finetuned_from_quartile/history_BS.csv\n",
            "Completed processing for BS\n",
            "----------------------------------------\n",
            "All columns processed and models saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dyUafw51YrxO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}