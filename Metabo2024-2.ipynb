{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOsPaxpiGTM+hzZp3BkUr/i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/FundusPhoto/blob/main/Metabo2024-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Metabo2024-2**\n",
        "\n",
        "to do\n",
        "```\n",
        "・Finetune pretrained age estimation model\n",
        "・※pretrained model: MAE=2.88、Metabo2024の最後で検証済み\n",
        "```"
      ],
      "metadata": {
        "id": "oKefHJxnogp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Load pretrained weight**"
      ],
      "metadata": {
        "id": "D0InTXBepIW5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCu_knU8ogFN",
        "outputId": "4c98cf97-d5cd-4bc4-96fa-a8b401a50136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/431.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "<ipython-input-1-4e8bceffdcfa>:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# 必要ライブラリ読み込み\n",
        "!pip install timm==0.5.4 --q\n",
        "import random\n",
        "import timm\n",
        "import copy\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from timm.scheduler import CosineLRScheduler\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# モデル枠組み読み込み\n",
        "model = timm.create_model(model_name='swin_base_patch4_window12_384', num_classes=1, pretrained=False)\n",
        "\n",
        "# GPU使用する場合\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# 学習済みモデル読み込み\n",
        "model_path = '/content/drive/MyDrive/Deep_learning/Fundus_metabolic/model_20220903.pth'\n",
        "#model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.load_state_dict(torch.load(model_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Load cleaned dataset**\n",
        "\n",
        "・['AH', 'Blur']を削除したもの"
      ],
      "metadata": {
        "id": "uTI0g-2yrCQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_csv_path = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/label_train.csv\"\n",
        "original_df = pd.read_csv(original_csv_path)\n",
        "\n",
        "# Load the provided metabo_disease.csv file again\n",
        "disease_csv_path = '/content/drive/MyDrive/Deep_learning/Fundus_metabolic/metabo_disease.csv'\n",
        "disease_df = pd.read_csv(disease_csv_path)\n",
        "\n",
        "# Filter the disease_df to get only rows where reason is \"AH\" or \"Blur\"\n",
        "#exclude_df = disease_df[disease_df['reason'].isin(['AH', 'Blur', 'ERM', \"Hemorrhage\", \"Coagulation\", \"VO\", \"Degeneration\", \"AMD\", \"CRA\", \"Drusen\"])]\n",
        "exclude_df = disease_df[disease_df['reason'].isin(['AH', 'Blur', 'ERM', \"Hemorrhage\", \"Coagulation\", \"VO\", \"Degeneration\", \"AMD\", \"CRA\", \"Drusen\"])]\n",
        "\n",
        "# Extract the ids (filenames) from ah_blur_df that match the 'AH' or 'Blur' criteria\n",
        "ah_blur_ids = exclude_df['id'].tolist()\n",
        "\n",
        "# Now remove these filenames from comparison_df\n",
        "cleaned_df = original_df[~original_df['filename'].isin(ah_blur_ids)]\n",
        "len(cleaned_df)"
      ],
      "metadata": {
        "id": "qe7gfH8JrBeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Finetune age estimation model**\n",
        "\n",
        "1e-7➡2e-6: MSE 13.2399, R2 0.8778\n",
        "\n",
        "1e-6➡1e-4: MSE\n"
      ],
      "metadata": {
        "id": "fvVGk-TUqOqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "num_epochs = 200\n",
        "patience = 10\n",
        "seed = 42\n",
        "img_dir = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_whole_384px\"\n",
        "\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(seed)\n",
        "\n",
        "\n",
        "# データセットクラスの定義\n",
        "class FundusDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_dir, transform=None):\n",
        "        self.data = dataframe\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        label = self.data.iloc[idx, 1]  # AGEカラムのインデックス\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# データの前処理とオーグメンテーション\n",
        "transform_train = transforms.Compose([\n",
        "    #transforms.Resize((224, 224)),\n",
        "    #transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=5),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
        "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.RandomGrayscale(p=0.1),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 検証用の変換（オーグメンテーションなし）\n",
        "transform_val = transforms.Compose([\n",
        "    #transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "full_dataset = FundusDataset(cleaned_df, img_dir)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_dataset.dataset.transform = transform_train\n",
        "val_dataset.dataset.transform = transform_val\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "# CosineLRSchedulerの設定\n",
        "scheduler = CosineLRScheduler(\n",
        "    optimizer,\n",
        "    t_initial=num_epochs,\n",
        "    lr_min=1e-6,\n",
        "    warmup_t=5,\n",
        "    warmup_lr_init=1e-7,\n",
        "    warmup_prefix=True\n",
        ")\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "# Early Stopping クラス\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.best_model = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_model = copy.deepcopy(model.state_dict())\n",
        "            return True\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_model = copy.deepcopy(model.state_dict())\n",
        "            self.counter = 0\n",
        "            return True\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "            return False\n",
        "\n",
        "# トレーニング関数の修正\n",
        "def train(model, train_loader, criterion, optimizer, device, scaler):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device).float()\n",
        "        inputs = inputs.to(memory_format=torch.channels_last)\n",
        "\n",
        "        for param in model.parameters():\n",
        "            param.grad = None\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda'):\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "        all_predictions.extend(outputs.detach().cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    mse = mean_squared_error(all_targets, all_predictions)\n",
        "    r2 = r2_score(all_targets, all_predictions)\n",
        "    return epoch_loss, mse, r2\n",
        "\n",
        "# 評価関数の修正\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device).float()\n",
        "            inputs = inputs.to(memory_format=torch.channels_last)\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    mse = mean_squared_error(all_targets, all_predictions)\n",
        "    r2 = r2_score(all_targets, all_predictions)\n",
        "    return epoch_loss, mse, r2\n",
        "\n",
        "# GradScalerの更新\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "# モデルの出力層の調整（必要に応じて）\n",
        "# model.fc = nn.Linear(model.fc.in_features, 1)  # 1つの出力（回帰の場合）\n",
        "\n",
        "# 損失関数の変更\n",
        "criterion = nn.MSELoss()  # 回帰問題の場合\n",
        "\n",
        "# トレーニングループの修正\n",
        "early_stopping = EarlyStopping(patience=patience)\n",
        "history = {'train_loss': [], 'train_mse': [], 'train_r2': [],\n",
        "           'val_loss': [], 'val_mse': [], 'val_r2': [], 'lr': []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train_loss, train_mse, train_r2 = train(model, train_loader, criterion, optimizer, device, scaler)\n",
        "    val_loss, val_mse, val_r2 = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    scheduler.step(epoch + 1)\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    history['lr'].append(current_lr)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_mse'].append(train_mse)\n",
        "    history['train_r2'].append(train_r2)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_mse'].append(val_mse)\n",
        "    history['val_r2'].append(val_r2)\n",
        "\n",
        "    is_best = early_stopping(val_loss, model)\n",
        "\n",
        "    if early_stopping.best_model is not None:\n",
        "        model.load_state_dict(early_stopping.best_model)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train MSE: {train_mse:.4f}, Train R2: {train_r2:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val MSE: {val_mse:.4f}, Val R2: {val_r2:.4f}\")\n",
        "    print(f\"Epoch duration: {epoch_duration:.2f} seconds\")\n",
        "    print(f\"Best model {'updated' if is_best else 'not updated'}\")\n",
        "    print(f\"Current learning rate: {current_lr:.6f}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "# 最終評価の修正\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        inputs = inputs.to(memory_format=torch.channels_last)\n",
        "        outputs = model(inputs).squeeze()\n",
        "        all_preds.extend(outputs.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "final_mse = mean_squared_error(all_labels, all_preds)\n",
        "final_r2 = r2_score(all_labels, all_preds)\n",
        "print(f\"Final MSE: {final_mse:.4f}\")\n",
        "print(f\"Final R2 Score: {final_r2:.4f}\")\n",
        "\n",
        "# 訓練結果のグラフ表示\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.plot(history['train_mse'], label='Train MSE')\n",
        "plt.plot(history['val_mse'], label='Validation MSE')\n",
        "plt.title('Mean Squared Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.plot(history['train_r2'], label='Train R2')\n",
        "plt.plot(history['val_r2'], label='Validation R2')\n",
        "plt.title('R2 Score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('R2')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.plot(history['lr'], label='Learning Rate')\n",
        "plt.title('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Khn1IEStpeni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bJGmwW1rTUQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# 散布図の追加（サイズを大きくする）\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.scatter(all_labels, all_preds, alpha=0.5)\n",
        "plt.plot([min(all_labels), max(all_labels)], [min(all_labels), max(all_labels)], 'r--', lw=2)\n",
        "plt.xlabel('True Values', fontsize=12)\n",
        "plt.ylabel('Predictions', fontsize=12)\n",
        "plt.title('True vs Predicted Values', fontsize=14)\n",
        "\n",
        "# 残差プロットの追加\n",
        "residuals = np.array(all_labels) - np.array(all_preds)\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.scatter(all_preds, residuals, alpha=0.5)\n",
        "plt.xlabel('Predicted Values', fontsize=12)\n",
        "plt.ylabel('Residuals', fontsize=12)\n",
        "plt.title('Residual Plot', fontsize=14)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# MAEの計算\n",
        "mae = mean_absolute_error(all_labels, all_preds)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")"
      ],
      "metadata": {
        "id": "DXQaisHlVMSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# モデルの保存先パスを設定\n",
        "save_path = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/models/finetune_agemodel/age.pth\"\n",
        "\n",
        "# パスが存在することを確認し、必要に応じてディレクトリを作成\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "# モデルの状態辞書を保存\n",
        "torch.save(model.state_dict(), save_path)\n",
        "\n",
        "print(f\"Model saved successfully to {save_path}\")\n",
        "\n",
        "# オプション: モデルの読み込みを確認\n",
        "# モデルの状態辞書を読み込む\n",
        "loaded_state_dict = torch.load(save_path)\n",
        "\n",
        "# 新しいモデルインスタンスに状態辞書を適用する\n",
        "# (ここでは 'model' が既に定義されていると仮定しています)\n",
        "model.load_state_dict(loaded_state_dict)\n",
        "\n",
        "print(\"Model loaded successfully for verification.\")"
      ],
      "metadata": {
        "id": "RjoS9WeRperc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習がうまくいかないことに対する対応版"
      ],
      "metadata": {
        "id": "SWtCd3D81UoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Finetune other classification model**\n",
        "\n",
        "```\n",
        "分ける基準\n",
        "Age: <45: 1901, >=58: 929 --> age estimation modelが完成済\n",
        "AC: <85: 1555, >=95: 1410\n",
        "SBP: <125: 1429, >=140: 1374\n",
        "DBP: <80: 2055, >=85: 2111\n",
        "HDLC: <45: 1349, >60: 1496\n",
        "TG: <100: 1608, >=200: 1414\n",
        "#BS: <75: 204, >=130: 398 --> 眼底で分別\n",
        "```"
      ],
      "metadata": {
        "id": "P4Mis_T6W7HX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Load the dataset from your specified path\n",
        "file_path = '/content/drive/MyDrive/Deep_learning/Fundus_metabolic/label_train.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ssOYSVeHWrZG",
        "outputId": "3109f58c-753b-43eb-a9db-83e570365beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Deep_learning/Fundus_metabolic/label_train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1418f418fb66>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the dataset from your specified path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Deep_learning/Fundus_metabolic/label_train.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Deep_learning/Fundus_metabolic/label_train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new columns to classify the data based on the provided thresholds\n",
        "data['AC_class'] = data['AC'].apply(lambda x: 'low' if x < 85 else 'high' if x >= 95 else 'mid')\n",
        "data['SBP_class'] = data['SBP'].apply(lambda x: 'low' if x < 125 else 'high' if x >= 140 else 'mid')\n",
        "data['DBP_class'] = data['DBP'].apply(lambda x: 'low' if x < 80 else 'high' if x >= 85 else 'mid')\n",
        "data['HDLC_class'] = data['HDLC'].apply(lambda x: 'low' if x < 45 else 'high' if x > 60 else 'mid')\n",
        "data['TG_class'] = data['TG'].apply(lambda x: 'low' if x < 100 else 'high' if x >= 200 else 'mid')\n",
        "\n",
        "# Display the updated dataframe with the new classification columns\n",
        "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Classified Data\", dataframe=data)\n",
        "\n",
        "data[['filename', 'AC_class', 'SBP_class', 'DBP_class', 'HDLC_class', 'TG_class']].head()\n"
      ],
      "metadata": {
        "id": "jC5b7rYvRPM5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}