{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPxZBL2bn4u0ws7e85ImXmv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/FundusPhoto/blob/main/Metabo2024-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Metabo2024-2**\n",
        "\n",
        "to do\n",
        "```\n",
        "・Finetune pretrained age estimation model\n",
        "・※pretrained model: MAE=2.88、Metabo2024の最後で検証済み\n",
        "```"
      ],
      "metadata": {
        "id": "oKefHJxnogp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Load pretrained weight**"
      ],
      "metadata": {
        "id": "D0InTXBepIW5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCu_knU8ogFN",
        "outputId": "c989fe63-0b21-467a-b60c-1cfd4b5e2982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-5f9d3be010b3>:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# 必要ライブラリ読み込み\n",
        "!pip install timm==0.5.4 --q\n",
        "import random\n",
        "import timm\n",
        "import copy\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from timm.scheduler import CosineLRScheduler\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# モデル枠組み読み込み\n",
        "model = timm.create_model(model_name='swin_base_patch4_window12_384', num_classes=1, pretrained=False)\n",
        "\n",
        "# GPU使用する場合\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# 学習済みモデル読み込み\n",
        "model_path = '/content/drive/MyDrive/Deep_learning/Fundus_metabolic/model_20220903.pth'\n",
        "#model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.load_state_dict(torch.load(model_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Load cleaned dataset**\n",
        "\n",
        "・['AH', 'Blur']を削除したもの"
      ],
      "metadata": {
        "id": "uTI0g-2yrCQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_csv_path = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/label_train.csv\"\n",
        "original_df = pd.read_csv(original_csv_path)\n",
        "\n",
        "# Load the provided metabo_disease.csv file again\n",
        "disease_csv_path = '/content/drive/MyDrive/Deep_learning/Fundus_metabolic/metabo_disease.csv'\n",
        "disease_df = pd.read_csv(disease_csv_path)\n",
        "\n",
        "# Filter the disease_df to get only rows where reason is \"AH\" or \"Blur\"\n",
        "#exclude_df = disease_df[disease_df['reason'].isin(['AH', 'Blur', 'ERM', \"Hemorrhage\", \"Coagulation\", \"VO\", \"Degeneration\", \"AMD\", \"CRA\", \"Drusen\"])]\n",
        "exclude_df = disease_df[disease_df['reason'].isin(['AH', 'Blur', 'ERM', \"Hemorrhage\", \"Coagulation\", \"VO\", \"Degeneration\", \"AMD\", \"CRA\", \"Drusen\"])]\n",
        "\n",
        "# Extract the ids (filenames) from ah_blur_df that match the 'AH' or 'Blur' criteria\n",
        "ah_blur_ids = exclude_df['id'].tolist()\n",
        "\n",
        "# Now remove these filenames from comparison_df\n",
        "cleaned_df = original_df[~original_df['filename'].isin(ah_blur_ids)]\n",
        "len(cleaned_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe7gfH8JrBeu",
        "outputId": "7abc9145-05ce-4aef-f7f4-64c1eca62da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4618"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Finetune age estimation model** *斜体テキスト*"
      ],
      "metadata": {
        "id": "fvVGk-TUqOqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "num_epochs = 50\n",
        "patience = 10\n",
        "seed = 42\n",
        "img_dir = \"/content/drive/MyDrive/Deep_learning/Fundus_metabolic/images_whole_384px\"\n",
        "\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(seed)\n",
        "\n",
        "\n",
        "# データセットクラスの定義\n",
        "class FundusDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_dir, transform=None):\n",
        "        self.data = dataframe\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        label = self.data.iloc[idx, 1]  # AGEカラムのインデックス\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# データの前処理とオーグメンテーション\n",
        "transform_train = transforms.Compose([\n",
        "    #transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=5),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
        "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomGrayscale(p=0.1),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 検証用の変換（オーグメンテーションなし）\n",
        "transform_val = transforms.Compose([\n",
        "    #transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "full_dataset = FundusDataset(cleaned_df, img_dir)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_dataset.dataset.transform = transform_train\n",
        "val_dataset.dataset.transform = transform_val\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "# CosineLRSchedulerの設定\n",
        "scheduler = CosineLRScheduler(\n",
        "    optimizer,\n",
        "    t_initial=num_epochs,\n",
        "    lr_min=1e-4,\n",
        "    warmup_t=5,\n",
        "    warmup_lr_init=5e-5,\n",
        "    warmup_prefix=True\n",
        ")\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "# Early Stopping クラス\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.best_model = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_model = copy.deepcopy(model.state_dict())\n",
        "            return True\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_model = copy.deepcopy(model.state_dict())\n",
        "            self.counter = 0\n",
        "            return True\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "            return False\n",
        "\n",
        "# トレーニング関数の修正\n",
        "def train(model, train_loader, criterion, optimizer, device, scaler):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device).float()\n",
        "        inputs = inputs.to(memory_format=torch.channels_last)\n",
        "\n",
        "        for param in model.parameters():\n",
        "            param.grad = None\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda'):\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "        all_predictions.extend(outputs.detach().cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    mse = mean_squared_error(all_targets, all_predictions)\n",
        "    r2 = r2_score(all_targets, all_predictions)\n",
        "    return epoch_loss, mse, r2\n",
        "\n",
        "# 評価関数の修正\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device).float()\n",
        "            inputs = inputs.to(memory_format=torch.channels_last)\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    mse = mean_squared_error(all_targets, all_predictions)\n",
        "    r2 = r2_score(all_targets, all_predictions)\n",
        "    return epoch_loss, mse, r2\n",
        "\n",
        "# GradScalerの更新\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "# モデルの出力層の調整（必要に応じて）\n",
        "# model.fc = nn.Linear(model.fc.in_features, 1)  # 1つの出力（回帰の場合）\n",
        "\n",
        "# 損失関数の変更\n",
        "criterion = nn.MSELoss()  # 回帰問題の場合\n",
        "\n",
        "# トレーニングループの修正\n",
        "early_stopping = EarlyStopping(patience=patience)\n",
        "history = {'train_loss': [], 'train_mse': [], 'train_r2': [],\n",
        "           'val_loss': [], 'val_mse': [], 'val_r2': [], 'lr': []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train_loss, train_mse, train_r2 = train(model, train_loader, criterion, optimizer, device, scaler)\n",
        "    val_loss, val_mse, val_r2 = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    scheduler.step(epoch + 1)\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    history['lr'].append(current_lr)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_mse'].append(train_mse)\n",
        "    history['train_r2'].append(train_r2)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_mse'].append(val_mse)\n",
        "    history['val_r2'].append(val_r2)\n",
        "\n",
        "    is_best = early_stopping(val_loss, model)\n",
        "\n",
        "    if early_stopping.best_model is not None:\n",
        "        model.load_state_dict(early_stopping.best_model)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train MSE: {train_mse:.4f}, Train R2: {train_r2:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val MSE: {val_mse:.4f}, Val R2: {val_r2:.4f}\")\n",
        "    print(f\"Epoch duration: {epoch_duration:.2f} seconds\")\n",
        "    print(f\"Best model {'updated' if is_best else 'not updated'}\")\n",
        "    print(f\"Current learning rate: {current_lr:.6f}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "# 最終評価の修正\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        inputs = inputs.to(memory_format=torch.channels_last)\n",
        "        outputs = model(inputs).squeeze()\n",
        "        all_preds.extend(outputs.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "final_mse = mean_squared_error(all_labels, all_preds)\n",
        "final_r2 = r2_score(all_labels, all_preds)\n",
        "print(f\"Final MSE: {final_mse:.4f}\")\n",
        "print(f\"Final R2 Score: {final_r2:.4f}\")\n",
        "\n",
        "# 訓練結果のグラフ表示\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.plot(history['train_mse'], label='Train MSE')\n",
        "plt.plot(history['val_mse'], label='Validation MSE')\n",
        "plt.title('Mean Squared Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.plot(history['train_r2'], label='Train R2')\n",
        "plt.plot(history['val_r2'], label='Validation R2')\n",
        "plt.title('R2 Score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('R2')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.plot(history['lr'], label='Learning Rate')\n",
        "plt.title('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Khn1IEStpeni",
        "outputId": "238e986c-a669-4ab1-8aff-5afe7668dcfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-a4dc6a134a4d>:86: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 散布図の追加\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.scatter(all_labels, all_preds, alpha=0.5)\n",
        "plt.plot([min(all_labels), max(all_labels)], [min(all_labels), max(all_labels)], 'r--', lw=2)\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predictions')\n",
        "plt.title('True vs Predicted Values')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 残差プロットの追加\n",
        "residuals = np.array(all_labels) - np.array(all_preds)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(all_preds, residuals, alpha=0.5)\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rl8qtd4ZpepZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RjoS9WeRperc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}