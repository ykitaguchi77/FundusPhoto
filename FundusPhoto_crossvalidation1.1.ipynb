{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled90.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/FundusPhoto/blob/main/FundusPhoto_crossvalidation1.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4vbMuKXy9W6",
        "outputId": "19a6a612-41d5-4ff9-f11e-393a6fb1f98c"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "import csv\n",
        "import pandas as pd\n",
        "import glob\n",
        "from PIL import Image\n",
        "import sys\n",
        "import statistics\n",
        "\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "#あればGPUを使用\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "!nvidia-smi -L\n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_optimizer in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer) (0.1.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->torch_optimizer) (3.7.4.3)\n",
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-3d160d94-9d74-a56c-0bd0-37c2611ab109)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "lV9UyGl4BL3c",
        "outputId": "3e82a29a-6c19-4e1b-8b1e-7ddbcac3fb52"
      },
      "source": [
        "name = \"cropped\"\n",
        "model = \"A2\"\n",
        "\n",
        "MODEL_NAME = name +\"_\"+model+\"_pretrained\"\n",
        "DATASET_NAME = name+'_img_trainval'\n",
        "DATASET_PATH = '/content/drive/MyDrive/Deep_learning/FundusPhoto'\n",
        "NET_NAME = \"RepVGG_\" +model  #RepVGG_A2 or RepVGG_B3\n",
        "PRETRAINED = True\n",
        "\n",
        "os.chdir(DATASET_PATH)\n",
        "\n",
        "TRAIN_FOLDER_NAME = 'train' #TRAINイメージのフォルダ\n",
        "VAL_FOLDER_NAME = 'val' #VALイメージのフォルダ\n",
        "\n",
        "FILENAME_LABELCSV = 'name_age.csv' #年齢の値のcsv\n",
        "FILENAME_RESULTCSV = 'result.csv' #年齢推定結果を書き出すcsv\n",
        "FILENAME_RESULT_ANALYSISCSV = 'result_analysis.csv' #推定結果の解析結果を書き出すcsv\n",
        "\n",
        "\n",
        "MODEL_PATH = '/content/drive/MyDrive/Deep_learning/FundusPhoto/model'\n",
        "#OPTIMIZER_PATH = \"./optimizer_multi.pth\"\n",
        "LOG_PATH = \"./log_multi.txt\"\n",
        "ROC_PATH = \"./roc_multi.png\"\n",
        "CHECKPOINT_COUNT = 10\n",
        "EPOCH = 100\n",
        "PATIENCE = 20 #early stopping patience; how long to wait after last time validation loss improved.\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "\n",
        "# transforms param\n",
        "PX = 224 #画像のサイズ\n",
        "TRAIN_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "TRAIN_CROP_SCALE =(0.75,1.0)\n",
        "TRAIN_BRIGHTNESS_PARAM = 0.2\n",
        "TRAIN_CONTRAST_PARAM = 0.1\n",
        "TRAIN_SATURATION_PARAM = 0.1\n",
        "TRAIN_RANDOM_ROTATION = 3\n",
        "TRAIN_HUE_PARAM = 0.02\n",
        "VAL_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "\n",
        "\n",
        "#csvファイルを開く\n",
        "df_labelcsv = pd.read_csv(FILENAME_LABELCSV)\n",
        "\n",
        "#csvファイルを表示\n",
        "print(df_labelcsv)\n",
        "\n",
        "#ID,ageの列の値をリストとして取り出す\n",
        "df_filename = df_labelcsv['filename'].values\n",
        "df_age = df_labelcsv['age'].values\n",
        "\n",
        "#CSVファイル内の画像数\n",
        "print(len(df_labelcsv))\n",
        "\n",
        "\"\"\"\n",
        "#画像フォルダ内の画像数\n",
        "print(len(os.listdir(DATASET_NAME +\"/\"+ TRAIN_FOLDER_NAME))\n",
        "+len(os.listdir(DATASET_NAME +\"/\"+ VAL_FOLDER_NAME))\n",
        "+len(os.listdir(DATASET_NAME +\"/\"+ TEST_FOLDER_NAME)))\n",
        "\"\"\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   filename  age\n",
            "0     img00085008_00_1R.jpg   61\n",
            "1     img00085024_00_1R.jpg   29\n",
            "2     img00241280_10_1R.jpg   51\n",
            "3     img00265140_00_1R.jpg   29\n",
            "4     img00265140_00_2L.jpg   29\n",
            "...                     ...  ...\n",
            "1409  img76791392_10_1R.jpg   38\n",
            "1410  img76843122_10_1R.jpg   49\n",
            "1411  img76843122_11_1R.jpg   49\n",
            "1412  img76888512_00_1R.jpg   74\n",
            "1413  img76888512_00_2L.jpg   74\n",
            "\n",
            "[1414 rows x 2 columns]\n",
            "1414\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#画像フォルダ内の画像数\\nprint(len(os.listdir(DATASET_NAME +\"/\"+ TRAIN_FOLDER_NAME))\\n+len(os.listdir(DATASET_NAME +\"/\"+ VAL_FOLDER_NAME))\\n+len(os.listdir(DATASET_NAME +\"/\"+ TEST_FOLDER_NAME)))\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ejg4rneA3qI"
      },
      "source": [
        "#**Modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srdzPaN3A1nr"
      },
      "source": [
        "####################################\n",
        "#Test with early-stopping\n",
        "####################################\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, loss_func, batch_size, optimizer, patience, n_epochs, device):\n",
        "    \n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = [] \n",
        "    \n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "\n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        for batch, (image_tensor, target) in enumerate(train_loader, 1):\n",
        "            # convert batch-size labels to batch-size x 1 tensor\n",
        "            #target = target.squeeze(1)\n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor)\n",
        "            # calculate the loss\n",
        "            loss = loss_func(output, target)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "       \n",
        "        model.eval() # prep model for evaluation\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        for image_tensor, target in val_loader:  \n",
        "            #target = target.squeeze(1)         \n",
        "            target = target.view(len(target), 1)\n",
        "\n",
        "            image_tensor = image_tensor.to(device)\n",
        "            target = target.to(device)\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(image_tensor)\n",
        "            # calculate the loss\n",
        "            # record validation loss\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "        # print training/validation statistics \n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "        \n",
        "        epoch_len = len(str(n_epochs))\n",
        "        \n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' )\n",
        "        \n",
        "        print(print_msg)\n",
        "        \n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        \n",
        "        # early_stopping needs the validation loss to check if it has decresed, \n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        \n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return  model, avg_train_losses, avg_valid_losses\n",
        "\n",
        "\n",
        "\n",
        "##################################\n",
        "#Define RepVGG-B3\n",
        "##################################\n",
        "\n",
        "\n",
        "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
        "    result = nn.Sequential()\n",
        "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                                  kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
        "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
        "    return result\n",
        "\n",
        "class RepVGGBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
        "        super(RepVGGBlock, self).__init__()\n",
        "        self.deploy = deploy\n",
        "        self.groups = groups\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        assert kernel_size == 3\n",
        "        assert padding == 1\n",
        "\n",
        "        padding_11 = padding - kernel_size // 2\n",
        "\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "\n",
        "        if deploy:\n",
        "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
        "\n",
        "        else:\n",
        "            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n",
        "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
        "            print('RepVGG Block, identity = ', self.rbr_identity)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if hasattr(self, 'rbr_reparam'):\n",
        "            return self.nonlinearity(self.rbr_reparam(inputs))\n",
        "\n",
        "        if self.rbr_identity is None:\n",
        "            id_out = 0\n",
        "        else:\n",
        "            id_out = self.rbr_identity(inputs)\n",
        "\n",
        "        return self.nonlinearity(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out)\n",
        "\n",
        "\n",
        "\n",
        "#   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
        "#   You can get the equivalent kernel and bias at any time and do whatever you want,\n",
        "    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n",
        "#   May be useful for quantization or pruning.\n",
        "    def get_equivalent_kernel_bias(self):\n",
        "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
        "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
        "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
        "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
        "\n",
        "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
        "        if kernel1x1 is None:\n",
        "            return 0\n",
        "        else:\n",
        "            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n",
        "\n",
        "    def _fuse_bn_tensor(self, branch):\n",
        "        if branch is None:\n",
        "            return 0, 0\n",
        "        if isinstance(branch, nn.Sequential):\n",
        "            kernel = branch.conv.weight\n",
        "            running_mean = branch.bn.running_mean\n",
        "            running_var = branch.bn.running_var\n",
        "            gamma = branch.bn.weight\n",
        "            beta = branch.bn.bias\n",
        "            eps = branch.bn.eps\n",
        "        else:\n",
        "            assert isinstance(branch, nn.BatchNorm2d)\n",
        "            if not hasattr(self, 'id_tensor'):\n",
        "                input_dim = self.in_channels // self.groups\n",
        "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
        "                for i in range(self.in_channels):\n",
        "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
        "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
        "            kernel = self.id_tensor\n",
        "            running_mean = branch.running_mean\n",
        "            running_var = branch.running_var\n",
        "            gamma = branch.weight\n",
        "            beta = branch.bias\n",
        "            eps = branch.eps\n",
        "        std = (running_var + eps).sqrt()\n",
        "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
        "        return kernel * t, beta - running_mean * gamma / std\n",
        "\n",
        "    def repvgg_convert(self):\n",
        "        kernel, bias = self.get_equivalent_kernel_bias()\n",
        "        return kernel.detach().cpu().numpy(), bias.detach().cpu().numpy(),\n",
        "\n",
        "\n",
        "\n",
        "class RepVGG(nn.Module):\n",
        "\n",
        "    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False):\n",
        "        super(RepVGG, self).__init__()\n",
        "\n",
        "        assert len(width_multiplier) == 4\n",
        "\n",
        "        self.deploy = deploy\n",
        "        self.override_groups_map = override_groups_map or dict()\n",
        "\n",
        "        assert 0 not in self.override_groups_map\n",
        "\n",
        "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
        "\n",
        "        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy)\n",
        "        self.cur_layer_idx = 1\n",
        "        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n",
        "        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n",
        "        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n",
        "        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\n",
        "\n",
        "\n",
        "    def _make_stage(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        blocks = []\n",
        "        for stride in strides:\n",
        "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
        "            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n",
        "                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy))\n",
        "            self.in_planes = planes\n",
        "            self.cur_layer_idx += 1\n",
        "        return nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stage0(x)\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.stage4(out)\n",
        "        out = self.gap(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "optional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\n",
        "g2_map = {l: 2 for l in optional_groupwise_layers}\n",
        "g4_map = {l: 4 for l in optional_groupwise_layers}\n",
        "\n",
        "def create_RepVGG_A0(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A1(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_A2(deploy=False):\n",
        "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
        "                  width_multiplier=[1.5, 1.5, 1.5, 2.75], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B0(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B1g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B2g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "def create_RepVGG_B3(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=None, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g2(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g2_map, deploy=deploy)\n",
        "\n",
        "def create_RepVGG_B3g4(deploy=False):\n",
        "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\n",
        "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g4_map, deploy=deploy)\n",
        "\n",
        "\n",
        "func_dict = {\n",
        "'RepVGG-A0': create_RepVGG_A0,\n",
        "'RepVGG-A1': create_RepVGG_A1,\n",
        "'RepVGG-A2': create_RepVGG_A2,\n",
        "'RepVGG-B0': create_RepVGG_B0,\n",
        "'RepVGG-B1': create_RepVGG_B1,\n",
        "'RepVGG-B1g2': create_RepVGG_B1g2,\n",
        "'RepVGG-B1g4': create_RepVGG_B1g4,\n",
        "'RepVGG-B2': create_RepVGG_B2,\n",
        "'RepVGG-B2g2': create_RepVGG_B2g2,\n",
        "'RepVGG-B2g4': create_RepVGG_B2g4,\n",
        "'RepVGG-B3': create_RepVGG_B3,\n",
        "'RepVGG-B3g2': create_RepVGG_B3g2,\n",
        "'RepVGG-B3g4': create_RepVGG_B3g4,\n",
        "}\n",
        "def get_RepVGG_func_by_name(name):\n",
        "    return func_dict[name]\n",
        "\n",
        "\n",
        "\n",
        "#   Use this for converting a customized model with RepVGG as one of its components (e.g., the backbone of a semantic segmentation model)\n",
        "#   The use case will be like\n",
        "#   1.  Build train_model. For example, build a PSPNet with a training-time RepVGG as backbone\n",
        "#   2.  Train train_model or do whatever you want\n",
        "#   3.  Build deploy_model. In the above example, that will be a PSPNet with an inference-time RepVGG as backbone\n",
        "#   4.  Call this func\n",
        "#   ====================== the pseudo code will be like\n",
        "#   train_backbone = create_RepVGG_B2(deploy=False)\n",
        "#   train_backbone.load_state_dict(torch.load('RepVGG-B2-train.pth'))\n",
        "#   train_pspnet = build_pspnet(backbone=train_backbone)\n",
        "#   segmentation_train(train_pspnet)\n",
        "#   deploy_backbone = create_RepVGG_B2(deploy=True)\n",
        "#   deploy_pspnet = build_pspnet(backbone=deploy_backbone)\n",
        "#   whole_model_convert(train_pspnet, deploy_pspnet)\n",
        "#   segmentation_test(deploy_pspnet)\n",
        "def whole_model_convert(train_model:torch.nn.Module, deploy_model:torch.nn.Module, save_path=None):\n",
        "    all_weights = {}\n",
        "    for name, module in train_model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            all_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            all_weights[name + '.rbr_reparam.bias'] = bias\n",
        "            print('convert RepVGG block')\n",
        "        else:\n",
        "            for p_name, p_tensor in module.named_parameters():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.detach().cpu().numpy()\n",
        "            for p_name, p_tensor in module.named_buffers():\n",
        "                full_name = name + '.' + p_name\n",
        "                if full_name not in all_weights:\n",
        "                    all_weights[full_name] = p_tensor.cpu().numpy()\n",
        "\n",
        "    deploy_model.load_state_dict(all_weights)\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "#   Use this when converting a RepVGG without customized structures.\n",
        "#   train_model = create_RepVGG_A0(deploy=False)\n",
        "#   train train_model\n",
        "#   deploy_model = repvgg_convert(train_model, create_RepVGG_A0, save_path='repvgg_deploy.pth')\n",
        "def repvgg_model_convert(model:torch.nn.Module, build_func, save_path=None):\n",
        "    converted_weights = {}\n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'repvgg_convert'):\n",
        "            kernel, bias = module.repvgg_convert()\n",
        "            converted_weights[name + '.rbr_reparam.weight'] = kernel\n",
        "            converted_weights[name + '.rbr_reparam.bias'] = bias\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "            converted_weights[name + '.weight'] = module.weight.detach().cpu().numpy()\n",
        "            converted_weights[name + '.bias'] = module.bias.detach().cpu().numpy()\n",
        "    del model\n",
        "\n",
        "    deploy_model = build_func(deploy=True)\n",
        "    for name, param in deploy_model.named_parameters():\n",
        "        print('deploy param: ', name, param.size(), np.mean(converted_weights[name]))\n",
        "        param.data = torch.from_numpy(converted_weights[name]).float()\n",
        "\n",
        "    if save_path is not None:\n",
        "        torch.save(deploy_model.state_dict(), save_path)\n",
        "\n",
        "    return deploy_model\n",
        "\n",
        "\n",
        "class mod_RepVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mod_RepVGG, self).__init__()\n",
        "        repVGG = model_ft\n",
        "        num_ftrs = model_ft.linear.in_features  #in_featureはA2では1408、B3では2560\n",
        "        self.repVGG = nn.Sequential(*list(model_ft.children())[:-1])\n",
        "        self.fc = nn.Linear(in_features=num_ftrs, out_features=1)  #out_featuresを1に\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.repVGG(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "#####################################\n",
        "##### Datasets and Dataloader ############\n",
        "#####################################\n",
        "\n",
        "class SimpleImageDataset(Dataset):\n",
        "    def __init__(self, folder_path, csv_path, transform):\n",
        "        self.transform = transform\n",
        "        self.folder_path = folder_path\n",
        "        self.item_paths = []\n",
        "        self.item_dict = {}\n",
        "        self.age = []\n",
        "\n",
        "        for i in range(len(os.listdir(folder_path))):\n",
        "              img_name = os.listdir(self.folder_path)[i]\n",
        "              age_temp = df_labelcsv[df_labelcsv['filename'] == img_name].iloc[0,1] #age\n",
        "              self.age.append(float(age_temp)/100)\n",
        "\n",
        "              img_path = os.path.join(self.folder_path, img_name)\n",
        "              self.item_paths.append(img_path)\n",
        "              #self.item_dict[image_path] = self.age\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.item_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.item_paths[idx]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image)\n",
        "        target = torch.tensor([self.age[idx]])      \n",
        "        return tensor_image, target\n",
        "\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomVerticalFlip(),\n",
        "                transforms.RandomRotation(degrees=10),\n",
        "                transforms.ToTensor(),\n",
        "                ])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                transforms.Resize(PX),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomVerticalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                ]) \n",
        "\n",
        "\"\"\"\n",
        "#水増し後の画像を可視化する関数\n",
        "def show_img(dataset):\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    for i in range(5):\n",
        "        image, label = dataset[i]\n",
        "        image = image.permute(1, 2, 0)\n",
        "        plt.subplot(1, 5, i+1)\n",
        "        plt.tick_params(labelbottom=False, labelleft=False, labelright=False, labeltop=False)\n",
        "        plt.tick_params(bottom=False, left=False, right=False, top=False)\n",
        "        plt.imshow(image)\n",
        "        \n",
        "\n",
        "#画像の可視化\n",
        "show_img(train_dataset)\n",
        "\n",
        "#print(train_dataset[1])\n",
        "\"\"\"\n",
        "\n",
        "#Calculating result\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(result_csv, image_name):\n",
        "      image_name = image_name\n",
        "      label = df_result[df_result.iloc[:,0] == image_name].iloc[0,1] #df_resultよりimage_nameが含まれる行を抜き出して年齢を取得\n",
        "      return(image_name, label)\n",
        "\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.ToTensor()])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "\n",
        "def my_round(x, d=0): #四捨五入\n",
        "    p = 10 ** d\n",
        "    return float(math.floor((x * p) + math.copysign(0.5, x)))/p\n",
        "\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を出力\n",
        "def image_eval(image_tensor, model_ft): \n",
        "    output = model_ft(image_tensor).item()*100\n",
        "    return output\n",
        "\n",
        "#result.csvに結果を記入\n",
        "def write_result(df, image_name, pred, row):\n",
        "    df.loc[df_result.iloc[:,0] == image_name, row] = pred  #df_resultよりimage_nameが含まれる行を抜き出して年齢を取得\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM3zE1Ft0exa"
      },
      "source": [
        "#ここからがメイン\n",
        "process = [0,1,2,3,4]\n",
        "for i in process:\n",
        "    print(\"Start \"+ str(i) + \" th analysis!\")\n",
        "\n",
        "    train_dataset = SimpleImageDataset(os.path.join(DATASET_NAME, \"1\", TRAIN_FOLDER_NAME), os.path.join(DATASET_NAME, FILENAME_LABELCSV), train_data_transforms)\n",
        "    val_dataset = SimpleImageDataset(os.path.join(DATASET_NAME, \"1\", VAL_FOLDER_NAME), os.path.join(DATASET_NAME, FILENAME_LABELCSV),  val_data_transforms)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
        "\n",
        "    #print(TRAIN_FOLDER_NAME + \"_dataset_size：\" + str(len(train_dataset)))\n",
        "    #print(VAL_FOLDER_NAME + \"_dataset_size：\" + str(len(val_dataset)))\n",
        "\n",
        "    ####################################\n",
        "    #ConvNetの調整\n",
        "    ####################################\n",
        "\n",
        "    if NET_NAME == \"RepVGG_A2\":\n",
        "        model_ft = create_RepVGG_A2(deploy=False)\n",
        "        if PRETRAINED == True:\n",
        "            model_ft.load_state_dict(torch.load('/content/drive/MyDrive/Deep_learning/RepVGG-A2-train.pth'))\n",
        "            print(\"Pretrained model downloaded\")\n",
        "        elif PRETRAINED == False:\n",
        "            pass\n",
        "        else:\n",
        "            print(\"TrueあるいはFalseで指定して下さい\")\n",
        "            sys.exit(1)\n",
        "\n",
        "    elif NET_NAME == \"RepVGG_B3\":\n",
        "        model_ft = create_RepVGG_B3(deploy=False)\n",
        "        if PRETRAINED == True:\n",
        "            model_ft.load_state_dict(torch.load('/content/drive/MyDrive/Deep_learning/RepVGG-B3-200epochs-train.pth'))\n",
        "            print(\"Pretrained model downloaded\")\n",
        "        elif PRETRAINED == False:\n",
        "            pass\n",
        "        else:\n",
        "            print(\"TrueあるいはFalseで指定して下さい\")\n",
        "            sys.exit(1)\n",
        "    else:\n",
        "        print(\"RepVGG_A2あるいはRepVGG_B3を指定して下さい\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    #model_ft.load_state_dict(torch.load(MODEL_PATH)) \n",
        "    model_ft = mod_RepVGG()\n",
        "\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #損失関数を定義\n",
        "    loss_func = nn.MSELoss()\n",
        "\n",
        "    #Optimizer\n",
        "    optimizer_ft = torch.optim.AdamW(model_ft.parameters(), 0.0002)\n",
        "\n",
        "    ####################################\n",
        "    # Train and Save network\n",
        "    ####################################\n",
        "\n",
        "    model, train_loss, valid_loss = train_model(model_ft, loss_func, BATCH_SIZE, optimizer_ft, PATIENCE, EPOCH, device)\n",
        "\n",
        "    #ネットワークの保存\n",
        "    PATH = MODEL_PATH+'/'+MODEL_NAME+'_'+str(i)+'.pth'\n",
        "    torch.save(model_ft.state_dict(), PATH)\n",
        "\n",
        "    ####################################\n",
        "    # Result Analysis\n",
        "    ####################################\n",
        "\n",
        "    model_ft.eval() # prep model for evaluation\n",
        "\n",
        "    outputs,targets,errors =[], [], []\n",
        "    for image_tensor, target in val_loader:  \n",
        "          target = target.view(len(target), 1)         \n",
        "          image_tensor = image_tensor.to(device)\n",
        "          target = target.to(device)\n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          output = model_ft(image_tensor)\n",
        "\n",
        "          outputs.append((output[0]*100).item())      \n",
        "          targets.append((target[0]*100).item())\n",
        "          #print('estimate R:'+str(my_round(output[0,0].item()))+'mm, L:'+str(my_round(output[0,1].item()))+'mm / target R:'+str(target[0,0].item())+'mm, L:'+str(target[0,1].item())+'mm')\n",
        "\n",
        "          errors.append((output[0]*100).item()-(target[0]*100).item())\n",
        "    \n",
        "    AbsError = [abs(i) for i in errors]\n",
        "    MeanError = str(statistics.mean(errors))\n",
        "    StdError = str(statistics.stdev(errors))\n",
        "    MeanAbsError = str(statistics.mean(AbsError))\n",
        "    StdAbsError = str(statistics.stdev(AbsError))\n",
        "\n",
        "    print('MeanError: '+MeanError)\n",
        "    print('StdError: '+StdError)\n",
        "    print('MeanAbsError: '+MeanAbsError)\n",
        "    print('StdAbsError: '+StdAbsError)\n",
        "\n",
        "    \n",
        "    #result_analysis.csv作成（ファイルがなければ）\n",
        "    if os.path.exists(FILENAME_RESULT_ANALYSISCSV) == False:\n",
        "        columns = []\n",
        "        index = [\"AveError\", \"StdError\", \"AveAbsError\", \"StdAbsError\"]\n",
        "        df_result_analysis = pd.DataFrame(index=index, columns=columns)\n",
        "        df_result_analysis.to_csv(FILENAME_RESULT_ANALYSISCSV)\n",
        "    else:\n",
        "        print(FILENAME_RESULT_ANALYSISCSV + \" already exists!\")\n",
        "\n",
        "    df_result_analysis = pd.read_csv(FILENAME_RESULT_ANALYSISCSV, index_col=0)\n",
        "    df_result_analysis[MODEL_NAME+\"_\"+str(i)] = [MeanError, StdError, MeanAbsError, StdAbsError]\n",
        "\n",
        "\n",
        "    PATH = MODEL_PATH+'/'+MODEL_NAME+'.csv'\n",
        "    df_result_analysis.to_csv(FILENAME_RESULT_ANALYSISCSV, index=True)\n",
        "    \n",
        "\n",
        "\n",
        "    #result.csv作成（ファイルがなければ）\n",
        "    if os.path.exists(FILENAME_RESULTCSV) == False:\n",
        "        df_result = df_labelcsv.copy()\n",
        "        df_result.to_csv(FILENAME_RESULTCSV)\n",
        "    else:\n",
        "        print(FILENAME_RESULTCSV + \" already exists!\")\n",
        "\n",
        "    df_result = pd.read_csv(FILENAME_RESULTCSV)\n",
        "    #print(df_result)\n",
        "    print(\"Calculating prediction results!\")\n",
        "\n",
        "    #valフォルダ内のファイル名を取得\n",
        "    train_data_path = glob.glob(DATASET_NAME + \"/\" +str(i) + \"/\" + TRAIN_FOLDER_NAME+\"/*\")\n",
        "    val_data_path = glob.glob(DATASET_NAME + \"/\" + str(i) + \"/\" + VAL_FOLDER_NAME+\"/*\")\n",
        "\n",
        "    data_path = [train_data_path, val_data_path]\n",
        "    k=0\n",
        "    for j in data_path:\n",
        "        for m in j:\n",
        "              image_name, label = getlabel(df_result, os.path.basename(m))  #画像の名前とラベルを取得\n",
        "              image_tensor = image_transform(m)  #予測のための画像下処理\n",
        "              pred = image_eval(image_tensor, model_ft)  #予測結果を出力   \n",
        "              write_result(df_result, image_name, pred, MODEL_NAME+\"_\"+str(i))\n",
        "              print(str(k)+\"/\"+str(len(df_result)) + \" images processed! Predicted age is \"+str(pred))\n",
        "              k+=1\n",
        "    #print(df_result)\n",
        "\n",
        "    #Resultファイルを書き出し\n",
        "    df_result.to_csv(FILENAME_RESULTCSV, index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg_lhEMldF5v"
      },
      "source": [
        "#**Load network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rVSslGCJNPb"
      },
      "source": [
        "name = \"cropped\"\n",
        "model = \"A2\"\n",
        "\n",
        "MODEL_NAME = name +\"_\"+model+\"_pretrained\"\n",
        "DATASET_NAME = name+'_img_trainval'\n",
        "DATASET_PATH = '/content/drive/MyDrive/Deep_learning/FundusPhoto'\n",
        "NET_NAME = \"RepVGG_\" +model  #RepVGG_A2 or RepVGG_B3\n",
        "PRETRAINED = True"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y_0OA1br317",
        "outputId": "7a32397d-54cf-440d-df27-f926001d291d"
      },
      "source": [
        "if NET_NAME == \"RepVGG_A2\":\n",
        "    model_ft = create_RepVGG_A2(deploy=False)\n",
        "\n",
        "elif NET_NAME == \"RepVGG_B3\":\n",
        "    model_ft = create_RepVGG_B3(deploy=False)\n",
        "\n",
        "else:\n",
        "    print(\"RepVGG_A2あるいはRepVGG_B3を指定して下さい\")\n",
        "    sys.exit(1)\n",
        "\n",
        "#model_ft.load_state_dict(torch.load(MODEL_PATH)) \n",
        "model_ft = mod_RepVGG()\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "RepVGG Block, identity =  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fO7Jlr21roBg",
        "outputId": "69b92c69-79f6-4ae1-a660-82921499739f"
      },
      "source": [
        "#ここからがメイン\n",
        "process = [0,1,2,3,4]\n",
        "for i in process:\n",
        "    print(\"Start \"+ str(i) + \" th analysis!\")\n",
        "\n",
        "    #トレーニングしたパラメーターを適用\n",
        "    PATH = MODEL_PATH+'/'+MODEL_NAME+'_'+str(i)+'.pth'\n",
        "    model_ft.load_state_dict(torch.load(PATH))\n",
        "    model_ft.eval()\n",
        "\n",
        "    df_result = pd.read_csv(FILENAME_RESULTCSV)\n",
        "    print(PATH)\n",
        "    print(\"Calculating prediction results!\")\n",
        "\n",
        "    #valフォルダ内のファイル名を取得\n",
        "    train_data_path = glob.glob(DATASET_NAME + \"/\" +str(i) + \"/\" + TRAIN_FOLDER_NAME+\"/*\")\n",
        "    val_data_path = glob.glob(DATASET_NAME + \"/\" + str(i) + \"/\" + VAL_FOLDER_NAME+\"/*\")\n",
        "\n",
        "    data_path = [train_data_path, val_data_path]\n",
        "    k=0\n",
        "    for j in data_path:\n",
        "        for m in j:\n",
        "              image_name, label = getlabel(df_result, os.path.basename(m))  #画像の名前とラベルを取得\n",
        "              image_tensor = image_transform(m)  #予測のための画像下処理\n",
        "              pred = image_eval(image_tensor, model_ft)  #予測結果を出力   \n",
        "              write_result(df_result, image_name, pred, MODEL_NAME+\"_\"+str(i))\n",
        "              print(str(k)+\"/\"+str(len(df_result)) + \" images processed!\")\n",
        "              print(\"image_name: \"+image_name+\", pred: \"+pred+\", label: \"+label)\n",
        "              k+=1\n",
        "    print(df_result)\n",
        "\n",
        "    #Resultファイルを書き出し\n",
        "    df_result.to_csv(FILENAME_RESULTCSV, index=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start 0 th analysis!\n",
            "/content/drive/MyDrive/Deep_learning/FundusPhoto/model/cropped_A2_pretrained_0.pth\n",
            "Calculating prediction results!\n",
            "0/1414 images processed!\n",
            "1/1414 images processed!\n",
            "2/1414 images processed!\n",
            "3/1414 images processed!\n",
            "4/1414 images processed!\n",
            "5/1414 images processed!\n",
            "6/1414 images processed!\n",
            "7/1414 images processed!\n",
            "8/1414 images processed!\n",
            "9/1414 images processed!\n",
            "10/1414 images processed!\n",
            "11/1414 images processed!\n",
            "12/1414 images processed!\n",
            "13/1414 images processed!\n",
            "14/1414 images processed!\n",
            "15/1414 images processed!\n",
            "16/1414 images processed!\n",
            "17/1414 images processed!\n",
            "18/1414 images processed!\n",
            "19/1414 images processed!\n",
            "20/1414 images processed!\n",
            "21/1414 images processed!\n",
            "22/1414 images processed!\n",
            "23/1414 images processed!\n",
            "24/1414 images processed!\n",
            "25/1414 images processed!\n",
            "26/1414 images processed!\n",
            "27/1414 images processed!\n",
            "28/1414 images processed!\n",
            "29/1414 images processed!\n",
            "30/1414 images processed!\n",
            "31/1414 images processed!\n",
            "32/1414 images processed!\n",
            "33/1414 images processed!\n",
            "34/1414 images processed!\n",
            "35/1414 images processed!\n",
            "36/1414 images processed!\n",
            "37/1414 images processed!\n",
            "38/1414 images processed!\n",
            "39/1414 images processed!\n",
            "40/1414 images processed!\n",
            "41/1414 images processed!\n",
            "42/1414 images processed!\n",
            "43/1414 images processed!\n",
            "44/1414 images processed!\n",
            "45/1414 images processed!\n",
            "46/1414 images processed!\n",
            "47/1414 images processed!\n",
            "48/1414 images processed!\n",
            "49/1414 images processed!\n",
            "50/1414 images processed!\n",
            "51/1414 images processed!\n",
            "52/1414 images processed!\n",
            "53/1414 images processed!\n",
            "54/1414 images processed!\n",
            "55/1414 images processed!\n",
            "56/1414 images processed!\n",
            "57/1414 images processed!\n",
            "58/1414 images processed!\n",
            "59/1414 images processed!\n",
            "60/1414 images processed!\n",
            "61/1414 images processed!\n",
            "62/1414 images processed!\n",
            "63/1414 images processed!\n",
            "64/1414 images processed!\n",
            "65/1414 images processed!\n",
            "66/1414 images processed!\n",
            "67/1414 images processed!\n",
            "68/1414 images processed!\n",
            "69/1414 images processed!\n",
            "70/1414 images processed!\n",
            "71/1414 images processed!\n",
            "72/1414 images processed!\n",
            "73/1414 images processed!\n",
            "74/1414 images processed!\n",
            "75/1414 images processed!\n",
            "76/1414 images processed!\n",
            "77/1414 images processed!\n",
            "78/1414 images processed!\n",
            "79/1414 images processed!\n",
            "80/1414 images processed!\n",
            "81/1414 images processed!\n",
            "82/1414 images processed!\n",
            "83/1414 images processed!\n",
            "84/1414 images processed!\n",
            "85/1414 images processed!\n",
            "86/1414 images processed!\n",
            "87/1414 images processed!\n",
            "88/1414 images processed!\n",
            "89/1414 images processed!\n",
            "90/1414 images processed!\n",
            "91/1414 images processed!\n",
            "92/1414 images processed!\n",
            "93/1414 images processed!\n",
            "94/1414 images processed!\n",
            "95/1414 images processed!\n",
            "96/1414 images processed!\n",
            "97/1414 images processed!\n",
            "98/1414 images processed!\n",
            "99/1414 images processed!\n",
            "100/1414 images processed!\n",
            "101/1414 images processed!\n",
            "102/1414 images processed!\n",
            "103/1414 images processed!\n",
            "104/1414 images processed!\n",
            "105/1414 images processed!\n",
            "106/1414 images processed!\n",
            "107/1414 images processed!\n",
            "108/1414 images processed!\n",
            "109/1414 images processed!\n",
            "110/1414 images processed!\n",
            "111/1414 images processed!\n",
            "112/1414 images processed!\n",
            "113/1414 images processed!\n",
            "114/1414 images processed!\n",
            "115/1414 images processed!\n",
            "116/1414 images processed!\n",
            "117/1414 images processed!\n",
            "118/1414 images processed!\n",
            "119/1414 images processed!\n",
            "120/1414 images processed!\n",
            "121/1414 images processed!\n",
            "122/1414 images processed!\n",
            "123/1414 images processed!\n",
            "124/1414 images processed!\n",
            "125/1414 images processed!\n",
            "126/1414 images processed!\n",
            "127/1414 images processed!\n",
            "128/1414 images processed!\n",
            "129/1414 images processed!\n",
            "130/1414 images processed!\n",
            "131/1414 images processed!\n",
            "132/1414 images processed!\n",
            "133/1414 images processed!\n",
            "134/1414 images processed!\n",
            "135/1414 images processed!\n",
            "136/1414 images processed!\n",
            "137/1414 images processed!\n",
            "138/1414 images processed!\n",
            "139/1414 images processed!\n",
            "140/1414 images processed!\n",
            "141/1414 images processed!\n",
            "142/1414 images processed!\n",
            "143/1414 images processed!\n",
            "144/1414 images processed!\n",
            "145/1414 images processed!\n",
            "146/1414 images processed!\n",
            "147/1414 images processed!\n",
            "148/1414 images processed!\n",
            "149/1414 images processed!\n",
            "150/1414 images processed!\n",
            "151/1414 images processed!\n",
            "152/1414 images processed!\n",
            "153/1414 images processed!\n",
            "154/1414 images processed!\n",
            "155/1414 images processed!\n",
            "156/1414 images processed!\n",
            "157/1414 images processed!\n",
            "158/1414 images processed!\n",
            "159/1414 images processed!\n",
            "160/1414 images processed!\n",
            "161/1414 images processed!\n",
            "162/1414 images processed!\n",
            "163/1414 images processed!\n",
            "164/1414 images processed!\n",
            "165/1414 images processed!\n",
            "166/1414 images processed!\n",
            "167/1414 images processed!\n",
            "168/1414 images processed!\n",
            "169/1414 images processed!\n",
            "170/1414 images processed!\n",
            "171/1414 images processed!\n",
            "172/1414 images processed!\n",
            "173/1414 images processed!\n",
            "174/1414 images processed!\n",
            "175/1414 images processed!\n",
            "176/1414 images processed!\n",
            "177/1414 images processed!\n",
            "178/1414 images processed!\n",
            "179/1414 images processed!\n",
            "180/1414 images processed!\n",
            "181/1414 images processed!\n",
            "182/1414 images processed!\n",
            "183/1414 images processed!\n",
            "184/1414 images processed!\n",
            "185/1414 images processed!\n",
            "186/1414 images processed!\n",
            "187/1414 images processed!\n",
            "188/1414 images processed!\n",
            "189/1414 images processed!\n",
            "190/1414 images processed!\n",
            "191/1414 images processed!\n",
            "192/1414 images processed!\n",
            "193/1414 images processed!\n",
            "194/1414 images processed!\n",
            "195/1414 images processed!\n",
            "196/1414 images processed!\n",
            "197/1414 images processed!\n",
            "198/1414 images processed!\n",
            "199/1414 images processed!\n",
            "200/1414 images processed!\n",
            "201/1414 images processed!\n",
            "202/1414 images processed!\n",
            "203/1414 images processed!\n",
            "204/1414 images processed!\n",
            "205/1414 images processed!\n",
            "206/1414 images processed!\n",
            "207/1414 images processed!\n",
            "208/1414 images processed!\n",
            "209/1414 images processed!\n",
            "210/1414 images processed!\n",
            "211/1414 images processed!\n",
            "212/1414 images processed!\n",
            "213/1414 images processed!\n",
            "214/1414 images processed!\n",
            "215/1414 images processed!\n",
            "216/1414 images processed!\n",
            "217/1414 images processed!\n",
            "218/1414 images processed!\n",
            "219/1414 images processed!\n",
            "220/1414 images processed!\n",
            "221/1414 images processed!\n",
            "222/1414 images processed!\n",
            "223/1414 images processed!\n",
            "224/1414 images processed!\n",
            "225/1414 images processed!\n",
            "226/1414 images processed!\n",
            "227/1414 images processed!\n",
            "228/1414 images processed!\n",
            "229/1414 images processed!\n",
            "230/1414 images processed!\n",
            "231/1414 images processed!\n",
            "232/1414 images processed!\n",
            "233/1414 images processed!\n",
            "234/1414 images processed!\n",
            "235/1414 images processed!\n",
            "236/1414 images processed!\n",
            "237/1414 images processed!\n",
            "238/1414 images processed!\n",
            "239/1414 images processed!\n",
            "240/1414 images processed!\n",
            "241/1414 images processed!\n",
            "242/1414 images processed!\n",
            "243/1414 images processed!\n",
            "244/1414 images processed!\n",
            "245/1414 images processed!\n",
            "246/1414 images processed!\n",
            "247/1414 images processed!\n",
            "248/1414 images processed!\n",
            "249/1414 images processed!\n",
            "250/1414 images processed!\n",
            "251/1414 images processed!\n",
            "252/1414 images processed!\n",
            "253/1414 images processed!\n",
            "254/1414 images processed!\n",
            "255/1414 images processed!\n",
            "256/1414 images processed!\n",
            "257/1414 images processed!\n",
            "258/1414 images processed!\n",
            "259/1414 images processed!\n",
            "260/1414 images processed!\n",
            "261/1414 images processed!\n",
            "262/1414 images processed!\n",
            "263/1414 images processed!\n",
            "264/1414 images processed!\n",
            "265/1414 images processed!\n",
            "266/1414 images processed!\n",
            "267/1414 images processed!\n",
            "268/1414 images processed!\n",
            "269/1414 images processed!\n",
            "270/1414 images processed!\n",
            "271/1414 images processed!\n",
            "272/1414 images processed!\n",
            "273/1414 images processed!\n",
            "274/1414 images processed!\n",
            "275/1414 images processed!\n",
            "276/1414 images processed!\n",
            "277/1414 images processed!\n",
            "278/1414 images processed!\n",
            "279/1414 images processed!\n",
            "280/1414 images processed!\n",
            "281/1414 images processed!\n",
            "282/1414 images processed!\n",
            "283/1414 images processed!\n",
            "284/1414 images processed!\n",
            "285/1414 images processed!\n",
            "286/1414 images processed!\n",
            "287/1414 images processed!\n",
            "288/1414 images processed!\n",
            "289/1414 images processed!\n",
            "290/1414 images processed!\n",
            "291/1414 images processed!\n",
            "292/1414 images processed!\n",
            "293/1414 images processed!\n",
            "294/1414 images processed!\n",
            "295/1414 images processed!\n",
            "296/1414 images processed!\n",
            "297/1414 images processed!\n",
            "298/1414 images processed!\n",
            "299/1414 images processed!\n",
            "300/1414 images processed!\n",
            "301/1414 images processed!\n",
            "302/1414 images processed!\n",
            "303/1414 images processed!\n",
            "304/1414 images processed!\n",
            "305/1414 images processed!\n",
            "306/1414 images processed!\n",
            "307/1414 images processed!\n",
            "308/1414 images processed!\n",
            "309/1414 images processed!\n",
            "310/1414 images processed!\n",
            "311/1414 images processed!\n",
            "312/1414 images processed!\n",
            "313/1414 images processed!\n",
            "314/1414 images processed!\n",
            "315/1414 images processed!\n",
            "316/1414 images processed!\n",
            "317/1414 images processed!\n",
            "318/1414 images processed!\n",
            "319/1414 images processed!\n",
            "320/1414 images processed!\n",
            "321/1414 images processed!\n",
            "322/1414 images processed!\n",
            "323/1414 images processed!\n",
            "324/1414 images processed!\n",
            "325/1414 images processed!\n",
            "326/1414 images processed!\n",
            "327/1414 images processed!\n",
            "328/1414 images processed!\n",
            "329/1414 images processed!\n",
            "330/1414 images processed!\n",
            "331/1414 images processed!\n",
            "332/1414 images processed!\n",
            "333/1414 images processed!\n",
            "334/1414 images processed!\n",
            "335/1414 images processed!\n",
            "336/1414 images processed!\n",
            "337/1414 images processed!\n",
            "338/1414 images processed!\n",
            "339/1414 images processed!\n",
            "340/1414 images processed!\n",
            "341/1414 images processed!\n",
            "342/1414 images processed!\n",
            "343/1414 images processed!\n",
            "344/1414 images processed!\n",
            "345/1414 images processed!\n",
            "346/1414 images processed!\n",
            "347/1414 images processed!\n",
            "348/1414 images processed!\n",
            "349/1414 images processed!\n",
            "350/1414 images processed!\n",
            "351/1414 images processed!\n",
            "352/1414 images processed!\n",
            "353/1414 images processed!\n",
            "354/1414 images processed!\n",
            "355/1414 images processed!\n",
            "356/1414 images processed!\n",
            "357/1414 images processed!\n",
            "358/1414 images processed!\n",
            "359/1414 images processed!\n",
            "360/1414 images processed!\n",
            "361/1414 images processed!\n",
            "362/1414 images processed!\n",
            "363/1414 images processed!\n",
            "364/1414 images processed!\n",
            "365/1414 images processed!\n",
            "366/1414 images processed!\n",
            "367/1414 images processed!\n",
            "368/1414 images processed!\n",
            "369/1414 images processed!\n",
            "370/1414 images processed!\n",
            "371/1414 images processed!\n",
            "372/1414 images processed!\n",
            "373/1414 images processed!\n",
            "374/1414 images processed!\n",
            "375/1414 images processed!\n",
            "376/1414 images processed!\n",
            "377/1414 images processed!\n",
            "378/1414 images processed!\n",
            "379/1414 images processed!\n",
            "380/1414 images processed!\n",
            "381/1414 images processed!\n",
            "382/1414 images processed!\n",
            "383/1414 images processed!\n",
            "384/1414 images processed!\n",
            "385/1414 images processed!\n",
            "386/1414 images processed!\n",
            "387/1414 images processed!\n",
            "388/1414 images processed!\n",
            "389/1414 images processed!\n",
            "390/1414 images processed!\n",
            "391/1414 images processed!\n",
            "392/1414 images processed!\n",
            "393/1414 images processed!\n",
            "394/1414 images processed!\n",
            "395/1414 images processed!\n",
            "396/1414 images processed!\n",
            "397/1414 images processed!\n",
            "398/1414 images processed!\n",
            "399/1414 images processed!\n",
            "400/1414 images processed!\n",
            "401/1414 images processed!\n",
            "402/1414 images processed!\n",
            "403/1414 images processed!\n",
            "404/1414 images processed!\n",
            "405/1414 images processed!\n",
            "406/1414 images processed!\n",
            "407/1414 images processed!\n",
            "408/1414 images processed!\n",
            "409/1414 images processed!\n",
            "410/1414 images processed!\n",
            "411/1414 images processed!\n",
            "412/1414 images processed!\n",
            "413/1414 images processed!\n",
            "414/1414 images processed!\n",
            "415/1414 images processed!\n",
            "416/1414 images processed!\n",
            "417/1414 images processed!\n",
            "418/1414 images processed!\n",
            "419/1414 images processed!\n",
            "420/1414 images processed!\n",
            "421/1414 images processed!\n",
            "422/1414 images processed!\n",
            "423/1414 images processed!\n",
            "424/1414 images processed!\n",
            "425/1414 images processed!\n",
            "426/1414 images processed!\n",
            "427/1414 images processed!\n",
            "428/1414 images processed!\n",
            "429/1414 images processed!\n",
            "430/1414 images processed!\n",
            "431/1414 images processed!\n",
            "432/1414 images processed!\n",
            "433/1414 images processed!\n",
            "434/1414 images processed!\n",
            "435/1414 images processed!\n",
            "436/1414 images processed!\n",
            "437/1414 images processed!\n",
            "438/1414 images processed!\n",
            "439/1414 images processed!\n",
            "440/1414 images processed!\n",
            "441/1414 images processed!\n",
            "442/1414 images processed!\n",
            "443/1414 images processed!\n",
            "444/1414 images processed!\n",
            "445/1414 images processed!\n",
            "446/1414 images processed!\n",
            "447/1414 images processed!\n",
            "448/1414 images processed!\n",
            "449/1414 images processed!\n",
            "450/1414 images processed!\n",
            "451/1414 images processed!\n",
            "452/1414 images processed!\n",
            "453/1414 images processed!\n",
            "454/1414 images processed!\n",
            "455/1414 images processed!\n",
            "456/1414 images processed!\n",
            "457/1414 images processed!\n",
            "458/1414 images processed!\n",
            "459/1414 images processed!\n",
            "460/1414 images processed!\n",
            "461/1414 images processed!\n",
            "462/1414 images processed!\n",
            "463/1414 images processed!\n",
            "464/1414 images processed!\n",
            "465/1414 images processed!\n",
            "466/1414 images processed!\n",
            "467/1414 images processed!\n",
            "468/1414 images processed!\n",
            "469/1414 images processed!\n",
            "470/1414 images processed!\n",
            "471/1414 images processed!\n",
            "472/1414 images processed!\n",
            "473/1414 images processed!\n",
            "474/1414 images processed!\n",
            "475/1414 images processed!\n",
            "476/1414 images processed!\n",
            "477/1414 images processed!\n",
            "478/1414 images processed!\n",
            "479/1414 images processed!\n",
            "480/1414 images processed!\n",
            "481/1414 images processed!\n",
            "482/1414 images processed!\n",
            "483/1414 images processed!\n",
            "484/1414 images processed!\n",
            "485/1414 images processed!\n",
            "486/1414 images processed!\n",
            "487/1414 images processed!\n",
            "488/1414 images processed!\n",
            "489/1414 images processed!\n",
            "490/1414 images processed!\n",
            "491/1414 images processed!\n",
            "492/1414 images processed!\n",
            "493/1414 images processed!\n",
            "494/1414 images processed!\n",
            "495/1414 images processed!\n",
            "496/1414 images processed!\n",
            "497/1414 images processed!\n",
            "498/1414 images processed!\n",
            "499/1414 images processed!\n",
            "500/1414 images processed!\n",
            "501/1414 images processed!\n",
            "502/1414 images processed!\n",
            "503/1414 images processed!\n",
            "504/1414 images processed!\n",
            "505/1414 images processed!\n",
            "506/1414 images processed!\n",
            "507/1414 images processed!\n",
            "508/1414 images processed!\n",
            "509/1414 images processed!\n",
            "510/1414 images processed!\n",
            "511/1414 images processed!\n",
            "512/1414 images processed!\n",
            "513/1414 images processed!\n",
            "514/1414 images processed!\n",
            "515/1414 images processed!\n",
            "516/1414 images processed!\n",
            "517/1414 images processed!\n",
            "518/1414 images processed!\n",
            "519/1414 images processed!\n",
            "520/1414 images processed!\n",
            "521/1414 images processed!\n",
            "522/1414 images processed!\n",
            "523/1414 images processed!\n",
            "524/1414 images processed!\n",
            "525/1414 images processed!\n",
            "526/1414 images processed!\n",
            "527/1414 images processed!\n",
            "528/1414 images processed!\n",
            "529/1414 images processed!\n",
            "530/1414 images processed!\n",
            "531/1414 images processed!\n",
            "532/1414 images processed!\n",
            "533/1414 images processed!\n",
            "534/1414 images processed!\n",
            "535/1414 images processed!\n",
            "536/1414 images processed!\n",
            "537/1414 images processed!\n",
            "538/1414 images processed!\n",
            "539/1414 images processed!\n",
            "540/1414 images processed!\n",
            "541/1414 images processed!\n",
            "542/1414 images processed!\n",
            "543/1414 images processed!\n",
            "544/1414 images processed!\n",
            "545/1414 images processed!\n",
            "546/1414 images processed!\n",
            "547/1414 images processed!\n",
            "548/1414 images processed!\n",
            "549/1414 images processed!\n",
            "550/1414 images processed!\n",
            "551/1414 images processed!\n",
            "552/1414 images processed!\n",
            "553/1414 images processed!\n",
            "554/1414 images processed!\n",
            "555/1414 images processed!\n",
            "556/1414 images processed!\n",
            "557/1414 images processed!\n",
            "558/1414 images processed!\n",
            "559/1414 images processed!\n",
            "560/1414 images processed!\n",
            "561/1414 images processed!\n",
            "562/1414 images processed!\n",
            "563/1414 images processed!\n",
            "564/1414 images processed!\n",
            "565/1414 images processed!\n",
            "566/1414 images processed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-10-bc84c889fa35>\", line 24, in <module>\n",
            "    image_tensor = image_transform(m)  #予測のための画像下処理\n",
            "  File \"<ipython-input-7-dcff738519e9>\", line 546, in image_transform\n",
            "    image=Image.open(image_path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/PIL/Image.py\", line 2852, in open\n",
            "    prefix = fp.read(16)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 733, in getmodule\n",
            "    if ismodule(module) and hasattr(module, '__file__'):\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdMWm5gkEsXa",
        "outputId": "2a6b2819-4990-4e0d-a581-530dad084b41"
      },
      "source": [
        "#######################\n",
        "##1枚の画像のみを判定\n",
        "#######################\n",
        "\n",
        "\n",
        "print(MODEL_PATH)\n",
        "print(MODEL_NAME)\n",
        "\n",
        "outputs = []\n",
        "for i in range(5):\n",
        "    #トレーニングしたパラメーターを適用\n",
        "    PATH = MODEL_PATH+'/'+MODEL_NAME+'_'+str(i)+'.pth'\n",
        "    model_ft.load_state_dict(torch.load(PATH))\n",
        "    model_ft.eval()\n",
        "\n",
        "    df_result = pd.read_csv(FILENAME_RESULTCSV)\n",
        "    print(MODEL_NAME+'_'+str(i)+'.pth')\n",
        "    print(\"Calculating prediction results!\")\n",
        "\n",
        "    #ファイル名を取得\n",
        "    img_path = \"/content/drive/MyDrive/Deep_learning/FundusPhoto/cropped_img_trainval/0/train/img00085008_00_1R.jpg\"\n",
        "\n",
        "    image_tensor = image_transform(img_path)  #予測のための画像下処理\n",
        "    pred = image_eval(image_tensor, model_ft)  #予測結果を出力   \n",
        "\n",
        "    outputs.append(pred)\n",
        "\n",
        "print(outputs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cropped_A2_pretrained_0.pth\n",
            "Calculating prediction results!\n",
            "cropped_A2_pretrained_1.pth\n",
            "Calculating prediction results!\n",
            "cropped_A2_pretrained_2.pth\n",
            "Calculating prediction results!\n",
            "cropped_A2_pretrained_3.pth\n",
            "Calculating prediction results!\n",
            "cropped_A2_pretrained_4.pth\n",
            "Calculating prediction results!\n",
            "[50.76050162315369, 40.92430770397186, 62.7096951007843, 62.94362545013428, 59.010350704193115]\n"
          ]
        }
      ]
    }
  ]
}